<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homepage on Miao Yu | 于淼 </title>
    <link>/index.xml</link>
    <description>Recent content in Homepage on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Feb 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>主成分分析那些事儿</title>
      <link>/cn/2017/09/14/pca/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/09/14/pca/</guid>
      <description>&lt;p&gt;目之所及，主成分分析应该是科研领域里最通用的一种数据分析手段。在相当长的一段时间里，我认为这种方法主要是用来进行探索分析的可视化手段与数据降维，但最近因为出现了一个绕不过去的数据问题就把主成分分析又拎出来看了一下，这才意识到这个方法其实四通八达，可以把很多数据分析的概念连接起来。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;从线到点&lt;/h2&gt;
&lt;p&gt;首先还是回到一个最简单的场景，我有一堆数，想找出一个来做代表。从距离测量角度，就是找一个点让它离所有点最近，这就成了个优化问题，此时不同测量方法结论是不一样的。例如你考虑距离的绝对值最小，那你就会得到中位数；如果是差异的平方，求导后就是均值。回想下对一堆数找一个数，其实就是一种降维，从1维降低到0维。这里我们只考虑最小化差异的平方，那么求均值就是主成分分析把从1维降低到0维的应用场景。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;从面到线&lt;/h2&gt;
&lt;p&gt;现在复杂一点，我们设想一个二维（无码）平面，如果我们对其降维，那么降一维就是线，降两维就是点。而且我们可以确定降两维的那个点肯定就在降一维的线上，不然你这个降维就丧失了代表性。至于如何保障代表性，一般来说要交给数学家。那么这条线会通过所有点的均值，此时你应该想起来二维线性回归也通过这个点，那条线可以通过最小二乘得到，会不会就是我们要找的那条线？这个答案是否定的，最小二乘里最小化的是因变量到回归线的值，但是这里主成分分析最小化的是所有点到一条线的垂直距离，模型上细微的差别导致结果也会有区别，事实上求解过程也不对等。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;主成分分析的求解思想&lt;/h2&gt;
&lt;p&gt;虽然最小二乘回归线是高斯－马尔可夫定理下线性回归的最佳无偏估计，但主成分分析里二维降一维里那条线的求解思想并非回归到均值，常见有两种解释。第一种是寻找低维度空间，使投影点上到高维度点距离最近；另一种则是从统计角度寻找让点之间方差最大的方向映射，然后寻找跟这个方向正交的方向中方差最大的方向继续映射。从求解上，这两种解释都可转化成最优化问题，都是先归一化，然后求协方差矩阵，通过求导求特征向量跟特征值，那个方差最大的方向或距离最短子空间维度就是协方差矩阵里特征值最大的特征向量，剩下的方向或维度跟前面那个正交，再次找方差最大或距离最小即可。当然协方差矩阵不是一定要求的，如果你选用奇异值分解的套路就完全不用求。在这个求解策略下，解析解就是正交的，如果不是，那就不是主成分分析了。&lt;/p&gt;
&lt;p&gt;除此之外，理论上你也可以用隐藏变量模型迭代求解，不过有解析解不要用数值分析去逼近，而且有些矩阵运算可以进行分布式计算，这个在数据量大时是要特别考虑的。主成分分析求解上可以用矩阵是很大的优势，虽然理论上其概率解释并不完美。不同求解思想的多元分析方法其实背后都是有思想跟应用场景的，虽然理论上很多都是通用方法，但如果不适合你的数据就不要用。当前由于技术进步，之前很多很耗性能的方法目前都可以计算得到，如果搞科研我们要找那个最完美的，但工业应用可能更看重性价比。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;归一化&lt;/h2&gt;
&lt;p&gt;如果我们进一步考察三维空间，那么我们的降维就首先是一个平面，然后是平面上的线，然后是线上的点。此时如果你对所有数据点乘2，那么很自然点、线、面的坐标位置都要变化，这样你就可以理解一个事实，那就是主成分分析对尺度是敏感的，所以一般来说都要对不同尺度／维度的测量进行归一化，否则你的映射会被大尺度的维度带跑偏。到现在为止，我们可以大概对主成分分析有个直观感受：将高维度空间的点映射到低纬度空间的点且要保证这些点之间的差异关系最大程度地保留，至于怎么保留，不同求解思想实际求解结果一致，都可以用矩阵运算，内含了进行转换或映射时要沿着正交的维度走（使用了正定阵），所以求解完矩阵就可以得到符合要求的低维度空间，而且低维空间是正交的。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;投影点的方差&lt;/h2&gt;
&lt;p&gt;主成分分析经常用来可视化，这里我们回到二维平面降维的场景仔细看看我们究竟可视化了什么。首先我们有一个二维点A，这个点投影到一维线上得到点B，这个点跟所有点的均值C连线就是到0维的投影。目前我们已知AC这个线，同时A到一维线的距离又要最小也就是垂直，这样A、B及C构成一个直角三角形。此时根据勾股定理BC这个距离最大，也就是一维到0维时所有投影点的距离之和最长，在这个方向中各点间方差最大程度保留，也就是找到了方差最大的方向。事实上，因为前面提到的直角三角形，每降低一次维度，点之间的距离比高维度都不可避免的减少，如果此时点聚到一起不一定相似度很高，但如果主成分占总方差比重比较大，那么这些点就很有可能十分相似。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;多维标度分析&lt;/h2&gt;
&lt;p&gt;说到距离，其实主成分分析也是多维标度分析的一种。在经典多维标度分析中，测定点很困难，但可以测到点之间欧式距离并构建距离矩阵，因为其映射子空间里点之间方差最大时可以证明点之间的距离也是最大的，这个特性保证了当我们只有距离矩阵时进行主成分分析得到的低维映射也可以保证两个空间间距离最短，这样主成分分析事实上符合经典多维标度分析。也就是说，在你能够测到欧式距离而不是点时，是有可能重构出原始标度的，这点在结构生物学上有应用，但我完全不了解。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;概率化的主成分分析&lt;/h2&gt;
&lt;p&gt;主成分分析在求解上基本都走了矩阵运算的捷径，结果也是等价的。但这个过程不算是一个概率模型，因为可能产生不确定度的白噪音根本没出现在求解模型中。此时，我们应该意识到，这个子空间可能是某个概率模型的解，但如同我们只求了均值没求方差一样，似乎我们没有考虑模型的不确定度。这样我们需要从统计角度把主成分分析统一到基于统计学的数据分析中，这样也许会对将来构建相关假设检验模型有用，当然这也意味着我们可能不太方便再用矩阵运算来求解了。&lt;/p&gt;
&lt;p&gt;首先，我们对数据点进行假设，例如来自一个正态分布，那么主成分分析的问题就转化为求一个子空间，使得映射后的距离最小。让我们把这个映射关系描述成下面这样：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t = Wx + \mu + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里t是我们观察到的数据点，W是映射关系，维度不变可以理解成坐标轴旋转，x是映射后的点，&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;代表x的均值，&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;代表高斯随机变量。这样我们看到的点符合均值&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(WW^t + \psi\)&lt;/span&gt;的正态分布，这里&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;代表了随机误差，如果我们不考虑这一项，那么主成分分析是完全可以用特征值跟特征向量求解的，此时我们默认那个误差项0方差。但是，实际场景中我们都很清楚每一个高维样本点都至少存在测量误差，这个误差的方差不是0，那么此时我们应该在模型中包含误差项，但一个很尴尬的问题是我们对这个误差一无所知。此时我们假定所有点的误差项来自于某一个方差统一的正态分布，然后有了这个限制条件就可以求解了。加入了这一部分后，主成分分析就可以进行假设检验了，例如某个点是否属于异常值。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EM算法&lt;/h2&gt;
&lt;p&gt;说到求解EM算法是绕不过去的，这个算法普适性比较强，存在隐藏变量的模型求解都可以用。主成分分析可以看作一种存在隐藏变量的模型，我们在低维空间看到的点背后是高维空间点但看不到，反之也成立。这样我们先随意假设一个新空间出来，这样我们就可以进行E阶段计算，也就是把看到的点投影到这个新空间上，然后计算距离。之后我们就可以进行M阶段，也就是最小化距离，这样就做出了一个比起始新空间距离更小的空间。然后再进行E阶段，M阶段，直到距离无法缩小。说白了就是模型不存在时先人工创造一个，然后不断按你的目标迭代让模型跟数据契合。在EM算法里，我们就可以很轻松把前面的方差项也扔进去一同优化，最后也能求解。这样概率化的主成分分析就有解了。不过这个算法具体实现存在很高的技巧性，我们吃现成就可以了。同时你会发现，其实EM算法思想可以用在很多不同模型参数求解上，马尔可夫过程、贝叶斯网络、条件随机场等有隐含变量的都可以用。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;因子分析&lt;/h2&gt;
&lt;p&gt;其实在更多资料中引入概率化的主成分分析主要是为了引入因子分析，因子分析跟概率化主成分分析最大区别在于其不限制误差来自方差相同的正态分布。这当然增加了计算难度，但其实因子分析对于解释这种隐藏结构其实比主成分分析更靠谱。但是，因子分析求解上不如主成分分析容易理解，需要通过一些方法来决定因子数或干脆使用者自己决定。此外，因子分析是可以进行预测的，目标就是潜在因子。从概率角度讲主成分分析自然也可进行预测，不过你得想清楚应用场景。同时，因子分析得到的成分也是正交的，这点跟主成分分析一致。正交的优点在于映射之间不相关，但不一定独立，如果数据分布需要独立因素就需要独立成分分析。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;独立成分分析&lt;/h2&gt;
&lt;p&gt;独立成分分析在独立成分符合正态分布时其实就是主成分分析，但当你独立成分并不来自正态分布时，独立成分分析就更有优势将其反推出来。因为独立跟相关是不同的，独立在统计学里比不相关约束条件更强，不相关不一定独立但独立一定不相关，独立因素间的互信息为0或者说高阶统计量上都不相关。最经典的应用就是鸡尾酒会问题，在一个嘈杂的场景里很多人都在说话，你在固定位置放了几个麦克风，这样麦克风收集到的就是多种声音的混合，现在你需要把混音中每个人的声音提取出来。此时你要用主成分分析，你会得到所有人声音的共性，但独立成分分析就可以分辨出每个个体，或者说潜在变量，所以你也猜到了，EM算法也可以求解独立成分分析。需要注意的是独立成分分析不管降维，基本你设定分多少个就有多少个。但不论主成分分析、因子分析还是独立成分分析，本质上都是线性模型的结构，也就是所谓的主成分、因子、独立成分都是原始数据的线性组合。&lt;/p&gt;
&lt;p&gt;在我看来生物组学数据更适合独立成分分析，你可以直接从数据中提取出一组独立模块进行注释，这样可以直接去关联生物学功能而不是反过来。当然你可能会说主成分分析或因子分析也可以做啊，但你如何保证其分布假设与正交假设？不过我们还是用主成分分析来说一下，因为很多人没意识到这个功能。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;聚类／共性／压缩／降噪&lt;/h2&gt;
&lt;p&gt;有些论文用主成分分析搞聚类画圈圈来说明样品间存在内在共性。这个在环境分析中比较常见，因为环境分析通常同时测定上百种化合物，前面提到低维映射里最大程度保留了样品点的差异，此时映射到一起就有可能说明那些样品污染特征接近，便于探索来源或环境过程。实际上此时不一定需要局限在主成分分析，可以直接用聚类分析等统计模型。&lt;/p&gt;
&lt;p&gt;很多人搞不清楚特征值、特征向量还有载荷等的细节，所以主成分分析就被用成了降维画图工具，但其实这个探索分析是针对背后隐藏变量的，具体到主成分分析就是共性。还是举个例子来说吧，我有100个样品，每个样品测了1000个指标，现在我就有了个&lt;span class=&#34;math inline&#34;&gt;\(100*1000\)&lt;/span&gt;的矩阵，通过主成分分析我得到了&lt;span class=&#34;math inline&#34;&gt;\(100*250\)&lt;/span&gt;的矩阵，这个矩阵包含了原数据95%的方差。好了，现在我问你，这250个新指标是什么？对，特征向量，特征向量就是新投影方向，投影可以看作隐含共性。特征值又是什么，共性的权重，越大代表越重要，毕竟可以代表更多的方差。那么载荷又是什么，大概可以理解成原来1000个指标对250个新指标的贡献。那么进行分析时我们在样本和指标之间多了一个共性层，一方面减少了数据维度，另一方面算是提取了指标间不相关的共性（但不一定独立，切记）。对于多出来的共性层，我们同时知道样品在这些共性上的分布，也知道每个指标对共性的分布，常见的biplot就可以同时绘制样品点跟指标在两个最重要共性上的分布，一目了然。此时我们的专业知识就要上场了，我们可能会通过指标相互作用发现共性背后对应隐含因素的物理含义，也可以发现某种分离样品的共性背后暗示的样品潜在来源。总之，多了一个共性层后，我们可以研究的机理就更明显了，例如自然语言处理里可以用其寻找文本主题，基因组学里可以用来寻找基因模块等。但需要提醒的是，这个“共性”并不代表客观规律，只是一种线性变换后的结果，如果跟实际想研究的因素不对应还不如直接上回归分析。&lt;/p&gt;
&lt;p&gt;主成分分析或者说实现主成分分析的奇异值分解的另一个应用就是可以用来压缩数据，上面的例子中100*1000的数据空间如果赶上稀疏矩阵十分浪费，此时就可以用奇异值分解压缩存储空间。从信号处理的角度去看，主成分分析跟傅立叶变换等变换过程本质上都是用一组新信号替代原来信号，由于一般认为信号方差高于噪音方差，通过变换时保留主成分或特定频谱，我们同时可以做到降噪。图形处理也可以用，而所有数据其实都可以用图形展示，那么作为图形降噪的主成分分析背后跟数据降噪是可以联系到一起的，特别环境痕量分析中的降噪。结合前面的结构重构、方差保留等性质，其实哪怕你就只会主成分分析，想明白了能做的事也很多。&lt;/p&gt;
&lt;p&gt;你只是需要一点想象力。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>科研博客圈的书剑恩仇</title>
      <link>/cn/2017/09/04/sci-blog-discussion/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/09/04/sci-blog-discussion/</guid>
      <description>&lt;p&gt;有人的地方就有江湖。&lt;/p&gt;
&lt;p&gt;推动科学进步的是学术争论，大家围坐一席以数据与逻辑为工具互相质疑，寻求共识。但事实上这个过程中并不缺乏个人或群体情感的介入，这一方面是现代科研职业化所导致的拿钱吃饭，另一方面则是科研人员自身的主观好恶。这一点在科学家的博客上展示的淋漓尽致，虽然在学术期刊里发评论比较正式，但在预印本、数据共享与可重复性研究的大趋势下，越来越多的科学家选择时效性更高的非同行评议的博客来对科学进展进行评论。借助这些社交媒体，我们也可以一窥他们对学术观点的爱恨情仇，也许有人不屑于这些主观性比较强的评论，但从学术交流的角度出发，如果我们仅仅通过学术期刊与会议交流学术观点，由于存在审稿与运作周期，很多共识会消耗大量的传播成本来达成，这不仅与信息时代脱节，也会造成资源浪费。下面我们看些案例感受下国外学术界在博客这一媒介上的观点交锋：&lt;/p&gt;
&lt;p&gt;案例一：“主观”的贝叶斯方法&lt;/p&gt;
&lt;p&gt;哥伦比亚大学的 Andrew Gelman 的博客可以算得上是个火药桶了，他本身主张贝叶斯学派，而赶巧贝叶斯学派跟频率学派可以算得上科研数据分析里哲学思想差异最大的两派，起码按我的粗浅认识是根本无法调和的，所以即便实用上甚至算法上都差异不大，想对这两种思想和稀泥基本都会被 Gelman 教授无情嘲讽，如果你还打算说贝叶斯不好，基本上会被博文讨伐。当然，也不是所有人都有这个待遇，同舟子的做法类似， Gelman 教授基本也是逮着大鱼去坑。需要提醒的是他可不是舟子那种十几年不做科研的学术圈外人士，其本人是哥伦比亚大学应用统计中心的主任，其团队的研究领域十分广阔，大家可以感受一下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;why it is rational to vote; why campaign polls are so variable when elections are so predictable; why redistricting is good for democracy; reversals of death sentences; police stops in New York City, the statistical challenges of estimating small effects; the probability that your vote will be decisive; seats and votes in Congress; social network structure; arsenic in Bangladesh; radon in your basement; toxicology; medical imaging; and methods in surveys, experimental design, statistical inference, computation, and graphics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;顺带一提，著名贝叶斯统计软件 stan 就出自这个团队。&lt;/p&gt;
&lt;p&gt;这次事情的起因是卡内基·梅隆大学的 Larry Wasserman 教授（2016年当选美国国家科学院院士）在接受一个博客&lt;a href=&#34;https://errorstatistics.com/2013/12/28/wasserman-on-wasserman-update-december-28-2013/&#34;&gt;采访&lt;/a&gt;时对频率学派与贝叶斯学派下了个定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wish people were clearer about what Bayes is/is not and what frequentist inference is/is not. Bayes is the analysis of subjective beliefs but provides no frequency guarantees. Frequentist inference is about making procedures that have frequency guarantees but makes no pretense of representing anyone’s beliefs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Gelman 教授对其频率学派的观点没啥意见，但那个 “subjective” 直接引爆了火药桶。而按照 Gelman 的定义，贝叶斯方法应该是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Using inference from the posterior distribution, p(theta|y)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;特别的，他还认为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Science is always full of subjective human choices, and it’s always about studying larger questions that have an objective reality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;坦白说这个看法是比较符合科学史的，虽然当今科学理论体系逻辑上相对完备（先排除下哥德尔跟量子力学），但其发展确实很曲折，在实验数据跟统计决策成为主流之前，很多理论在发现或提出时主流科学家并不接受，有的是逻辑上不接受（很多新理论完全不容于旧理论），有的则属于威权集团打压，可以说相当主观。&lt;/p&gt;
&lt;p&gt;但在后面的论述中，Gelman 教授就开始开嘲讽技能了，Larry 认为在高维数据处理中贝叶斯方法没意义无法解释，Gelman 教授则反驳说他觉得除了贝叶斯方法别的方法也都是解释不通的，并且他认为 Larry 自己不懂贝叶斯还瞎定义是十分不妥的。不得不说这段论述很没营养，跟小学生吵架差不多。紧接着 Gelman 教授又提到主观确实是贝叶斯方法的一部分但不是全部，那频率学派是不是可以说成“简单随机采样的技术”，科学研究范围在拓展，各种方法也在发展，贝叶斯方法可以研究客观问题。这个说法也比较中肯，接下来 Gelman 教授又开启了挖坟模式，他把 Larry 08年到13年关于贝叶斯方法中随机性看法的转变给列了出来，紧接着又说我也有这个转变过程。但文章最后他又翻了 Larry 对经济学家的旧账，认为他存在个人偏见。&lt;/p&gt;
&lt;p&gt;看起来这个文章似乎比较正常，但这篇博文真正有趣的是评论，基本上集中了当今统计学中各路高手，下面是个不完全名单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nick Cox 杜伦大学 Stata 元老级开发者&lt;/li&gt;
&lt;li&gt;Larry Wasserman 卡内基·梅隆大学教授 当事人&lt;/li&gt;
&lt;li&gt;Deborah G. Mayo 宾夕法尼亚大学教授 采访 Larry 的人 errorstatistics.com 博主&lt;/li&gt;
&lt;li&gt;Kevin Dick 斯坦福毕业 创业者 possibleinsight.com 博主&lt;/li&gt;
&lt;li&gt;Judea Pearl UCLA 教授 &lt;a href=&#34;http://causality.cs.ucla.edu/blog/&#34; class=&#34;uri&#34;&gt;http://causality.cs.ucla.edu/blog/&lt;/a&gt; 博主&lt;/li&gt;
&lt;li&gt;Christian Hennig 伦敦大学学院教授&lt;/li&gt;
&lt;li&gt;Norm Matloff UC Davis 教授&lt;/li&gt;
&lt;li&gt;Brendan K O’Rourke 都柏林理工教授 &lt;a href=&#34;http://www.brendankorourke.com/&#34; class=&#34;uri&#34;&gt;http://www.brendankorourke.com/&lt;/a&gt; 博主&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以看出这么几件事：首先，这些领域内专家会互相关注对方的个人网站并通过这种方式互动；其次，看他们的讨论很有启发，必看书本上的干货更有意思；再次，很多讨论虽然对问题是没营养的，但有助于我们了解一些学术界的风格或流派。在前沿领域由于知识不全，多数情况是无法达成共识的，但通过了解其流派风格会帮助你更全面的看问题。&lt;/p&gt;
&lt;p&gt;案例二：两个软件会产生一个结果吗？&lt;/p&gt;
&lt;p&gt;巴拉巴西是一位呼声很高的诺奖候选人，其畅销书《链接》可以说把不少科研人员吸引到了网络科学的研究领域，现实中的无尺度网络的幂律分布所具有的奇特性质在很多并不相关的领域都有展示。但这个故事主角不是他，提到他只是想提前表示下同情，因为他在加州理工教授 Lior Pachter 的博客里躺枪了。事实上，2014年 Lior Pachter 在博客上开了个三部曲，本意就是对 MIT 教授 Manolis Kellis 的个人恩怨，但为了把故事讲的通透点，这位老兄追根溯源并展示了自己强大的数理功底，先后对两篇发表文章的创新性进行质疑，从图表到算法，其中一篇就是巴拉巴西的，另一篇则是 Manolis Kellis 教授的。这个故事科学网薛宇老师曾经翻译评论过，我这里不细讲。但 Lior Pachter 教授在后续的博文中又对号称H指数100多的巴拉巴西来了次二次扒皮，严格说被炮轰的其实不算冤枉，但被人挂的如此直白也只有 Lior Pachter 教授能做得出来。而我今天要讲的是他最近又跟纽约大学石溪分校&amp;amp;哈佛&amp;amp;卡内基·梅隆大学的同行掐架了，上演了进攻-防守-再进攻的三部曲。&lt;/p&gt;
&lt;p&gt;首先 Lior 讲在他们那个 RNA 测序定量的圈子里，软件跟软件差异都是很大的，基本你用不同软件想得到一样的结果非常困难（这也说明这个领域的研究共识没有达成）。然后他话锋一转，说自己组里2016开发的一个软件跟最近发表在 Nature Methods 上的软件处理结果却出奇的一致，皮尔逊相关系数三个九，然后又是一通追根溯源。这里岔开说一句，Lior 之所以可以追根溯源，是因为预印本及版本控制系统的流行，最近 ACS 也对化学领域提供了预印本服务，预计不久就会覆盖绝大多数涉及数据分析的实验学科。从版本上 Lior 发现在他们论文发表后 Rob Patro 的软件也有了一个很大的更新，更新前跟他们组软件差异明显，更新后确几乎一样了，最后他认为 Rob Patro 所发表的文章实际上就是抄了自己组里开发软件的思想，然后加了个矫正。当然 Rob Patro 也很快在 github 上发表了一个回应，大意是他们在文章跟源码中多次引用了 Lior 组的论文并且在有些数据集中这两个软件的结果是不一样的，工作流程也不一样。但 Lior 教授显然并不满意，他又写了一篇博文指出其回复混淆视听，所谓的不一样是下游分析，而在 RNA 定量上这两个差距还是很小，如果你去看这篇回复会发现 Lior 甚至使用了动画来展示两者区别很小，可谓精心准备。我在读这三篇文章时学到很多的论述方法与追踪验证方法，可以说很多方法现在还没出现在教科书中，但可以感到早晚会形成趋势。&lt;/p&gt;
&lt;p&gt;凭心而论， Rob Patro 的文章就算是对 Lior 软件的改进也是值得发表的，因为当前科研基本都是N+1模式，都是在前人基础上做功课。但我也比较理解 Lior 为什么这么火大，首先在他眼里这两个软件本来就是一回事，凭什么发 Nature Methods，他自己那篇都没发这么好，另一方面就是 Rob Patro 文章在他看来有硬伤，速度也不快，效果也没那么好，评价标准还有问题。其实说白了也有点个人恩怨而不是就事论事，但在这些问题上你去要求当事人一碗水端平也很难。如同第一个案例所言，科学研究就是会掺杂各种主观情感，但作为旁观者，我们可以从中去学习他们讨论问题的方法，例如 Lior Pachter 教授的论证过程，虽然不如发表文章里那么逻辑完备，但思考步骤都是比较清晰的，而这个过程你在期刊论文中往往看不到，好比你看到的总是对方站在山顶但怎么爬上去的一般都不会写，但有时候这些看似琐碎的步骤却足够让你永远达不到那个高度。&lt;/p&gt;
&lt;p&gt;顺带一提， Lior Pachter 教授的博客上还友情链接了 Andrew Gelman 教授的博客。我想说的是在国外是真真切切存在着通过博客的学术交流的，参与学者的水平也是相当强悍，而且不同于国内科研向公众号或博客满足于对论文的解读，这些博客上更多出现的是一种批判式讨论，而且夹杂了相当重的个人情绪，如果你打算阅读也是需要辨伪存真的，这本身对于提高科研思维也有帮助，所以我推荐高年级本科生、研究生跟科研一线的学者都可以去寻找自己感兴趣领域大牛的博客，省的每次找推荐审稿人都搞近亲繁殖，如果你能从这些火药桶博客里获得正面评价，那么恭喜你，科研对你并不是个坑。&lt;/p&gt;
&lt;p&gt;其实类似的故事还有很多，你可以从这篇文章里出发用关键词去探索。我在前面的文本分析的文章中曾提到越是高端的论文，发表勘误的比率就越是很高，这说明前沿领域的研究不确定性是很高的，思想碰撞也很激烈。如果把社交媒体上的各类花式吐槽也算进去，你会发现科研领域有很多烧脑的故事，各路参与者也从来都不缺名校光环跟牛文加持，阴谋诡计、解释掩饰、爱恨情仇等可能被小心翼翼地埋藏在数据与图表之中，虽然看懂需要比较高的门槛，但也正是这种门槛屏蔽了围观群众，上演一幕幕精彩绝伦但需要自行判断的书剑恩仇。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>唐提保险、区块链与个性化</title>
      <link>/cn/2017/08/07/tontine-insurance/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/08/07/tontine-insurance/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;唐提保险&lt;/h2&gt;
&lt;p&gt;在纽交所还没成立时，活跃在华尔街的经纪人业务并不像今天这样职业化，他们基本接受各种代理业务，其中有一种名为唐提的养老保险特别受欢迎，规则简单到一句话就可以说清：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有参与者投资，一段时间后幸存者获取所有资金的收益。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;举例而言，10个人每人出1000块，约定10年保险期然后把10000块交给投资人打理，许诺收益率6%，这样每年投资人会拿回600块利息，然后刚开始是10个人分每人60块，到了第五年，有4个人过世，投资人收益将由幸存者分，也就是每人100块。当然也可以不约定保险期，直到最后一个人过世为止，在那之前最后的幸存者独享每年600块的收益。至于本金可以在约定时间后留给死者后代或也留给那个幸存者，如果持续将后代也算进来那么理论上这种保险可以持续运行很久，直到某个家族完全消失，然后幸存家族的份额会得到提升。&lt;/p&gt;
&lt;p&gt;这种保险形式非常特别，因为买这个保险完全不需要精算群体死亡率，只要看懂规则愿意加入就可以，甚至完全不需要保险公司来中介。从某种意义上，这是一种混合了分红与赌博的产品，只要你赌自己可以活的长就行了，保险成本的负担从整体转嫁给了个人。事实上美国就有类似的教育储蓄产品，家长拿出一大笔钱放到银行一年后转回本金，然后大量本金产生的利息将作为子女完成高中教育后上大学的分红，但只有考上大学的人才能参与这笔分红。对每个人而言，整体入学率或死亡率都是没有太大意义的，只要你敢赌自己考得上或能活下来，那么就可以参与这个博弈。作为博弈的管理者也毫无精算风险，只要专注于许诺的收益率就可以了，不用担心投保人大量报案导致的高风险。其实诸如意外险、财产险甚至运费险背后都有这种逻辑，那就是让受害者而不是幸存者收获非受害者的本金。&lt;/p&gt;
&lt;p&gt;从历史上看，最早提出概念的Lorenzo de Tonti 是建议用这种方法来为法王路易十四筹集军费，后来英国也采用过这个方法，他们给出的年利率都不低于10%。但100年后国家层面的唐提保险就消失了，不是因为幸存者之间互相残杀引发的道德问题，而是参与这个保险的人平均幸存率太高，结果国家层面许诺的利率又达不到，所以就维系不下去了。其实这些国家本来是给不出10%的利率的，但如果参与者不断有人离世，那么这个利率就可以维持，结果倒霉的是参与者似乎都很长寿（有些是通过欺诈），然后仅是维持许诺利率都让国家不堪重负了，最后无奈把这个保险转成了年金制。后来这个保险跑到了美国成了一种退休金并开始流行，到上世纪初，这种保险的规模甚至达到了美国总财富的7.5%。事实上，当时4%年收益的唐提保险实际上可以对投资者产生10%的收益。不过，到1904年这种形式就被禁止了，主要问题是保险公司利用这一制度短期聚集了大量资金并且20年内不交代用途只考虑分红，同时就是投资者死亡后面临本金无法赎回的道德问题。所有的参与人都希望自己活得长的另一个说法就是所有参与人都希望别人活的短，这甚至给美国文学作品创作提供了灵感。&lt;/p&gt;
&lt;p&gt;事实上，直接形式的唐提保险虽然被禁止，但间接形式的唐提保险一直存在，在以色列集体农场中，一直都有农场共同基金是采用唐提保险形式的，这样作为集体的财产可以一直保留在集体中而不是伴随遗产外流。但最近几年，美国又有人提出唐提保险可能是一种很好的养老金形式，促成这个想法的技术趋势就是区块链。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;区块链&lt;/h2&gt;
&lt;p&gt;区块链算得上一个被IT行业炒烂的概念，跟它一组的还有：共享经济、大数据还有知识付费。我最近读了下中本聪那篇论文，说实话被震撼到了，并不是说看好比特币投机而是看好这个技术的巧妙设计，用同样的构架可以做很多事。下面我就大概说下核心概念：&lt;/p&gt;
&lt;p&gt;如果我打算从市场买一个产品，付款后得到产品，然后作为媒介的钱究竟怎么来怎么去我并不感兴趣。但现在我告诉你存在一个公开的账本，所有交易的细节都可以广播后从所有人的账本上查到，这样你可以追踪你的货币到其铸造出的那一刻，而货币的铸造则是一个对大量交易打包封装成区块过程的验证奖金，最先验证成功的区块发出广播，所有的账本都会加入这个区块。也就是说，这是一个分布式造就的整体权威而非中心化的，如果活跃账本中多数先收到验证成功的广播，那么这就是主链区块。同时，基于数学特性，几乎不可能有人能小范围打包多个区块来声明更多的货币，因为这样即便延长了小范围的区块链但也不能被其他人认可，后果就是那个链成为废链。同时，所有交易是基于地址的，而这个地址根本不需要权威验证身份，也就是同时做到了匿名与公开，所有人从公共账本里看到的都是地址间的交易，但地址属于谁是无法得知的，因为这样的地址可以无限生成且互相交易，一换手就没办法查了。这也就是为什么今年上半年勒索病毒要求比特币作为赎金，我们也能基于地址查到多少人交了赎金，但就是不知道地址主人是谁。&lt;/p&gt;
&lt;p&gt;那么这跟唐提保险有什么关系呢？唐提保险当初被人诟病很大程度是因为这可能是个熟人间的保险，大家相互认可才签协议，举例而言你不想跟个婴儿签共同的唐提保险，其大概率会成为最终的幸存者，但如果大家相互认识就存在道德问题了，那就是有些人可通过谋杀另一些人来获利。但如果用区块链技术，我们可以匿名参与唐提保险，而由于你是一个区块链用户，我们可以追踪你在区块链上的活动来确认你是否符合参与保险的条件，例如在区块链设计中加入每天步行数上传或心率数据上传的设计，那么只要你活在可追踪的区块链中，唐提保险就会不断分红直到你的区块链活跃度为零。这样道德风险就几乎为零了，因为所有人都不知道谁在参与。同时唐提保险可以设计为无数条链，每条链人数固定，只有同一条链上的人可以凭借自己特有的地址不断收到作为分红的代币，只要保险公司或代币市场可以将其转为法币，那么这个游戏就可以悄无声息的运转，即使你知道别人也在游戏中但因为可能不在一条链上而没有利益冲突。其实这个机制可以很好的回答另一个问题：大数据下的个人价值。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;个性化&lt;/h2&gt;
&lt;p&gt;如果你的行为数据可以被收集用来预测寿命，一个生活习惯良好无不良嗜好的人基本不需要买保险或价格很低，留个遗嘱就可以了；而那些抽烟且有酒驾记录的人想买也会因为价格过高而无法负担。这其实对行业是不利的，保险行业里想买的需要那些不需要买的来共同承担风险，但如果我通过自测发现不需要买，那我很有可能就不加入，结果保险保的都成了高风险的人，最后也会入不敷出。技术上其实越来越允许数据持有者了解并预测自己的行为，巴拉巴西在《爆发》中提到，93%的人类行为可以预测，配合精准医疗技术，未来很多人可能很轻易的知晓自己的行为与最佳生存模式，这种情况下的危机就是人不会主动参与共同抵御危机。举例而言，如果你不可能患罕见病就无所谓购买相关保险，但罕见病患者则可能成为商业规则下被忘记的一批人，无利可图所以没有人关心，市场就是这样。越是精准的营销策略其实越不需要营销，因为你想让人买而那个人其实本来就想买，找到人就OK了。但这事实上会造成不平等，你的价格可能是量身定做的，但往往却是无法负担的。这样的不平等在我看来是可以通过技术来解决的。&lt;/p&gt;
&lt;p&gt;在数据时代没有隐私，但隐私的价值却是变化的。如果存在一种算法，可以综合个人各种风险就可能解决信息不平等。举例而言，A喜欢极限运动但身体健康，B生活规律但带有癌症基因，也就是说A需要意外险不需要重疾险，B需要重疾险而不需要意外险，如果两种保险去卖估计都只能卖出去一份且价格偏高，但是如果A跟B是一组去买两人都可以得到保险且由于卖出两份边际成本会下降则价格不高。如果把一个人一生的动态风险不断打包整合到一个区块链上，综合风险类似的人出现在同一条链上而且不同区块链可以动态转接进行风险控制，保险公司收费后只要在不同风险的链上定期注资，概率上一定有人收益，这样所有人可能都乐于加入这样一个计划，只要你不是一个全面极端的人，在一点上的极端是可以被另外多点的不极端而中和掉。运行这样一个区块链系统同样并不需要实名，只要你乐意加入就够了。你总是会不断出现在跟你综合风险类似的人的链条上，由于你并不清楚对方实际哪方面有风险，实际我们可以引入唐提保险机制，这样可以保证参与度与整体系统的稳定性。这个机制的优点在于每个人即使对自己一清二楚也总能找到一个跟自己类似但细节完全不同的人进行生存游戏，每个人在算法公平下其实也实现了共同保险，而且算法够强的话没有人知道你具体是谁。&lt;/p&gt;
&lt;p&gt;本着个人谋求私利的心降低整体风险，这是对算法技术的新要求，没必要寄托在慢半拍的政治、经济、法规跟伦理上，新的技术与博弈机制可能最终会解决社会风险问题。而个人隐私其实也成了未来每个人的生存筹码，毕竟天天在家里宅着看视频就不太可能被毒蛇咬到，那就找个野外科考队员来分担你II型糖尿病的风险吧，虽然你的数据被泄漏了，但也从某种方面证明了其价值，当这一切被配对整合后，大家都应该可以过自己想要的生活而不用过多担心风险。&lt;/p&gt;
&lt;p&gt;技术产生的问题还是要技术来解决，反正魔盒已经开启。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;参考文献&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/gp/product/B00WMRPWVG/ref=as_li_ss_tl?ie=UTF8&amp;amp;btkr=1&amp;amp;linkCode=sl1&amp;amp;tag=kitcescom-20&amp;amp;linkId=0d2632884eb0eff4608787c0f4f3d660&#34;&gt;King William’s Tontine&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://bitcoin.org/bitcoin.pdf&#34;&gt;Bitcoin: A Peer-to-Peer Electronic Cash System&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kitces.com/blog/tontine-agreement-instead-of-annuity-lifetime-income-mortality-credits-and-book-review-of-milevsky-king-williams-tontine/&#34;&gt;Could A Tontine Be Superior To Today’s Lifetime Annuity Income Products?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2017/03/24/business/retirement/tontines-retirement-annuity.html?_r=0&#34;&gt;When Others Die, Tontine Investors Win&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>从真空里的球形鸡到社会财富分配（二）</title>
      <link>/cn/2017/07/29/%E4%BB%8E%E7%9C%9F%E7%A9%BA%E9%87%8C%E7%9A%84%E7%90%83%E5%BD%A2%E9%B8%A1%E5%88%B0%E7%A4%BE%E4%BC%9A%E8%B4%A2%E5%AF%8C%E5%88%86%E9%85%8D%E4%BA%8C/</link>
      <pubDate>Sat, 29 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/07/29/%E4%BB%8E%E7%9C%9F%E7%A9%BA%E9%87%8C%E7%9A%84%E7%90%83%E5%BD%A2%E9%B8%A1%E5%88%B0%E7%A4%BE%E4%BC%9A%E8%B4%A2%E5%AF%8C%E5%88%86%E9%85%8D%E4%BA%8C/</guid>
      <description>&lt;p&gt;前面说到在一个理想自发交易的稳态环境下，社会财富的分配会符合小球模型里的玻尔兹曼分布：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E) = A e^{-E/E_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在这里，&lt;span class=&#34;math inline&#34;&gt;\(E_c\)&lt;/span&gt; 代表了温度，可以对应经济系统里总体财富的平均水平；&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; 代表了个人财富水平。这是个描述概率的指数函数，个人财富越多，越不可能出现。不过其实更多人熟悉的是财富分配的二八法则，也就是帕累托-齐普夫定律，那么这个定律又是什么鬼？跟玻尔兹曼分布又有什么区别呢？&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;二八法则&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>从真空里的球形鸡到社会财富分配（一）</title>
      <link>/cn/2017/07/24/boltzmann-distribution-1/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/07/24/boltzmann-distribution-1/</guid>
      <description>&lt;p&gt;曾经有个笑话吐槽物理学家研究鸡不下蛋问题时首先假设了一只存在真空中的球形鸡，进而认为理工类研究过于简化了现实世界，再进而推导出了一种不修边幅、不谙人情世故且高智商低情商的理工科书呆子刻板印象。就我个人生活经验而言，这种看法倒也没啥问题，对，就是没啥问题，你没看错。&lt;/p&gt;
&lt;p&gt;但是（说“但是”前的都是废话–马克·于瘟），当今学术研究的一个大趋势就是用理工科的思维研究社会自发行为，我曾经尝试&lt;a href=&#34;http://blog.sciencenet.cn/blog-430956-869450.html&#34;&gt;用马尔可夫过程解释社会阶层流动&lt;/a&gt;，但其实很多理工科出身的科学家是很严肃地研究这类问题的。我们将会看到一个松散假设下的物理模型是可以解释很多（但绝不是全部）自发的宏观社会行为的。在某些视角下，我们也许就是个真空中的球状社会信息综合体。首先，我们从具象到抽象，来了解下一个统计物理学的概念——玻尔兹曼分布。在后续的文章中我们会用这个分布来探索下社会行为，特别是经济行为。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为的抽象&lt;/h2&gt;
&lt;p&gt;社会行为其实就是个体间的相互作用，在物理学视角下，这个个体间相互作用有四种（但我知道多数人不关心这个），简单说就是物质能量交换。而社会经济行为视角下，这个相互作用可以看成价值流动，通俗一点就是钱。钱可以在流通成本很低的现代信息社会下快速在社会个体间转移，在物理学视角下可看作能量在不同粒子间的传递。那么类比一下我们会发现，如果我想知道一个社会整体的财富分配，我们可以类比到一个装满带有能量的小球的空间去观察小球的能量分布。这里有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;社会个体（人、公司、团体）-&amp;gt; 空间里的小球&lt;/li&gt;
&lt;li&gt;财富（钱、资产） -&amp;gt; 能量&lt;/li&gt;
&lt;li&gt;价值交换（买卖行为、资本流动）-&amp;gt; 小球间能量交换&lt;/li&gt;
&lt;li&gt;社会个体的财富分配 -&amp;gt; 空间里的能量分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里我们需要一些假设，首先我们考虑的是一个热力学平衡，也就是说，空间里能量总和一定，也就是要是个封闭空间，同样，社会里总财富在某个时间点也是稳定的。其次，我们想知道的是财富分布或能量分布，那么我们就要有不同财富区间与能量区间来构成分布，这里我们用钱数或能量值来划分，拥有钱数或能量在同一区间的社会个体或小球在分布中是一样的，无法区分，也是等概率的。最后，能量交换或价值交换要达到热力学平衡，这个在小球模型里很容易实现，但社会实际状态只能近似认为达到，这意味着交易是随机的且充分的，后续我们会修改这个限制让模型更符合社会实际。&lt;/p&gt;
&lt;p&gt;根据上面的类比与假设，目前我们大概可以用一堆小球去模拟一些稳态社会行为了，这个抽象过程是大多数理工类科学研究的起点，不然一上来就过于复杂很难讨论。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;封闭空间里的赋能小球&lt;/h2&gt;
&lt;p&gt;好了，目前我们先不去想复杂的社会行为来考虑小球。在一个封闭空间里，小球的能量总和是固定的，但能量范围却可以很广，小球数也可以很多，那么我们再简化下，就考虑3个球总能量6，能量区间就会有7个离散数值：0，1，2，3，4，5，6，7。那么我们想得到的是宏观状态下能量分布，我们首先来个原始方法：数。&lt;/p&gt;
&lt;p&gt;首先要数下有多少个宏观态。例如有一个球上有6的能量，其余的球能量都只能是0。我们可以列出下面的7种宏观态：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Macrostate.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;然后我们考虑每一种宏观态下有多少种微观态，因为球有3个，区间有7个，我们可以看成一个排列组合问题。微观下，一共有&lt;span class=&#34;math inline&#34;&gt;\(3!\)&lt;/span&gt;种排列组合，但在基本假设中，我们认为属于同一宏观状态的球是一种，那么这些情况就要被排除掉。例如在宏观态7中，虽然球可以有6种排列，但因为都在1个能量区间，实际只有1种微观态。也就是说，如果我们有N个球，排列组合上看虽然有&lt;span class=&#34;math inline&#34;&gt;\(N!\)&lt;/span&gt;种组合，但实际由于N个球中有n个球在区间i，我们要从总的排列组合中除掉这种组合。也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\# microstates = \frac{N!}{n_0!n_1!...n_i!}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这样我们可以得到共计28种微观态，在这里所有微观态的出现概率是一样的：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;State.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Microstate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;total&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在宏观态1中所有微观态出现的可能性为&lt;span class=&#34;math inline&#34;&gt;\(3/28\)&lt;/span&gt;，其中针对能量为0的区间贡献&lt;span class=&#34;math inline&#34;&gt;\(2*3/28 = 0.214\)&lt;/span&gt; 个小球，而宏观态2中对能量为0的也贡献&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球，同样宏观态3中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球而宏观态4中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.107\)&lt;/span&gt;个小球，这样会有&lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;个小球贡献给能量为0的区间，同样我们可以得到其他能量区间上有多少小球。这个小球的能量分布就是我们打算求的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-07-24-boltzmann-distribution-1_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在这个演示里，能量分布似乎跟数目是线性的，但是如果我们把系统放宽，加入更多的球跟能量，这条曲线应该是指数分布的且指数为负。当然我知道你们不信，这里我们用 Eisberg &amp;amp; Resnick 在量子物理教材中描述来进行一个理论推导。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;能量分布的数学形式&lt;/h2&gt;
&lt;p&gt;如果我们看到两个小球，这两个小球来自同一个能量分布，那么一个小球其出现在&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_1)\)&lt;/span&gt;，出现在&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;能量区间上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_2)\)&lt;/span&gt;。我们考虑一个特殊情况，一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;，发生这种情况的概率应该是两个概率的乘积，也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1) \times f(E_2) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后，我们考虑所有宏观态中能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况，因为我们进行了区间划分，所以能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况只出现在所有能量和为这个数的小球组合里。由于我们之前假设了所有微观态是等概率的，那么出现这个能量和的小球组合方式也是等概率的。那么在某种程度上，会有下面的公式成立：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1)+f(E_2) = h(E_1+E_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们可以认为上面那个特殊情况，也就是一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;的概率是属于&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;这个能量分布的，那么根据第一段的推理出现这个情况时的概率应该是乘积。也就是两个能量和的出现概率既是两个独立能量区间概率的和又是乘积，那么满足这个条件的数学形式只能是指数形式。那么我们可以得出，在某个能量区间上小球的概率密度函数应该是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E) = A e^{-E/E_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里面的常数&lt;span class=&#34;math inline&#34;&gt;\(E_c\)&lt;/span&gt;是描述独立于小球的系统变量，玻尔兹曼经过推导得出这一部分是系统温度（总能量）的线性函数，所以我们就有了玻尔兹曼分布的数学形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_b(E) = A e^{-E/kT}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这个分布描述了在封闭系统内不同能量的小球概率分布形式，简单说就是能量高的概率很低，而能量低的概率比较高。这个现象是客观存在的，但同时由于存在量子效应限制，有些能量段上的小球是无法稳定存在的，所以能谱分布实际并不连续，不过这就是另一个故事了。那么这个符合粒子运动的物理模型跟社会行为有什么联系呢？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为&lt;/h2&gt;
&lt;p&gt;在一个达到平衡的社会经济系统中，我们对小球的假设基本是符合现实的，能量代表财富，那么很直观的现象就是这个社会的财富分配天然就不是平均的，会基本符合指数分布。也就是说，总是少数人富有而多数人穷。那么有人会反对说发达国家都是纺锤形的，明显模型有问题。没错，发达国家可以是纺锤的，但如果把全球看成一个缓慢增长的经济体，一个区域特别稳定富有只可能是牺牲其他区域的财富增长才能实现。所谓共同富裕，在全球封闭尺度下是无法自发形成的。这里看起来跟自由市场理论有矛盾，在自由市场下，看不见的手可以调节保证一个健康的经济发展，但在小球模型里经济并不发展，只是自由分配与交易。不过换句话，在经济停滞后的交易行为应该会导致更极端的财富分配。至于说自由市场里的自发交易可以促进合理的财富分配，我觉得比较扯，更靠谱是富有者所承担的社会文化压力，要不然亚当·斯密也不至于再去写一本《道德情操论》。&lt;/p&gt;
&lt;p&gt;此外，请注意关键词：自发。也就是说不进行法律政策干预，但是如果博弈规则发生改变，那么物理系统就不那么好使了。另一个关键词是稳态，也就是如果社会整体财富增加了，那么即使分布不变，所有人生活的幸福指数也会提高。而增加社会整体财富的方式在最近这一百年的主要表现方式是科技革新，有意思的是，目前可以让财富分配更不平等的最主要动因也是科技的不平等。&lt;/p&gt;
&lt;p&gt;在接下来的两篇中，我们先看看到底这个世界的财富在不同的小球模型中会有如何的分布特征，然后再看看社会财富实际是如何分配的而物理学规律能给我们什么样的启发。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real high-throughput for LC/GC-MS</title>
      <link>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</guid>
      <description>&lt;p&gt;If someone want to know whether some compounds exist in certain samples. He always need to make pretreatment for that samples and make them into a little vial for analysis in sophisticated instruments like mass spectrum. If you want to analysis many components in one sample, you also need some separation methods like gas/liquid chromatography. How about analysis multiple samples and multiple compounds in a single run?&lt;/p&gt;
&lt;div id=&#34;pseudo-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pseudo high-throughput&lt;/h2&gt;
&lt;p&gt;If you use LC/GC-MS to analysis samples, all the efforts for high-throughput would be limited at the injection step. You could arrange 96-well plate for analysis. But wait, ONE BY ONE. If some short-live compound could survive in the pre-treatment, they would disappear in the auto-sampler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-mass-spectrum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to Mass Spectrum&lt;/h2&gt;
&lt;p&gt;Actually, similar issue happened when we perform Multiple Reaction Monitoring(MRM) to collect the intensities from different ions. If the detector could only measure one ion at one time, mass spectrum simply use a high frequency scan like 50ms or 20ms for one ion and re-construct the time profile by smooth the points into a line. To my knowledge, 15 points would fit a bell curve well for one peak with smooth. OK, if the peak width is 15s, for each ion we only need 1 scan per second as shown below. OK, that means if our instrument could reach 10ms per scan per ion, we could monitor 100 ions at the same time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-09-real-high-throughput-for-mass-spectrum_files/figure-html/sim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then how about full scan, some detectors like orbitrap and tof-tof could monitor a full scan for all ions at almost the same time. Common high resolution mass spectrum could collect more than 10 spectrum per second. If the compounds could show peaks’ width larger than 15s, we could actually collect 10 spectrum from different samples, which is the real high-throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real high-throughput&lt;/h2&gt;
&lt;p&gt;All we need to do is to synchronize the injection and mass spectrum scan. I have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this solution, we injected 6 samples at the same time. The column should be the same. Then when the samples reach MS part, they were arranged into one sequence. All we need to do is to ensure every six full scan on the mass spectrum could meet six identity sample from the LC/GC part. Then MS could collect and rebuilt six samples’ retention time - mass profile and output six data set for those samples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with Multiple channels&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this way, we need some controls on pumps to on/off when one channel’s sample get into MS for full scan. Or we could have cells to guide the samples into the slit before the lens. This option is better to avoid the cross contamination. However, we need re-design the MS ion source for multiple channels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single column with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you do not need to modify your current instruments. All you need to do is the injection of the samples by sequence without considering the full separation in former sample. However, you need a lot of efforts to deconvolution. Since it’s the same column, the separation for most ions should be same. You could build a model to capture such patterns and separate the samples by those patterns. In such way, the batch effects could be minimized. However, the requirements for data mining are maximization. I like this way.&lt;/p&gt;
&lt;p&gt;I think in the near future we would find the real high-throughput LC/GC-MS. Those devices would short the analysis time between sample collection and data acquisition. MS-based scientists could reach more interesting findings with REAL high-throughput.&lt;/p&gt;
&lt;p&gt;Happy explore!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text mining for top academic journals</title>
      <link>/en/2017/07/07/text-mining/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/07/text-mining/</guid>
      <description>&lt;p&gt;Text mining is often used to find spatiotemporal trends in news, government report and user generated contents in SNS websites. We could make sentiment analysis to find the real opinions of netizens. Also we could track the popularity of certain phases and find the connections among them. Such technique might also be useful for scientists or researchers to read papers.&lt;/p&gt;
&lt;p&gt;Scientists or researchers’ daily life is immersed by a lot of literature. Most of the them are only focused on limited area in certain subjects. However, a modern scientist should always know what had happened in all of the other subjects. Some techniques used in other research might inspire your research. The only problem is that you need a lot time reading top general journals like Science, Nature and PNAS. Wait, we actually do not need to know the technique details and all we want to know is the patterns in those journals. Well, text mining would help.&lt;/p&gt;
&lt;p&gt;The main advantage for text mining in academic journals is that academic papers always share same structures in one journals. Public academic databases such as PubMed or Google scholar could always show you the structured records for papers such as journal, authors, title, published dates and even abstracts. We could directly fetch those data and save in database. I developed scifetch package to get those data from PubMed. This package would support Google scholar, bing academic and baidu xueshu in the future. Actually I support PubMed in the first version because they have a user-friendly API and I could connect such pipe with xml2 package in a tidyverse way. Besides, easyPubMed package is also a good package to extract such data from PubMed.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data collection&lt;/h2&gt;
&lt;p&gt;Here I collected the information from all the papers published in SNP i.e. science, nature and PNAS in the past three years as xml format and clean them into a dataframe for further text mining. The search grammar could be find from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK3827/&#34;&gt;NCBI websites&lt;/a&gt; and a cheat sheet here.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/cheatsheetpubmed.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;There are 26559 papers and I will use such data for text mining. PubMed has a limitation for 10000 records per query. So we need to fetch the data multiple times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 26,557 x 7
##                          journal
##                            &amp;lt;chr&amp;gt;
##  1                        Nature
##  2                        Nature
##  3 Proc. Natl. Acad. Sci. U.S.A.
##  4 Proc. Natl. Acad. Sci. U.S.A.
##  5                       Science
##  6                       Science
##  7                       Science
##  8                       Science
##  9                       Science
## 10                       Science
## # ... with 26,547 more rows, and 6 more variables: title &amp;lt;chr&amp;gt;,
## #   date &amp;lt;date&amp;gt;, abstract &amp;lt;chr&amp;gt;, line &amp;lt;int&amp;gt;, time &amp;lt;dttm&amp;gt;, month &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;description-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description statistics&lt;/h2&gt;
&lt;p&gt;Firstly, let’s see the papers by journal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we could check the high frequency terms in the title and abstract of these papers.&lt;/p&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordtitle-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As top journals, one of the most obvious features is that they all need correction and reply. With high influences, those journals would be the best place to discuss the leading edge techniques and findings. However, Science likes U.S., glance and paleoanthropology more while Nature and PNAS like tumor to be used in the titles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordabs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, when we focused on the abstracts, something interesting happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They all focused on tumor while Nature use ‘tumour’ as a journal from U.K.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nature’s title and abstracts look similar while Science’s title always use different terms compared with their abstracts. Maybe Science’s authors like to be clickbaits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PNAS’s authors use a lot of abbreviation in their abstracts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Those top journals all like tumor, behavior and modeling and now you know how to pick up a topic to be published.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;temporal-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Temporal trends&lt;/h2&gt;
&lt;p&gt;Here we use logistic regression to examine whether the frequency of each word is increasing or decreasing over time. Every term will then have a growth rate associated with it.&lt;/p&gt;
&lt;div id=&#34;title-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find some trending terms like CRISPR, gut, and corrigendum are ‘promising’. However, some topics like Ebola, Hiv and cell differentiation would leave us. Another interesting trending is that the names of certain subjects is disappearing in those top journals like biology, chemistry, neuroscience, ecology and policy. Maybe most titles would like to focus on specific topics or certain problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Now let’s review the temporal trends of terms in abstracts during the past three years by months.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to find a clear pattern in growing words in abstracts. Maybe the abstracts focused more on technique details. However, shrinking words like viral, gene expression and pathway showed clear trends. Meanwhile, we could find some words like suggests, previously and production are discarded by the top scientist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-among-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship among words&lt;/h2&gt;
&lt;p&gt;N-gram analysis could be used to find a meaningful terms in those papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/bigramt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/biabs.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Well, climate change, transcription factor, stem cell and cancer would always be the favorite bigrams in the titles of top journals. For the abstracts, cell related topics such as function, protein and expression are always preferred. Anyway, life science is always the center of trending sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic modeling&lt;/h2&gt;
&lt;p&gt;Relationships among words could show us some trending. However, we could employ topic modeling to explore the topics as a bunch of words in the abstracts of those top journals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./images/tm.png&#34; /&gt; This topic model showed topics like climate change, virus infection, brain science and social science are other important research topics other than life science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment analysis&lt;/h2&gt;
&lt;p&gt;Text mining could also be used to find the sentiment behind those journal papers or the customs using certain words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/SA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, I think the afinn word list for sentiment analysis is not suitable for scientific literature. Some words is actually neutral in academic journals. If someone could develop a specific word list for scientists, we might find something ignored by the writing lessons on campus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here, I demo some basic text mining results for top academic journals. Just like Google trends might predict the popularity of flu, text mining for academic journal might be a good tool to reveal unknown patterns or trends in certain subjects or top journals. Besides, we could also find unique usages of some words and some tones behind the journal. Besides, such methods might work better than impact factor or H index as the evaluation tools for certain researcher, journal or institute in a dynamic view. The most attractive thing is that every scientist could use this tools through open databases and find their own answers. This is the best benefits from information era.&lt;/p&gt;
&lt;p&gt;You might read this excellent on-line &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;book&lt;/a&gt; and David Robinson’s &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;blog&lt;/a&gt; to make more findings.&lt;/p&gt;
&lt;p&gt;All the R code for this post could be found &lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/en/2017-07-07-text-mining-for-trends-in-top-journal.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>文献阅读的文本分析流派</title>
      <link>/cn/2017/06/16/nlp-literature/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/06/16/nlp-literature/</guid>
      <description>&lt;p&gt;读文献是科研人员的基本功，一方面是了解学科发展，另一方面更现实一点，就是为了发文章。起步阶段读论文一般是模仿与学习，但到了中后期如果你的视野不够开阔，很容易陷入到安全区陷阱，认为自己做自己那一小摊就挺好，其实很有可能大浪过来，全军覆没，说直白点就是申不到钱，课题与项目运转不下去，思路也会枯竭。当你去开学术会议时，那些大会报告的报告人的开场总有个全局概览的视野，这种评论是需要经验去堆的，但其实也挺虚的：你回头去看容易知道哪里有坑哪里有丘，但身处时代浪潮之中是不太容易感知趋势的。&lt;/p&gt;
&lt;p&gt;但传统基于核心关键词的检索跟全局观是本质相悖的，核心关键词往往限制了内容，虽然有利于聚焦但不利于发散与概览。不过当前文献数据空前开放，如果你有类似全局视野问题，是可以自己探索的。这里要用到一个名为自然语言处理（NLP）的工具，简单说就是我不去看单篇文献或荟萃分析，而是通过语义关系探索大量文献中的潜在模式，进而找出热点。今天我用pubmed这个免费的文摘数据库来做个演示，探索下科学研究的整体前沿，结论不一定对，但方法思路如果你能掌握并举一反三，会有发现新大陆的感觉。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据获取&lt;/h2&gt;
&lt;p&gt;数据获取思路是这样的：如果想知道整体前沿，最需要的是综合类期刊，全文的数据量我的笔记本也跑不了，就考虑摘要，这样也过滤了那些没有摘要的评论与观点，更多关注研究性论文。期刊选择为综合类的《科学》、《自然》与《美国科学院院刊》，收集2016年一整年的论文摘要，用&lt;code&gt;easyPubmed&lt;/code&gt;包来搜索并整理成相对干净的数据集。这里我只收集了题目、摘要、出版期刊与日期进行文本数据挖掘。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,815 x 6
##                                                                          title
##                                                                          &amp;lt;chr&amp;gt;
##  1 A high-throughput small molecule screen identifies synergism between DNA me
##  2                       2017 sneak peek: What the new year holds for science.
##  3                            Wolf transplant could reset iconic island study.
##  4 Scientists in Germany, Peru and Taiwan to lose access to Elsevier journals.
##  5                                                   How scientists use Slack.
##  6                 Saliva protein biomarkers and oral squamous cell carcinoma.
##  7 Reply to Galvão-Moreira and da Cruz: Saliva biomarkers to complement the vi
##  8         Cell morphology drives spatial patterning in microbial communities.
##  9                 Deborah S. Jin 1968-2016: Trailblazer of ultracold science.
## 10 Autophagy wins the 2016 Nobel Prize in Physiology or Medicine: Breakthrough
## # ... with 8,805 more rows, and 5 more variables: abstract &amp;lt;chr&amp;gt;,
## #   year &amp;lt;chr&amp;gt;, month &amp;lt;chr&amp;gt;, day &amp;lt;chr&amp;gt;, jabbrv &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;发文量&lt;/h2&gt;
&lt;p&gt;首先我们先看看着三份期刊的发文量：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这三份期刊里，PNAS发文量最大，占总数一半。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;高频词&lt;/h2&gt;
&lt;p&gt;然后我们看一下各期刊的前十大摘要高频词：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/abs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里解释一下，如果我们单纯寻找高频词其实这几个期刊都应该差不多，但这里我们用的是TF-IDF来加权筛选，这个加权不严谨的说就是这个词出现在该期刊的词频与出现在所有期刊词频的比例，通过这个值我们可以找到单个期刊比较重要的词。我们可以看到肿瘤与行为均出现在三个期刊的十大关键词中，推测相关研究应该是去年的重点。此外，《自然》与《美国科学院院刊》都出现了模型这个词。就特色而言，《自然》去年更关注造血过程、信号传递与衰老问题；《科学》杂志则关心磷酸化、spo11蛋白与火山口还有小尺度问题；《美国科学院院刊》主题特色不算明显，但比较喜欢强调研究重要性。&lt;/p&gt;
&lt;p&gt;如果我们只考虑题目里的文字呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/title-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们可以看出，《自然》上的论文题目跟摘要内容契合度比较高；《科学》上论文题目喜欢出现中美的国家标签；《美国科学院院刊》看意思题目里专业名词比较多。此外，三份期刊的题目里都出现了勘误，这倒是前沿高影响力期刊的特点：容易被质疑。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;词关系&lt;/h2&gt;
&lt;p&gt;看完整体你应该想到，单个词并非孤立，那么这些词之间会不会有相关性呢？这个问题我们也可以用NLP工具来研究：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-2.png&#34; width=&#34;672&#34; /&gt; 其实这个技术更常见，平时你用的输入法就实现去考察一些字词的关系，然后让其出现的排序更符合常识。这里我们可以看到，从题目里我们能看到气候变化、干细胞以及前面提到的勘误问题。从摘要里我们则会发现大多数是生物相关的主题，也就是前沿科研应该是生命科学在导向。但到目前为止我们都是把这一些文本当成一个整体，但科学是分科的，也就是有不同的主题，此时我们就要用到主题模型来探索去年前沿科研关注的主题分类。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;主题模型分类&lt;/h2&gt;
&lt;p&gt;所谓主题模型，就是通过探索字词间内部关系对文本进行分类的模型，举例来说某个潜在的主题包含7个关键词，如果某篇文章命中6个，那么这篇文章大概率就属于这个潜在主题。当然，现实生活我们并不知道这些潜在主题会是什么，但通过隐含狄利克雷分布，也就是LDA方法我们就可以去探索结构，然后去拟合实际经验。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/topic-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;从上面我们可以看出，有些探索出来的主题大概我们知道是哪个领域的，有些则属于误判或者说界限不明显的综合领域，这说明跨学科研究正在崛起。其中，我能识别出来的主题大体有癌症、脑科学、病毒、社会行为、基因组、膜蛋白结构、气候变化、进化、动态系统、材料。总体来看，细胞生物学与分子生物学还是主流，但病毒、气候变化等问题导向的学科也在发展。其实也可以直接分析10年的时间变化趋势，不过这个就留成课后题吧（其实是我个人电脑跑不动）。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;情感分析&lt;/h2&gt;
&lt;p&gt;一般认为科研人员都是比较乐观的，但其实文字背后究竟是否乐观可以用文本的情感分析来回答。这个分析的原理就是事先找个标注过情感的语料库，然后通过语料库与词频来分析具体文本的情感倾向性。正常这个语料库是要自己根据语境去构建的，例如商品的好评差评，但作为资深懒汉，我直接用了现成的AFINN语料库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sen-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果基本符合乐观为主的预期，不过按说有些词在科研中属于中性词，我们可以通过这个分析来考虑论文写作的用词方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;其他&lt;/h2&gt;
&lt;p&gt;其实这只是一个很初步的分析，我甚至没用用到引用与被引用的关系，也没有考虑作者与研究机构的时空分布特征，但类似这样的文本分析应该是一个现代科研人员所具备的属性。这种分析的好处在于你不是在采样，而是直接分析所拥有的整体，也就几十兆的文本量，如果你电脑跑得动，把十年二十年的文献沿革都可以概览一下，这是这个时代给我们的红利，不要白不要。&lt;/p&gt;
&lt;p&gt;你可以研究一个大牛几十年的论文发表来发现其独到的眼光；也可以针对某个期刊挖掘其关注点的变更；还可以构建自己认可的课题组的文献库，通过其发表内容探索同行那些自己都没意识到的行为改变。这个时代学科内的经验贬值飞速，很多东西没必要闭门造车慢慢悟，利用开放数据的便利性你可以很快了解整体学术动态，这样不至于随波逐流。更麻烦的是如果你不懂而别人懂，那你将很容易体会到别人眼神中的怜悯，做一个好奇心使然的科研人员，现在起步从来都不晚。&lt;/p&gt;
&lt;p&gt;更重要的是，这类技术本质是让你满足好奇心的，你可以用这个来了解社会，例如纽约时报就给个人提供API，你可以看看其对川普用词风格的变化；为什么最近比特币搜索指数集中在拉美？欧洲吸引难民究竟是政治正确还是劳动力人口不足？不要等着看新闻来指导自己，要学会发现生活中的闪光点；不要通过键盘上情感喧嚣来面对社会，要用键盘甚至语音编程（我果然很自然的想到了最懒的方法）从繁复的公开数据中挖掘趋势；不要总是等着大牛来带，在未知的领域人人都可能成为大牛，你需要掌握一些实现方法而已，你甚至不需要太了解算法细节（会忘，比如我），但要有自己的兵器库随想随用。你不需要带着目的性去学，这说到底只是一种生活方式，你变强了也秃了的可能性是存在的（你能否感到我最近在看漫画）。&lt;/p&gt;
&lt;p&gt;本文实现代码可见我的&lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/cn/2017-06-16-nlp.Rmd&#34;&gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;想要了解世界吗？想要的话可以全部給你，去找吧！我把所有的线索都放在互联网上了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;参考文献&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://tidytextmining.com/&#34; class=&#34;uri&#34;&gt;http://tidytextmining.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection of open source software platform for metabolomics or lipidomics</title>
      <link>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</guid>
      <description>&lt;p&gt;Recently I reviewed some platforms for metabolomics or lipidomics and tried to find out which one is currently available to my demands. I knew some instrumental company supplied software or solutions for metabolomics and lipidomics. I just disliked them since they concealed too many details about data processing which made omics studies as a magic box like Artificial Neural Network. It gains nothing for scientist to get insights from the data and you might only follow the workflow defined by the company. I read some papers using those kind of software and felt the authors know little about what they performed. Data analysis for metabolomics or lipidomics is a systems engineering. If someone really want to use this tools, just choose a open source platform and inspect the code when you needed. Otherwise, your work could be replaced by AI someday. It’s not a joke. Here is a summary for the selection of mass spectrum based metabolomics or lipidomics software platform.&lt;/p&gt;
&lt;div id=&#34;principles-for-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principles for selection&lt;/h2&gt;
&lt;p&gt;A decent solution should have functions covering the following required functions or features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open source: as I said before, researcher could benefit a lot from open source community. Based on your experiences, Java, matlab, python and R would be your choices. I liked R most while python might dominate everything in the further.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data input/output: this work could be done by msconvert from &lt;a href=&#34;http://proteowizard.sourceforge.net/tools.shtml&#34;&gt;proteowizard&lt;/a&gt;. I preferred to install msconvert directly on the instruments and convert the vendor files into mzML or mzXML files to perform further analysis. I just dislike Windows. On the other hand, some software from the instruments could output CDF files, which is also nice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turn the Raw data into mass-retention time matrix: single data file from mass spectrum would always contain matadata and profile data. The former container has records for that data and the latter were always full scans of mass spectrum indexed by retention time. Then you need to map such data into a matrix because the following peaks picking and alignments always based on algorithms for matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peaks picking: you also need functions to pick up the peaks from mass-retention time matrix. Functions like centWave or isotope-based algorithms would help to find the peaks. Sometimes you need to bin the matrix first before peak picking and such requirements should be carefully checked. Anyway you could just use one function to perform peaks picking in most software. The output of this step would be a peak list with intensity, mass and retention time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Retention time correction: when you process more than one data file such as technique replicates, the drift of retention time would make the final peak list contained replicated peaks. Someone use certain peaks to corrected the data, while I prefer some global similarity analysis based algorithm to undertake this task. However, some software might add one step to group the peaks before and/or after retention time correction. Anyway, the output of this step is the peak list from one group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Batch effects correction: this is another issue from analysis procedure and batch effects would also bury signals from noise. Researchers from analytical chemistry always use random experiment design and pooled sample to check if the batch effects exist. However, I think the most important things were correction. Just treat a series of samples suffered batch effects and use some statistical analysis to correct them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical analysis: when you get the peaks list, you could perform the following supervised or non-supervised analysis to find bio-markers or data pattern. I think &lt;a href=&#34;http://www.metaboanalyst.ca/&#34;&gt;Metaboanalyst&lt;/a&gt; could satisfy most researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Annotation or identification: the peaks annotation is also important. &lt;a href=&#34;www.hmdb.ca&#34;&gt;HMDB&lt;/a&gt; and &lt;a href=&#34;http://www.lipidmaps.org/&#34;&gt;LIPID MAPS&lt;/a&gt; would be the basic version for mapping peaks to compounds. In most cases, MS/MS data would be used. I think most of MS/MS dataset could be separated handled by &lt;a href=&#34;https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp&#34;&gt;GNPS&lt;/a&gt; or &lt;a href=&#34;https://metlin.scripps.edu&#34;&gt;Metlin&lt;/a&gt; and predicted by &lt;a href=&#34;http://www.csi-fingerid.org/&#34;&gt;FingerID&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pathway analysis: sometimes we also need to know the relationship among compounds and pathway analysis is always performed for that purpose.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Platform&lt;/h2&gt;
&lt;div id=&#34;xcms-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS online&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xcmsonline.scripps.edu/landing_page.php?pgcontent=mainPage&#34;&gt;XCMS online&lt;/a&gt; is hosted by Scripps Institute. If your datasets are not large, XCMS online would be the best option for you. Recently they updated the online version to support more functions for systems biology. They use metlin and iso metlin to annotate the MS/MS data. Pathway analysis is also supported. Besides, to accelerate the process, xcms online employed stream (windows only). You could use stream to connect your instrument workstation to their server and process the data along with the data acquisition automate. They also developed apps for xcms online, but I think apps for slack would be even cooler to control the data processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PRIMe&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://prime.psc.riken.jp/Metabolomics_Software/&#34;&gt;PRIMe&lt;/a&gt; is from RIKEN and UC Davis. It supports mzML and major MS vendor formats. They defined own file format ABF and eco-system for omics studies. The software are updated almost everyday. You could use MS-DIAL for untargeted analysis and MRMOROBS for targeted analysis. For annotation, they developed MS-FINDER and they also developed statistic tools with excel. This platform could replaced the dear software from company and well prepared for MS/MS data analysis and lipidomics. They are open source, work on Windows and also could run within mathmamtics. However, they don’t cover pathway analysis. Another feature is they always show the most recently spectral records from public repositories. You could always get the updated MSP spectra files for your own data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OpenMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.openms.de/&#34;&gt;OpenMS&lt;/a&gt; is another good platform for mass spectrum data analysis developed with C++. You could use them as plugin of &lt;a href=&#34;https://www.knime.org/&#34;&gt;KNIME&lt;/a&gt;. I suggest anyone who want to be a data scientist to get familiar with platform like KNIME because they supplied various API for different programme language, which is easy to use and show every steps for others. Also TOPPView in OpenMS could be the best software to visualize the MS data. You could always use the metabolomics workflow to train starter about details in data processing. pyOpenMS and OpenSWATH are also used in this platform. If you want to turn into industry, this platform fit you best because you might get a clear idea about solution and workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mzmine-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MZmine 2&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://mzmine.github.io/&#34;&gt;MZmine 2&lt;/a&gt; has three version developed on Java platform and the lastest version is included into &lt;a href=&#34;https://msdk.github.io/&#34;&gt;MSDK&lt;/a&gt;. Similar function could be found from MZmine 2 as shown in XCMS online. However, MZmine 2 do not have pathway analysis. You could use metaboanalyst for that purpose. Actually, you could go into MSDK to find similar function supplied by &lt;a href=&#34;http://www.proteosuite.org&#34;&gt;ProteoSuite&lt;/a&gt; and &lt;a href=&#34;https://www.openchrom.net/&#34;&gt;Openchrom&lt;/a&gt;. If you are a experienced coder for Java, you should start here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xcms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/release/bioc/html/xcms.html&#34;&gt;xcms&lt;/a&gt; is different from xcms online while they might share the same code. I used it almost every data to run local metabolomics data analysis. Recently, they will change their version to xcms 3 with major update for object class. Their data format would integrate into the MSnbase package and the parameters would be easy to set up for each step. Normally, I will use msconvert-IPO-xcms-xMSannotator-metaboanalyst as workflow to process the offline data. It could accelerate the process by parallel processing. However, if you are not familiar with R, you would better to choose some software above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emory-mahpic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Emory MaHPIC&lt;/h3&gt;
&lt;p&gt;This platform is composed by several R packages from Emory University including &lt;a href=&#34;https://sourceforge.net/projects/aplcms/&#34;&gt;apLCMS&lt;/a&gt; to collect the data, &lt;a href=&#34;https://sourceforge.net/projects/xmsanalyzer/&#34;&gt;xMSanalyzer&lt;/a&gt; to handle automated pipeline for large-scale, non-targeted metabolomics data, &lt;a href=&#34;https://sourceforge.net/projects/xmsannotator/&#34;&gt;xMSannotator&lt;/a&gt; for annotation of LC-MS data and &lt;a href=&#34;https://code.google.com/archive/p/atcg/wikis/mummichog_for_metabolomics.wiki&#34;&gt;Mummichog&lt;/a&gt; for pathway and network analysis for high-throughput metabolomics. This platform would be preferred by someone from environmental science to study exposome. I always use xMSannotator to annotate the LC-MS data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;others&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://genomics-pubs.princeton.edu/mzroll/index.php?show=index&#34;&gt;MAVEN&lt;/a&gt; from Princeton University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vpr.colostate.edu/pmf/wp-content/uploads/sites/23/2015/06/Broeckling_2014.pdf&#34;&gt;RAMclustR&lt;/a&gt; from Colorado State University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/html/MAIT.html&#34;&gt;MAIT&lt;/a&gt; based on xcms&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yufree/enviGCMS&#34;&gt;enviGCMS&lt;/a&gt; from me&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.ebi.ac.uk/metabolights/&#34;&gt;Metabolights&lt;/a&gt; for sharing data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;All of the platform above could be used for metabolomics and lipidomics. However, best selection would be based on your programming skills and the popularity in your research area. Every tool need training before analysis your data and you could choose one randomly and be focused on the source code for one day. If you feel you could handle it, just use it. Otherwise select another one. Enjoy and fight with the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>留学生、隔离与适应</title>
      <link>/cn/2017/05/22/study-aboard/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/22/study-aboard/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;留学生&lt;/h1&gt;
&lt;p&gt;作为国产土鳖，在海外做博后接触最多的除了土著就是华人移民、各国留学生与国内访问学者，这里面还是有很多有意思的现象，而这些现象其实对应了几篇非常有意思的论文。&lt;/p&gt;
&lt;p&gt;首先我们看一下国内来的留学生群体。目前国内的留学越来越低龄化，很多接触到的国内来的本科生，往往在国内读个一年高中就跑来这边读高中，用这边高中学历来申请学校。大学读完了基本都是身份导向，学签转工签，工签转枫叶卡，枫叶卡转公民。你如果问为什么不回国，他们的回答往往不是常见的国内机会不好，而是认为国内没有认识的人，主流社交关系都在国外，一回国跟回炉重造差不多。这是个死循环，出国年龄越小，人际关系越在海外，越不可能回国。而且很多本科生的高中同学其实在国内也是高中同学，有个留学生跟我说他们国内一个高中班，到了高二整建制的来了北美，这种情况怕是一个也回不去了。&lt;/p&gt;
&lt;p&gt;另一个有意思的现象是留学生中还存在鄙视链。高中过来的鄙视大学过来的，大学过来的觉得研究生过来的太多国内习气不合群，而他们又都鄙视国内交流项目过来的，认为交流生都是拿国家钱混经历，选的课还简单，语言也不行。同样的鄙视链在华人移民中也存在，最早的移民多是劳工移民，八九十年代流行技术移民，前十年是投资移民，最近是留学移民，鄙视链基本就是按照出国早晚与学历来的。国外生活处处需要职业代理人，找华人并不见得比找老外有经济优势，但年龄约长，越喜欢找固定的华人代理人，其实老外也一样。我跟一个刚办移民的博后聊天，他海外博士曾在国内工作过一年，但终究还是觉得国内环境不适应跑出来了，他目前就打算拿身份了而且也坦然说不希望国内发展太好，因为这样就会说明自己选择国外身份是错误的，他希望国内能发展到过的很舒适但比国外还差一点点的状态。这个想法大概在留学生跟移民的内心里都有点吧，不信你去mitbbs上转转就知道了，我倒觉得这个现象其实背后有博弈论的影子。&lt;/p&gt;
&lt;p&gt;还有一个现象是留学生置产。这是个经济问题，很多学生来读书到银行开户问的第一件事就是房价，一般来说，本科来了买房，交了首付把不住的房间出租，租金可以抵消月供，等毕业要回国或外地工作时转卖，读书的时间房价的上涨其实还能赚一笔作为工作启动资金。也许你觉得首付存在外汇管制比较难，我只能说私下的外汇交流挺多的，存在庞大的代理人市场，要不然比特币也不会中国人玩的多，此外国外有定居亲戚朋友需要国内置产的两方利益交换就可以了，当然这个比较吃信任，所以只能亲戚朋友。有位银行职员曾跟我吐槽中国人怎么那么有钱，经常全款买房，买的车也都是好车，一身加拿大鹅，我赶忙解释那些人国内也是有钱人，不代表平均水平，另外奢侈品国内定价比国外高一大截，他们买是因为便宜。&lt;/p&gt;
&lt;p&gt;再就是家长陪读现象。有次跟访学博后聚餐，很意外发现带孩子来的孩子都是学校本科生，说白了很多访学博后出国一方面是学术交流，另一方面也算是陪读。另外陪读家长一般都是母亲，有当地华人说美国西海岸是小三村，加拿大东部则可看作正房陪读村，至于一家之主大都身居国内经商赚钱。不得不说还是国人精打细算，懂得规划自己，处处“精致”。&lt;/p&gt;
&lt;p&gt;然后我们再看下另一个群体，港台留学生或移民，一个特别的现象就是这两拨人互相独立，就连华人学生会也是两个，显著区别就是语言，港台留学生英语更好，私下用粤语。据说97年左右大量香港人移民加拿大，后来又回去一拨，但这也造就了这个群体的人口基础。不过我遇到的港台人都是比较友善的，基本默认不聊政治，也听说过有台湾留学生打飞的回去投票展示优越性，不过台式民主也算是一道别样风景了。&lt;/p&gt;
&lt;p&gt;还有三个我们的近邻留学群体，一个是韩国人群体，一个是日本人群体，另一个就是印巴人群体。韩国人群体跟中国人比较像，勤奋好学，比较鲜明的特点就是韩国人专属教堂，感觉他们的传教士也算是一道风景了。日本人群体则很难发现，因为他们国际留学生读学位的非常少，一方面是因为本国好学校不少，另一方面半终身雇用的日式企业对大三学生要搞什么内定实习，你要跑国外了，回去根本找不到工作，也就是海归在日本并不吃香。但也有例外，那就是如果你去语言学校溜达，会突然发现扎堆的日本人，打听下才知道日本留学主体大都是学语言的，为了企业国际化，语言学校没有学位但有语言环境，时间也就几个月，而且女生比例奇高。这一方面说明日本人留学其实为了国内就业，另一方面，出来读的也大都是不喜欢日式大男子主义的女生，这个现象我们后面分析。至于印巴人，虽然英语带口音，但真的很主动去交流，抱团现象明显，家庭观念强。其比较特色的就是留学生之间相互认识，互相扶持，确实存在移民一个人，带来一大家的现象，而且普遍没有任何归国意愿。&lt;/p&gt;
&lt;p&gt;再有就是中东、南美与东欧留学生群体。这三个群体不知为何经常系统性被忽略，其实也都各具特色。中东留学生大都是来自伊朗，也是勤奋好学，应该说伊朗是个被制裁太多的地方，好好的从世俗国家搞成宗教国家了。但在那个人口基数上人才数量很庞大，灯塔国不欢迎他们，就跑枫叶国了。另外就是我见过的都很能说，滔滔不绝，喜欢打听事，这种八卦文化在中东据说比较普遍。南美留学生也属于比较抱团的，但模式是巴西人单独抱团，其他人抱团，估计也属于语言隔离。南美上世纪陷入中等收入陷阱，到今天感觉还没缓过来，但新兴领域的科研势头也很猛。跟南美人打交道要注意其宗教信仰与家庭结构，一般男士比较健谈活泼，但当家的其实是女士，组里的巴西小哥为了给女友买香水也吃了一个月的简餐。南美人相比中国人更熟悉日本人，可能是南美有数量不小的日本移民，跟他们聊日本动漫会发现也基本就是《七龙珠》跟《足球小子》，对了南美人周末会踢足球，他们主要社交就是足球与健身。东欧留学生跟我们思考方式也比较像，可能是冷战造就的，人口中波兰人非常多，语言上英文都挺好，有机会要去尝试波兰水饺，学术上对数学非常自信，也比较勤奋。&lt;/p&gt;
&lt;p&gt;除了上面这些，还有比较小众的西欧留学生跟东南亚留学生，接触很少，非洲留学生在国内天天见，这边并不多。最后就是加拿大当地人，普遍喜欢嘲笑美国人，特别是大选之后，但比较安于现状，对海外留学生存有迷之优越感但又表现的非常尊重你且尊重规则与多元文化，有时候让人哭笑不得。他们比较喜欢跟人打交道的专业，所以理工科基本是留学生天下，有自己的圈子，留学生基本进不去，但CBC就可以，主要是关注话题不同，脸书跟推特是他们第二家园，给我感觉就是单纯、理想主义但内里是有点排外的并且不太愿意主动了解世界其他“落后”地方。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;隔离&lt;/h1&gt;
&lt;p&gt;好了，扯了那么多的现象，现在说点理论。我现在感觉国内外最大的不同就在于国内其实是一元文化而国外存在更多的多元化，这个差别很可能是很多问题的根源。我们用日本来举例，日本就是单一民族国家，当其做学生时，也是大量派出留学生到海外学习，但当其发展起来后，其留学生就更多是实用主义了。中国虽然不是单一民族国家，但基本可以看作单一文化国家，中国人在海外两极分化比较重，要么就彻底认同海外生活，也不打算回来了；要么就根本不认同，在海外构建自己文化圈或就是实用主义，学成归国。但目前来说，留学生中前者比例占优，但如果国家能均衡发展到日本那个程度，后者比例就会提高。而且由于20-30岁的人际关系对事业初期发展很重要，所以出国年龄越早，前者比例会越高，这个也是很多海外优秀人才不想回去的主要原因，担心回去被国内熟人文化玩死。&lt;/p&gt;
&lt;p&gt;国外的多元化则是陌生人文化主导，强调反歧视与文化尊重，更简单的词汇就是政治正确。国内来的人会觉得有时候政治正确会效率低下，例如吃顿饭要先发个调查表问问食物禁忌与是否素食主义；建筑物设计里必须有残疾人车位与通道且经常见到所有车位都满了但残疾人车位还是空着一排；平时聊天需要注意各种雷区…这些跟前面提到的现象有关，一个社会如果是移民社会，那么所有团体都会为自己的文化与习惯争取政治上的尊重，这样的社会就存在单一文化社会里根本就意识不到的不尊重。例如你在沙特可以随意说娶多个老婆但你就是不能说喝酒的事，同样的话放到国内就成了无法理解的论题，而多元社会的人群构成很复杂，所以生活在这样的社会，你会感到所有人对你的尊重，前提是你需要政治争取，如果不争取，还不如单一文化社会，歧视更严重。&lt;/p&gt;
&lt;p&gt;这可能是国内推广女权、LGBT还有其他反歧视最大的难题，如果你生活在多元社会，权利分散而文化多样，大家就可以讨论小团体的问题。但如果这个国家文化惯性比较大，那么这个问题就变成了要么西风压倒东风，要么东风压倒西风，结果很容易把争取权利的一方搞得极端化与妖魔化。不过应该说虽然国家是单一文化的，但城市却可以是多元的，越是大城市与新城市，那些在原有环境不满的多元团体就容易聚集，最终发现自己的局部绿洲。灯塔国就是这么构建出来的，不管前提是原住民几乎死光。而前面提到日本留学生女性为主，很多人其实是不打算回日本，因为那里大男子主义横行，女性事业发展空间有限，此处不留爷，自有留爷处。&lt;/p&gt;
&lt;p&gt;这本质上是隔离问题，所有人都希望生活在最尊重自己的环境中而拒绝不舒服的地方，人又都是两足动物，所以不高兴就用脚投票。最后结果就是大家都在自己认为最适应的圈子里抱团取暖，不关心其他人的生活。这个隔离问题我在之前说过很多次，而且我不认为这是个很好的现象，或者说隔离现象还有一个孪生现象——适应。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;适应&lt;/h1&gt;
&lt;p&gt;所有人都是用脚投票，但其实这有个隐含前提，那就是已经做出了选择，观点明确。现实中，这个前提很有问题，多数人是在观察观点而不是发表意见，投票时不见得全心全意。目前我们可以看到两种聚集模式，一种就是按照谢林隔离模型所描述的种族隔离聚集，另一种则是以色列正统犹太人的街区黑化模型，前者我提过很多次不赘述，后者的含义则是当地人转化了外来人的文化，从外来人的角度就是适应了当地的生活方式。所以一个人跑到一个陌生的地方实际上有两种选择：融入或逃离。如果这个人不愿意适应当地文化，那么他就会逃离或者自我隔离；而如果他愿意去适应，就会融入新环境并从内部加入自己文化的组分。&lt;/p&gt;
&lt;p&gt;中国文化其实是很欢迎其他文化融入的，但比较麻烦的是不太欢迎整合其他文化。同样的问题也出现在西方国家，表面上是很欢迎移民但内在却排斥。所不同的是，西方国家存在政治正确的斗争来促进适应的发生而中国文化尊重传统基本是跟政治两条平行线，自由发展。所以适应性问题其实应该当成某种现代人素质进行理解式宣传而不是填鸭式宣传。所有人都不喜欢听别人说自己做错了，但引导他自己得出自己观点需要改进或变化是很有必要的，否则就是单纯情绪宣泄与抱团取暖，并不解决实际问题。&lt;/p&gt;
&lt;p&gt;适应问题其实也不是单一文化国家与多元国家之间的转换问题，搞成多元化却有可能出现“政治正确”而加深隔离现状，而是一个普遍存在尚未解决的问题，古话说就是要达到“和而不同，周而不比”的状态。目前大趋势是表面的文化多元化与内在的认知割裂化，不论国内国外，不论网上网下，不论职业年龄，具体怎么处理，只能说是个个人持续内心斗争决策与对外权益斗争的过程，很难一劳永逸。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;选修c站密歇根大学的模型思考课程，阅读参考教科书&lt;/li&gt;
&lt;li&gt;选修SFI的复杂性科学导论课程，阅读参考论文&lt;/li&gt;
&lt;li&gt;选修c站斯坦福与UBC合开的两门博弈论课程，阅读参考论文&lt;/li&gt;
&lt;li&gt;这是我的相关笔记本：yufree.cn/notes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最近重心转到科研，不会三个主题去写了，比较累。上面的参考文献如果能通读，则会显著提高对社会的了解，就这样。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>组学实验设计中的功效分析</title>
      <link>/cn/2017/05/21/omics-power-analysis/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/21/omics-power-analysis/</guid>
      <description>&lt;p&gt;最近设计了一组代谢组学实验，因为用到动物所以需要学校动物保护委员会的许可，完成了一系列理论学习与实验室参观后，主管人员要求提供预实验结果与功效分析来确定动物数量。这个要求就诡异了，组学实验同时测上千个指标，这功效分析根本无从谈起。但因为是必填项，硬着头皮去查了下资料，如果是代谢组学数据，确实基本没人提功效分析，有也是针对特定测量值。然而，蛋白质组学跟基因组学都曾经讨论过这个问题，这里我大体总结下。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;功效分析&lt;/h2&gt;
&lt;p&gt;常规功效分析至少需要显著性水平、组内标准差、组间差异、功效及检验方式来确定样本数。其实说白了就是知道想看到一个显著差异所需要的最小样本数，如果不能达到这个数，那么你这个实验基本可以当成白干了。功效分析跟显著性分析关注点并不一样，前者关心看到的差异真不真，后者关心这个差异存不存在。一般而言，控制实验都能看到差异，所以关心这个差异靠不靠谱其实对实验设计很重要，而具体表现就是样本数。样本越大越容易看出细微差异，如果样本数固定，那么可以通过功效分析测定哪些差异靠谱。从上面我们可以总结出两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;功效分析需要显著性水平的设定&lt;/li&gt;
&lt;li&gt;功效分析是知道多个求一个，这个数可以是样本数，也可以是差异本身&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;组学实验&lt;/h2&gt;
&lt;p&gt;组学实验最大问题就是同时测定两组样本的多维属性，这里面显著性水平要考虑整体错误发现率。同时，功效分析对于单一变量考察是没什么问题的，但如果你同时测量多个变量，那么这个样本数对不同变量应该是不一样的。你需要对结果进行筛选，而这个筛选反过来影响样本数。此外，组学实验中存在大量白噪音，不考虑这个问题的功效分析也是有问题的。我们通过下面的例子来说明。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BiocParallel)
library(xcms)
## Load 6 of the CDF files from the faahKO
cdf_files &amp;lt;- dir(system.file(&amp;quot;cdf&amp;quot;, package = &amp;quot;faahKO&amp;quot;), recursive = TRUE,
         full.names = TRUE)

## Define the sample grouping.
s_groups &amp;lt;- rep(&amp;quot;KO&amp;quot;, length(cdf_files))
s_groups[grep(cdf_files, pattern = &amp;quot;WT&amp;quot;)] &amp;lt;- &amp;quot;WT&amp;quot;
## Define a data.frame that will be used as phenodata
pheno &amp;lt;- data.frame(sample_name = sub(basename(cdf_files), pattern = &amp;quot;.CDF&amp;quot;,
                      replacement = &amp;quot;&amp;quot;, fixed = TRUE),
            sample_group = s_groups, stringsAsFactors = FALSE)
## Read the data.
raw_data &amp;lt;- readMSData2(cdf_files, pdata = new(&amp;quot;NAnnotatedDataFrame&amp;quot;, pheno))
## Find peaks 
cwp &amp;lt;- CentWaveParam(snthresh = 20, noise = 1000)
xod &amp;lt;- findChromPeaks(raw_data, param = cwp)
## Doing the obiwarp alignment using the default settings.
xod &amp;lt;- adjustRtime(xod, param = ObiwarpParam())
## Define the PeakDensityParam
pdp &amp;lt;- PeakDensityParam(sampleGroups = pData(xod)$sample_group,
            maxFeatures = 300, minFraction = 0.3)
## Group the peaks
xod &amp;lt;- groupChromPeaks(xod, param = pdp)
## Fill in peaks with default settings. Settings can be adjusted by passing
## a FillChromPeaksParam object to the method.
xod &amp;lt;- fillChromPeaks(xod)
## Get the data matrix
data &amp;lt;- featureValues(xod, value = &amp;quot;into&amp;quot;)
## Get the complete cases
data0 &amp;lt;- data[complete.cases(data),]
dim(data0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 243  12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在上面的示例中，两组数据每组样本是6个，共提取出了243个峰的完整数据，那么，此时的功效分析首先要确定整体错误发现率。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(genefilter)
## Get group infomation
lv &amp;lt;- as.factor(pData(xod)[,2])
## Get median difference and standard deviation in one group
tr &amp;lt;- rowttests(data0,lv)
## Estimates the proportion of true null p-values, for FDR
ts &amp;lt;- mean(abs(tr$statistic),na.rm = T)
library(qvalue)
pi0 &amp;lt;- pi0est(tr$p.value)$pi0
hist(tr$p.value,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-05-21-omics-power-analysis_files/figure-html/FDR-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从上面我们可以看出这一组多重比较数据的p值分布符合均匀分布, 道理上说这组数据基本可以看作没有多少差异，但直方图却显示应该存在一些有显著差异的峰。那么我们基于当前数据可以进行一下功效分析，看看究竟差异多大可以认为是真的。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get median difference and standard deviation in one group
dm &amp;lt;- median(abs(tr$dm))
sd &amp;lt;- median(rowSds(data0[,1:6]))
## Get the difference
library(ssize)
## Get the differences number fitting the power in this DoE
power.t.test.FDR(n = 6, sd = sd, FDR.level = 0.05,power = 0.8, pi0= 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 6
##           delta = 6149021
##              sd = 366486.2
##       FDR.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(tr$dm&amp;gt; 6149021)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get the sample numbers fitting the power and 50% differences true in this DoE
power.t.test.FDR(delta = dm, sd = sd, FDR.level = 0.05,power = 0.8, pi0= 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 9675.235
##           delta = 136197.7
##              sd = 366486.2
##       FDR.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们大体可以认为，在当前实验设计下，只有1个峰的显著差异是靠谱的。同理，也可以计算出如果想看到50%的差异为真，我们每组需要9675个样本。&lt;/p&gt;
&lt;p&gt;同时，如果不考虑错误发现率的结果则是：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power.t.test(n = 6, sd = sd, sig.level = 0.05,power = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 6
##           delta = 658041.6
##              sd = 366486.2
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(tr$dm&amp;gt;497799.2,na.rm = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power.t.test(delta = dm, sd = sd, sig.level = 0.05,power = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 114.6297
##           delta = 136197.7
##              sd = 366486.2
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当前实验可以发现27个差异是靠谱的，或者需要每组115个样品才能表示出一半的差异是真的。&lt;/p&gt;
&lt;p&gt;产生上述问题的本质在于高通量数据的错误发现率控制降低了发现差异的p值，所以更少的差异是真的。同时如果严格控制错误发现率，那么所需要的样本数会非常多。此外，这个结果提示我们如果样本数不多，那么你其实只能对那些差异很大的峰给予信心。&lt;/p&gt;
&lt;p&gt;但上面的计算有一个最大的问题，就是组内标准差其实差异很大，此时应考虑每个样本的情况:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power &amp;lt;- 0.8
alpha &amp;lt;- 0.05
pv &amp;lt;- tr$p.value
df &amp;lt;- cbind.data.frame(tr$dm,rowSds(data0[,1:6]),alpha,pv)
df &amp;lt;- df[df$pv&amp;lt;0.05,]
rs &amp;lt;- vector()
for (i in c(1:nrow(df))){
        r &amp;lt;- power.t.test.FDR(delta = abs(df[,1][i]), sd = df[,2][i], FDR.level = 0.05,power = power, pi0 = 1)
        rs[i] &amp;lt;- r$n
}
sum(rs&amp;lt;6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样，我们发现会有0个差异是真正显著的，这比使用平均样本控制得到的少，说明单个去看其实更难找到真正有差异的峰。&lt;/p&gt;
&lt;p&gt;功效分析不一定会得到跟FDR控制更少的峰，而且FDR控制的算法设计里实际也考虑了类似的过程，但是从全局分析的功效分析对于实验设计非常有用。当你进行了一组预实验，功效分析可以告诉你结论中多少峰是靠谱的，如果一个都没有，那么就增加样本量吧。如果你的研究目的是至少发现一个峰是靠谱的，也就是生物标记物研究，那么此时功效分析所需的样本数最好再多一个，否则结论可能无意义。&lt;/p&gt;
&lt;p&gt;下面就把上述过程整合成一个函数，输入提取好的MSnExp对象与功效值、p值与q值的阈值就可以返回一个满足条件的峰列表，这个函数只对两组数据对比有用，也仅适合 xcms 3 的新对象类型，不过稍微修改就可以推广到其他检验方式了。这个峰列表是进一步研究的基础，如果不严格考察，后面的分析大概率得到错误结论。在检验模型的选择上，如果你的数据存在技术重复或需要线性混合模型求解，这里面的功效分析会比较不同，需要你自己设计算法求解，这里不展开说了，大概是个生统硕士论文的工作量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(genefilter)
library(qvalue)
library(ssize)
getrealpeaks &amp;lt;- function(xod, power = 0.8, pt = 0.05, qt = 0.05, n = 6){
        data &amp;lt;- featureValues(xod, value = &amp;quot;into&amp;quot;)
        idx &amp;lt;- complete.cases(data)
        data1 &amp;lt;- featureDefinitions(xod)
        mz &amp;lt;- data1$mzmed[idx]
        rt &amp;lt;- data1$rtmed[idx]
        lv &amp;lt;- as.factor(pData(xod)[,2])
        data0 &amp;lt;- data[idx,]
        tr &amp;lt;- rowttests(data0,lv)
        qvalue &amp;lt;- qvalue(tr$p.value)
        pi0 &amp;lt;- qvalue$pi0
        alpha &amp;lt;- pt
        df &amp;lt;- cbind.data.frame(diff = tr$dm,sd = rowSds(data0[,1:n]),p = tr$p.value,qvalue = qvalue$qvalues,mz,rt,data0)
        df &amp;lt;- df[df$p &amp;lt; pt,]
        rs &amp;lt;- vector()
        for (i in c(1:nrow(df))){
                r &amp;lt;- power.t.test.FDR(delta = abs(df[,1][i]), sd = df[,2][i], FDR.level = qt,power = power, pi0 = pi0)
                rs[i] &amp;lt;- r$n
        }
        df &amp;lt;- cbind(n = rs,df)
        df &amp;lt;- df[df$n &amp;lt; n &amp;amp; df$qvalue&amp;lt;qt,]
        return(df)
}
getrealpeaks(xod = xod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] n        diff     sd       p        qvalue   mz       rt      
##  [8] ko15.CDF ko16.CDF ko18.CDF ko19.CDF ko21.CDF ko22.CDF wt15.CDF
## [15] wt16.CDF wt18.CDF wt19.CDF wt21.CDF wt22.CDF
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>差异、相似与模式</title>
      <link>/cn/2017/05/17/pattern/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/17/pattern/</guid>
      <description>&lt;p&gt;最近审了一份主题是质谱数据分析的稿子，领域是分析化学，很明显的感受就是作者但是很明显是为了用新方法而用，没有从问题出发，所以借机讨论总结下科研数据分析的思考视角。 差异&lt;/p&gt;
&lt;p&gt;科研数据分析最基础的出发点就是寻找差异，你观察到了两组数据，这个分组是根据实验设计或人为划分的，你想了解两组数据差异。最朴素的思路就是分组聚合，例如选取出现最多的众数，排序中间位置的中位数以及平均值。但是这个思路只是简单的将一组数描述为一个数，并无法表示这组数的离散程度，也就是丢失了一部分可以进行对比的信息。如果你考虑上表示离散程度的方差，结果就成了对比两个数。这样的对比其实只是描述性的，如果你愿意且具备统计与数学功底，你可以构建出无数用来描述一组数据的单一或多个数值进行比较，这些数值在不同领域可能有不同称呼，可以理解为指标，或者称作统计量。统计量的构建至少满足两个条件：包含想要考察的信息与具备可比较的数学性质。前者比较好理解，后者就需要概率论做基础了，在必要时可根据实际问题修改统计量的数学描述方式。万万不可别人让你用什么就去用什么，这样永远是雾里看花。&lt;/p&gt;
&lt;p&gt;有了统计量，我们就可以进行比较了。不论你使用p值还是贝叶斯推断，其核心思路就是将统计量对应到一个分布里去，然后从概率视角看一下这个差异离不离谱（或者跟先验的概率分布比较），如果离谱就认为差异是显著的，跟0差异不大就认为没有明显的差异。但需要知道的是没有明显的差异是对应你假设检验方法而言的，不是说真实是没差异的，用不同的检验方式，结果可能不同。对于严谨的科学发现，主流的检验方式得到的结论应该是相似的，如果结果有差异，那么要么是检验方式无效或不适用，要么就是你的数据对差异判断并不支持。很多时候，如果技术发展不到位，数据的噪音掩盖信号，此时你不能验证结论，需要上更先进的仪器。从这个角度看，数据采集技术是否先进会制约科学发现，这也是很多学者寻求发展时经常要考虑平台是否先进的根本原因，没有技术平台，科学发现只能说模棱两可。好比你想用放大镜研究细胞结构，基本只能拿着鸡蛋看看了。&lt;/p&gt;
&lt;p&gt;差异多数时候是两两间的，但有时候我们的问题是在某个因素不同水平的影响是否显著，例如我想知道某种污染物的自由态浓度是否受高中低不同土壤含水量的影响，这里土壤含水量是因素，高中低是三个水平，此时用方差分析就可以得到土壤含水量是否影响污染物自由态浓度的判断。推而广之，如果水平是连续变量，那么此时的差异分析实质上是相关分析或者说是线性模型的一个特例。你总是要对模型系数进行假设检验来确定这个系数是否影响了你要考察的变量，如果你要进一步进行讨论，参考下面的模式那一部分。&lt;/p&gt;
&lt;p&gt;发现一个差异并从统计角度说明其出现概率比较低或跟先验知识包含不同的信息量是科研数据分析最常见的应用场景。基本思考流程可以归纳为首先构建代表性可比较的统计量，然后进行假设检验的比较，最后根据结果给出判断。这个判断过程有统计学与概率论做支撑，独立于观察过程，因此判断具备相对客观的属性。&lt;/p&gt;
&lt;p&gt;相似&lt;/p&gt;
&lt;p&gt;另一个科研中常见的数据分析场景是寻找数据的共性，但其实基本思路跟找差异比较接近但面对的数据结构会不太一样。在差异分析中，通常考察的是单一属性；相似性分析中，通常你会得到对同一客体的多个描述角度。举例而言，我得到两组河水样品，想知道两组水样是否接近，此时每组样品如果只测定一个指标，那么对比一下就完了；但如果测了很多组指标如何来衡量？两组河水pH值很接近但COD差距很大，可同时溶解氧几乎相同，如何判断？&lt;/p&gt;
&lt;p&gt;一个朴素的想法就是测量可比指标间的标准化差异，然后求绝对值和或平方和，越大表示相似度越小，或者用类似核函数的思想把高维数据映射到低维空间，还可以进行傅立叶变换来通过低维数值保留核心信息量进行比较。此外一个我非常欣赏的思路就是通过打乱分组构建随机统计量来对比实际发生的统计量出现概率，大概就是Fisher精确检验的套路，这个角度是纯统计思路且在计算不那么贵时很好用。&lt;/p&gt;
&lt;p&gt;相似性分析的科研应用场景会越来越多的，首先是数据库比对，目前组学技术发展很快，相关数据库累积也很快，你发现一个功能蛋白，反推出序列可以直接去搜库做进化树，这里面的相似性分析大都用到的动态编程与数据变换，不然速度跟不上。在质谱上就是谱库检索与比对，此时要考虑同位素分布、质量亏损、源内反应加合物等等，不然也很难比对相似性。另一个应用场景是非监督学习里的聚类分析，这里面相似性统计量是进行聚类的基础。其实虽然科学发现里寻找差异更符合探索逻辑，但实际上当前的数据驱动性研究更多是发现共性，当数据累积越来越多，对于共性的新研究方法或新统计量的提出可能更有价值。&lt;/p&gt;
&lt;p&gt;模式&lt;/p&gt;
&lt;p&gt;差异与共性分析都是最基础直观的科研数据分析思路，学科内规律性的东西有时候并不能直接从差异跟共性分析中得到，这时需要识别数据中的模式规律。所谓模式在科研结果中有两种，一种是探索未知，另一种是拟合已知。前者并不需要特别强的理论基础，用来发现模式；后者需要有比较强的理论支撑。&lt;/p&gt;
&lt;p&gt;拟合已知的最常见，例如线性拟合、多项式拟合等，很多时候由本学科的理论来提供基本形式，例如物理里的牛顿三定律、化学里的能斯特方程、生物里的米氏方程。这些方程的数学形式已经在学科内得到了认可，所以当你获取相关数据时，模式是固定的，你所需要求解的是某个参数。参数的稳定性也可以侧面反应理论的真实性，但拟合严格说更像是验证模式而不是发现模式。同时拟合背后的理论基础及来源也是要深入理解的，例如做吸附等温线有两种基本拟合方式：弗里德里希方程与朗缪尔方程，如果你搞不清机理随便用任何一种都会发现拟合效果都说得过去，但一个是纯理论另一个是经验公式，在使用时得考虑你研究目的。有一类研究比较看重预测效果，此时拟合就可以放宽，例如用多项式拟合考虑个自由度就可以了，甚至有时候可以考虑不同数值范围采用不同数学形式，在边界上用样条平滑下就可以了。但不论拟合理论公式还是经验公式，起码这类分析总还是有个数学公式来做骨架的，探索分析则没有这个限制。&lt;/p&gt;
&lt;p&gt;探索未知模式并不是说让你直接把所有科学问题都抽象成 y = f(x) ，然后你收集一大堆y跟相关x，直接扔到多层人工神经网络里去训练，然后搞个验证集看下效果就用，这是工程学思路，到头来也许解决问题，但你可能根本不理解问题。正常的探索过程基本还是有个模型指导的，你可以从最简单的线性模型开始尝试，然后不断提高模型的复杂度，例如引入交互作用或不同x用不同模型的广义加性模型。当然，你可以引入层级模型来探索数据内部结构。当模型复杂到一定程度，就有点人工神经网络的意思了。如果你是为了发现新模式，那么可视化手段是很好的探索起点。但如果是为了预测，其实样条平滑、小波分析等黑魔法就可以随意发挥了，大前提是你真的理解算法，知道自己在干什么。&lt;/p&gt;
&lt;p&gt;从寻找差异开始，我们很多朴素的想法背后其实都有基本模型在起作用，例如t检验就是方差分析的特例、方差分析就是线性模型的特例，线性模式又可以推广到广义线性模型。从最原始的单一统计量构建到多个独立统计量关系探索，再到考虑交互作用，再到层级结构，模型的复杂度可以不断提升。模型的生成过程也要伴随大量的验证与理论支持，并不是随意可以套用，除非你搞机器学习。其实你应该体会到数据分析与科学发展是紧密联系而不是割裂的，很多数据分析方法就是为解决特定科学问题提出来的，没有想象的那么黑箱，反倒是总把新数据分析方法当成黑箱的思路是很危险的。在绝大多数情况下，科学问题的数据分析方法都是针对性的，你也应该能从分析过程体会到背后的抽象与基本假设，否则并不真正理解，虽然这可能不妨碍你发文章。&lt;/p&gt;
&lt;p&gt;从问题视角出发去构建一个方法要比直接套用学科内常见方法更容易体会到《科学研究的艺术》。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data analysis for Desorption Electrospray Ionization</title>
      <link>/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</link>
      <pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</guid>
      <description>&lt;p&gt;Desorption Electrospray Ionization (DESI) is known for on-site mass spectrum analysis. For example, you could use DESI-MS to get the distribution of certain ions on the surface or cross section of sample. Without chromatograph or related separation process, the mass spectrum is actually the average intensities of all the mass during the sampling time. Recently I am thinking how to process such data via xcms.&lt;/p&gt;
&lt;p&gt;Supposing we have data from 1 min sampling and we want to get the mass spectrum. For mass spectrum, 1 min usually means more than 100 full scan. The basic idea to process such data is directly binning the mass and average the intensity. However, I found &lt;code&gt;group.mzClust&lt;/code&gt; function and &lt;code&gt;MSW&lt;/code&gt; method are designed for this purpose.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(msdata)
mzdatapath &amp;lt;- system.file(&amp;quot;fticr&amp;quot;, package = &amp;quot;msdata&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names = TRUE)

xs &amp;lt;- xcmsSet(method=&amp;quot;MSW&amp;quot;, files=mzdatafiles, scales=c(1,7),
              SNR.method=&amp;#39;data.mean&amp;#39; , winSize.noise=500,
               peakThr=80000,  amp.Th=0.005)
xsg &amp;lt;- group.mzClust(xs)
xsg &amp;lt;- fillPeaks.MSW(xsg)
r &amp;lt;- groupval(xsg,&amp;#39;medret&amp;#39;,&amp;#39;into&amp;#39;)
z &amp;lt;- as.data.frame(groups(xsg))
file &amp;lt;- cbind(z$mz,r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could save the data as csv file. Then you could perform further analysis such as peak picking or PCA analysis. If your data could be organized for spatial analysis or imaging, all the data has been ready and you could draw them by one for loop or just use &lt;code&gt;animation&lt;/code&gt; package for a gif. I used this trick for my last group meeting as PhD students.&lt;/p&gt;
&lt;p&gt;PS: the new design of xcms 3 is much more friendly to process such data via &lt;code&gt;XCMSnExp&lt;/code&gt; object and the parameters design is very friendly to users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>社会感知、文献解读与棋盘思考</title>
      <link>/cn/2017/05/06/social-sense/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/06/social-sense/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会感知&lt;/h2&gt;
&lt;p&gt;最近看到一篇&lt;a href=&#34;http://journals.lww.com/co-allergy/Fulltext/2017/04000/Social_media_use_for_occupational_lung_disease.4.aspx&#34;&gt;综述&lt;/a&gt;，大意介绍了下通过社交网络数据来进行职业性肺病的研究趋势。其实回想一下，从大概四五年前就开始有利用类似数据进行研究的报导了，甚至还有个相关名词：社会感知。&lt;/p&gt;
&lt;p&gt;字面理解，社会感知通过收集社会群体传感数据来研究社会中群体行为或相关科学问题。例如，我收集医院呼吸科就诊数据，然后看看这一观察点是否与其他环境因素或个体遗传因素有关联，譬如有人会&lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153099&#34;&gt;发现&lt;/a&gt;雾霾严重时呼吸科就诊率会上升。但其实研究可以不局限于特定行业，社交网络的快速发展一方面成为很多人日常生活的必需品，另一方面也提供了大量信息输出。例如有人就&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2835988&#34;&gt;研究&lt;/a&gt;过在重度空气污染时人们会在微博上的讨论行为。同样的主题似乎在地理学研究中用到，我曾听过一个报告使用的就是微博签到数据来观察旅游旺季游客的聚集行为。如果不局限在国内，实际各大不存在的社交网站都发表过类似论文，有的是基于地理信息，有的是基于语义分析，还有的则基于传播结构用图论的方法去研究。&lt;/p&gt;
&lt;p&gt;总的来说，社会感知可以看作一个研究手段或视角，其应用面还可以更大。就环境分析而言，污染物调查类论文往往侧重于研究其在环境介质中的迁移转化规律，但逻辑上看，污染物的产生多半是人类行为，也就是说迁移转化规律也可能从社会感知角度去考察。例如我们可以关注一些污染企业的经济指标，其波动很有可能与污染物的环境浓度产生关联，如果是滞后性关联，我们是可以预测一些污染的集中爆发的。同样，人们在社交网络里关注的污染物是存在新闻传播高峰的，也就是脉冲式的，但污染物浓度的变化一般是连续的，这之间的差异或趋势可以用来区别环境污染的生理影响与心理影响。如果环境科研工作者不尽快掌握相关研究手段，那么不出意外做社会学研究或计算机科学研究的人会渗透过来。&lt;/p&gt;
&lt;p&gt;最近掌握相关数据的公司纷纷成立研究院，人员组成学科背景都比较多样，很有可能比学术界优先发现有意思的现象。相比之下，学术界学科间壁垒还比较明显，很有可能因为研究手段的落后而走下坡路，而且业界科研的待遇在国外是普遍高于起步期学界待遇的，而人才的流动一般都会是往前沿去，如果在业界那就会流向业界。现在的研究视角越来越多样化与面向问题出发，单一学科或视角、固定的研究模版与闭塞的交流很容易快速衰落而不自知。例如当前大家都喜欢投学科顶尖期刊，很多老牌期刊却存在门户之见，对新方向把握不充分，新研究无法在原有学科体系内成长就会到其他领域或开辟领域野蛮生长，很多人看不上眼的 plos one 跟 scientific report 等新综合性期刊在慢慢孕育新学科。从引用量上可能不明显，但媒体曝光度上来说，新的研究方向更高。如果所有科研经费总量不变，新学科趋势通过媒体影响力会去逐渐挤占传统学科的资源，届时可能会出现系统性学科衰落。这就好比科学共同体的新陈代谢，缓慢却不可逆。所有学科都曾是新学科，也将会成为传统学科，如果内部不能换血，那就只能等外部输血了。这里血指的就是类似社会感知这样的前沿趋势与视角，这对起步期科研人员尤其重要，因为一旦搞错形势，后面不论你多努力可能都会有一种听天由命的焦虑感。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;文献解读&lt;/h2&gt;
&lt;p&gt;最近几年，科普类网站或公众号的快速崛起促进了科学传播，越来越多的人愿意去把最前沿的研究发现带给有好奇心的听众，特别是目前正在进行科研工作的研究生与老师。但我发现很多人在进行解读时并没有很好的说明白研究类型而更多去讨论发现的事实与意义，这样会形成一些误解。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;没有研究基线的盲信&lt;/h3&gt;
&lt;p&gt;很少有研究是凭空蹦出来的，但对科学报道的受众而言，绝大多数都是头一回听到一些研究领域。也就是说听众都是白纸一张，你说什么他们会首先接纳为事实而不是批判思考，因为多半根本就没有提供批判思考的知识体系。所以文献解读很重要的一点就是要说明我们之前对这个问题了解到什么程度了，有一个基线与背景可以让读者去展开批判思考而不是全盘接受。由于很多论文解读出自领域内专家，而专家通常喜欢用复杂术语树立权威感，所以这个偏误多半只能读者自己揣度，留有余地。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;现象与规律分不清&lt;/h3&gt;
&lt;p&gt;社会科学方面的研究论文解读时经常出现，论文描述的是当前现象，但解读时进行了规律演绎。例如队列研究发现富人们存在早期打高尔夫球的共同喜好，这是一个现象，不能解读成当前你去打高尔夫球，以后就可能变成富人。现象与规律分不清不能用简单的相关不代表因果来解释，更深层次的问题在于，即便很多现象说明的因果关系是事实存在的，时过境迁后，这个因果关系解除了。同样是上面的例子，也许早期打高尔夫球其实是因为能在高尔夫球场获取更好的人脉，但如果现在富人们主要社交方式变成打壁球，那么这个现象只能描述为历史事件，曾经是合理的，但当前完全不合理了。规律是相对稳定不变的而现象是历史性的，一般读者看论文解读的视角是规律视角而不是现象视角，如果解读者自己没搞清楚，那就很容易产生误导。多数科学发现都是有语境的，语境就是适用范围，也可理解为历史条件下的现象。而放之四海而皆准的规律是可遇不可求的，特别社会科学里存在舆论反馈与行为习惯的转变，很多结论在自然态下成立在社会系统中就面目全非了，整个社会到处都是局部最优解与区域护城河，不能当成均相系统研究。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;决策风险&lt;/h3&gt;
&lt;p&gt;要知道很多人的决策，特别在健康、育儿与理财等切身相关的主题决策上是很依赖专家意见的，一旦误导后果都是自负的。烟盒上都标个吸烟有害健康，论文解读上最好也提升下决策风险。不知道从什么时候开始，很多原来自然而然的事都出现了各种专业人士去提供服务并帮助决策，所有人都变成了决策困难者。就育儿而言，当前最新的研究成果与30年前父母自己摸索的育儿经验究竟能造成多大的差异，我想这也是个摸石头过河的事。但解读研究成果的人往往并不提示风险，直接论述方法与成效。从读者角度，这个风险往往会被期刊的权威性所掩饰，但事实上越是影响力大的期刊，撤稿率越高；越是前沿的发现，越有可能是缺少证据的。如果论文解读最终变成了个人经验的论述，那么读者应该放弃其决策价值，不然跟信教没啥区别。&lt;/p&gt;
&lt;p&gt;关于文献解读甚至其他种类的文章，读的时候最好也要有点类似社会感知的想法，想想看这是不是现象唯一的解释与思考视角？解读者有没有夹藏私货？自己有没有意识到其中的偏误与决策风险？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;棋盘思考&lt;/h2&gt;
&lt;p&gt;上面关于文献解读的分析实际上也只是棋盘思考的一个应用。设想你跟我下棋，如果你下你的，我下我的那就没意思了，一般情况双方都会去思考对方的立场与策略。好比你去考试，如果你身经百战，那么你一定掌握了出题人视角与阅卷人视角，出题人考察你对某个问题的理解而阅卷人关注的是得分点，作为答题者最好就是把答案分条，展示你理解了某个知识点。其实出题人与阅卷人也会意识到答题者的应对策略去制定答案，这样最后真正懂问题的人总会高分通过，而一知半解的人很难蒙混过关。而在互联网上看信息或报道，你至少要意识到一个三方对弈场景。&lt;/p&gt;
&lt;p&gt;所有报道都存在一个事实基础，把事实展示给你的人是第二方，第三方就是你。大多数人并不能意识到事实与展示事实的人之间的互动关系而看成一体，但其实立场差异不同。A采访了B，B所叙述的事实经过A的加工往往会产生微妙的情感共鸣与调动，而受众就是C。C永远只看到A笔下的B，甚至B是否存在C都不会去怀疑。C当然可以自主采访B，但同样C在描述时也是带有某些情感共鸣的。想到这一点，所有报道，包括自己所提供的，对于别人都是相对主观的。也就是说，棋盘思考的核心不在于了解对方想的是什么，而是意识到自己的局限性。&lt;/p&gt;
&lt;p&gt;只有知道自己的局限性，才能对别人的局限性进行合理评估并达成一定程度上的信任或不信任。这始终都是个决策问题，只是日常生活中发生的太过频繁，很多人意识不到其中发生的过程，总是跟着感觉走。但其实跟着感觉走可能也是一种良好的决策，只是在行为经济学大行其道的今天，总有商人盯着你的行为，考虑能否从中挖到一桶金。无论如何，现代社会实在是错综复杂，所有人都在犯错或试错，站在承认局限性的位置思考总不会错的太离谱。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using xcms offline for metabolomics study</title>
      <link>/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</guid>
      <description>&lt;p&gt;XCMS online is preferred for its convenience, especially with Stream. However, the storage is limited and you need to wait for some time to process your data. Actually, almost all of the functions online could be processed offline on local computer. Here I will show you some tips about using xcms package locally in R.&lt;/p&gt;
&lt;div id=&#34;optimized-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimized Parameters&lt;/h2&gt;
&lt;p&gt;Most of the users like xcms online because they have optimized parameters for different instruments and you could directly choose them. Those parameters are related to peaks extraction, grouping, retention time correction and fill missing peaks. Authors of xcms online has published &lt;a href=&#34;http://www.nature.com/nprot/journal/v7/n3/fig_tab/nprot.2011.454_T1.html&#34;&gt;paper&lt;/a&gt; and show the table of suggested parameters. Thus in the local version, you could directly use them. If you still feel hard, I write a function &lt;code&gt;getdata&lt;/code&gt; in the &lt;code&gt;enviGCMS&lt;/code&gt; package. You could install it from Github (CRAN version has not been updated):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;#39;yufree/enviGCMS&amp;#39;)
# we need parallel computing
library(enviGCMS)
library(BiocParallel)
library(xcms)
# you need faahKO package for demo
cdfpath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;, package = &amp;quot;faahKO&amp;quot;)
# directly input path and you could get xcmsSet object
xset &amp;lt;- getdata(cdfpath, pmethod = &amp;#39;hplcqtof&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;getdata&lt;/code&gt; could directly perform peaks extraction, grouping, retention time correction and fill missing peaks and return the &lt;code&gt;xcmsSet&lt;/code&gt; object for further analysis.&lt;/p&gt;
&lt;p&gt;However, I suggest use &lt;code&gt;IPO&lt;/code&gt; package to optimize the parameters for certain instrumental. Here is the R script for optimizing. You need to be patient because such process usually take half day. After finding the parameters for your instrumental, you could use those parameters for the following studies. Here is the R script to optimize parameters for certain instrumental:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# path and files
# use pool qc or blank for this optimization
mzdatapath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;,package = &amp;quot;faahKO&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names=TRUE)
library(IPO)
# use centwave if you use obitrap
peakpickingParameters &amp;lt;- getDefaultXcmsSetStartingParams(&amp;#39;matchedFilter&amp;#39;)
#setting levels for min_peakwidth to 10 and 20 (hence 15 is the center point)
peakpickingParameters$min_peakwidth &amp;lt;- c(10,20) 
peakpickingParameters$max_peakwidth &amp;lt;- c(26,42)
#setting only one value for ppm therefore this parameter is not optimized
peakpickingParameters$ppm &amp;lt;- 20 
resultPeakpicking &amp;lt;- 
  optimizeXcmsSet(files = mzdatafiles[6:9], 
                  params = peakpickingParameters, 
                  nSlaves = 4, 
                  subdir = &amp;#39;rsmDirectory&amp;#39;)

optimizedXcmsSetObject &amp;lt;- resultPeakpicking$best_settings$xset

retcorGroupParameters &amp;lt;- getDefaultRetGroupStartingParams()
retcorGroupParameters$profStep &amp;lt;- 1
resultRetcorGroup &amp;lt;-
  optimizeRetGroup(xset = optimizedXcmsSetObject, 
                   params = retcorGroupParameters, 
                   nSlaves = 4, 
                   subdir = &amp;quot;rsmDirectory&amp;quot;)


writeRScript(resultPeakpicking$best_settings$parameters, 
             resultRetcorGroup$best_settings, 
             nSlaves=12)
# https://github.com/rietho/IPO/blob/master/vignettes/IPO.Rmd&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;Actually, the statistival methods in xcms online are limited compared with Metaboanalyst. In last post, I have shown how to install Metaboanalyst locally. Here, I also supply a function in &lt;code&gt;enviGCMS&lt;/code&gt; to directly get the csv file to be uploaded to Metaboanalyst. You need to show a xcmsSet object and the name for the file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this xcmsSet object could be directly get from getdata function
getupload(xset,name = &amp;#39;peaklist&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eic-and-boxplot-for-peaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EIC and Boxplot for peaks&lt;/h2&gt;
&lt;p&gt;If you like the report from xcms online, you could also get them with the figures. I also write a function called &lt;code&gt;plote&lt;/code&gt; in &lt;code&gt;enviGCMS&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# you also need the name for subdir of EIC and Boxplot, you might also change the test method for the diffreport
plote(xset,name = &amp;#39;test&amp;#39;,test = &amp;#39;t&amp;#39;, nonpara = &amp;#39;y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the function has been documented. I might update the CRAN version in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;waters-q-tof-mass-lock-issue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Waters Q-ToF mass lock issue&lt;/h2&gt;
&lt;p&gt;If you use Waters Q-ToF, you might be confused by data conversion. I suggest you use the most updated msconvert to convert RAW folder into mzxml, which you could input the lock mass(older version miss this function). However, such data still have gap, you might use the &lt;code&gt;lockMassFreq = T&lt;/code&gt; in xcms to imput such gap to get more peaks. Such parameters could be transfer in &lt;code&gt;getdata&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xset &amp;lt;- getdata(path,lockMassFreq = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation&lt;/h2&gt;
&lt;p&gt;For the annotation part, I suggest using &lt;code&gt;xMSannotator&lt;/code&gt; package. You could install it from my github repo since the author didn’t use github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# You might need to install the following packages before installing this package
install.packages(&amp;#39;data.table&amp;#39;)
install.packages(&amp;#39;digest&amp;#39;)
source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite(&amp;quot;SSOAP&amp;quot;)
biocLite(&amp;quot;KEGGREST&amp;quot;)
biocLite(&amp;quot;pcaMethods&amp;quot;)
biocLite(&amp;quot;Rdisop&amp;quot;)
biocLite(&amp;quot;GO.db&amp;quot;)
biocLite(&amp;quot;matrixStats&amp;quot;)
biocLite(&amp;#39;WGCNA&amp;#39;)
devtools::install_github(&amp;quot;yufree/xMSannotator&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other functions&lt;/h2&gt;
&lt;p&gt;I have writed some other functions in &lt;code&gt;enviGCMS&lt;/code&gt; package and you could explore them. You might find some Easter Eggs. Also I will documented them as vignette in the future.&lt;/p&gt;
&lt;p&gt;This post and the post before is about finding the peaks and performing statistical analysis for metabolomics. In the next post, I will show you some tips about annotation based on &lt;code&gt;xMSannotator&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;If you have other issues about metabolomics data analysis, you could comment here and I’d like to discuss them. Also you could sent email to &lt;a href=&#34;mailto:slack@yufree.cn&#34;&gt;slack@yufree.cn&lt;/a&gt; to get invitation for a slack group about metabolomics data analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>