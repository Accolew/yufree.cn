<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homepage on Miao Yu | 于淼 </title>
    <link>/index.xml</link>
    <description>Recent content in Homepage on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Feb 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>从真空里的球形鸡到社会财富分配（一）</title>
      <link>/cn/2017/07/24/boltzmann-distribution-1/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/07/24/boltzmann-distribution-1/</guid>
      <description>&lt;p&gt;曾经有个笑话吐槽物理学家研究鸡不下蛋问题时首先假设了一只存在真空中的球形鸡，进而认为理工类研究过于简化了现实世界，再进而推导出了一种不修边幅、不谙人情世故且高智商低情商的理工科书呆子刻板印象。就我个人生活经验而言，这种看法倒也没啥问题，对，就是没啥问题，你没看错。&lt;/p&gt;
&lt;p&gt;但是（说“但是”前的都是废话–马克·于瘟），当今学术研究的一个大趋势就是用理工科的思维研究社会自发行为，我曾经尝试&lt;a href=&#34;http://blog.sciencenet.cn/blog-430956-869450.html&#34;&gt;用马尔可夫过程解释社会阶层流动&lt;/a&gt;，但其实很多理工科出身的科学家是很严肃地研究这类问题的。我们将会看到一个松散假设下的物理模型是可以解释很多（但绝不是全部）自发的宏观社会行为的。在某些视角下，我们也许就是个真空中的球状社会信息综合体。首先，我们从具象到抽象，来了解下一个统计物理学的概念——玻尔兹曼分布。在后续的文章中我们会用这个分布来探索下社会行为，特别是经济行为。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为的抽象&lt;/h2&gt;
&lt;p&gt;社会行为其实就是个体间的相互作用，在物理学视角下，这个个体间相互作用有四种（但我知道多数人不关心这个），简单说就是物质能量交换。而社会经济行为视角下，这个相互作用可以看成价值流动，通俗一点就是钱。钱可以在流通成本很低的现代信息社会下快速在社会个体间转移，在物理学视角下可看作能量在不同粒子间的传递。那么类比一下我们会发现，如果我想知道一个社会整体的财富分配，我们可以类比到一个装满带有能量的小球的空间去观察小球的能量分布。这里有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;社会个体（人、公司、团体）-&amp;gt; 空间里的小球&lt;/li&gt;
&lt;li&gt;财富（钱、资产） -&amp;gt; 能量&lt;/li&gt;
&lt;li&gt;价值交换（买卖行为、资本流动）-&amp;gt; 小球间能量交换&lt;/li&gt;
&lt;li&gt;社会个体的财富分配 -&amp;gt; 空间里的能量分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里我们需要一些假设，首先我们考虑的是一个热力学平衡，也就是说，空间里能量总和一定，也就是要是个封闭空间，同样，社会里总财富在某个时间点也是稳定的。其次，我们想知道的是财富分布或能量分布，那么我们就要有不同财富区间与能量区间来构成分布，这里我们用钱数或能量值来划分，拥有钱数或能量在同一区间的社会个体或小球在分布中是一样的，无法区分，也是等概率的。最后，能量交换或价值交换要达到热力学平衡，这个在小球模型里很容易实现，但社会实际状态只能近似认为达到，这意味着交易是随机的且充分的，后续我们会修改这个限制让模型更符合社会实际。&lt;/p&gt;
&lt;p&gt;根据上面的类比与假设，目前我们大概可以用一堆小球去模拟一些稳态社会行为了，这个抽象过程是大多数理工类科学研究的起点，不然一上来就过于复杂很难讨论。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;封闭空间里的赋能小球&lt;/h2&gt;
&lt;p&gt;好了，目前我们先不去想复杂的社会行为来考虑小球。在一个封闭空间里，小球的能量总和是固定的，但能量范围却可以很广，小球数也可以很多，那么我们再简化下，就考虑3个球总能量6，能量区间就会有7个离散数值：0，1，2，3，4，5，6，7。那么我们想得到的是宏观状态下能量分布，我们首先来个原始方法：数。&lt;/p&gt;
&lt;p&gt;首先要数下有多少个宏观态。例如有一个球上有6的能量，其余的球能量都只能是0。我们可以列出下面的7种宏观态：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Macrostate.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;然后我们考虑每一种宏观态下有多少种微观态，因为球有3个，区间有7个，我们可以看成一个排列组合问题。微观下，一共有&lt;span class=&#34;math inline&#34;&gt;\(3!\)&lt;/span&gt;种排列组合，但在基本假设中，我们认为属于同一宏观状态的球是一种，那么这些情况就要被排除掉。例如在宏观态7中，虽然球可以有6种排列，但因为都在1个能量区间，实际只有1种微观态。也就是说，如果我们有N个球，排列组合上看虽然有&lt;span class=&#34;math inline&#34;&gt;\(N!\)&lt;/span&gt;种组合，但实际由于N个球中有n个球在区间i，我们要从总的排列组合中除掉这种组合。也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\# microstates = \frac{N!}{n_0!n_1!...n_i!}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这样我们可以得到共计28种微观态，在这里所有微观态的出现概率是一样的：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;State.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Microstate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;total&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在宏观态1中所有微观态出现的可能性为&lt;span class=&#34;math inline&#34;&gt;\(3/28\)&lt;/span&gt;，其中针对能量为0的区间贡献&lt;span class=&#34;math inline&#34;&gt;\(2*3/28 = 0.214\)&lt;/span&gt; 个小球，而宏观态2中对能量为0的也贡献&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球，同样宏观态3中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球而宏观态4中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.107\)&lt;/span&gt;个小球，这样会有&lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;个小球贡献给能量为0的区间，同样我们可以得到其他能量区间上有多少小球。这个小球的能量分布就是我们打算求的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-07-24-boltzmann-distribution-1_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在这个演示里，能量分布似乎跟数目是线性的，但是如果我们把系统放宽，加入更多的球跟能量，这条曲线应该是指数分布的且指数为负。当然我知道你们不信，这里我们用 Eisberg &amp;amp; Resnick 在量子物理教材中描述来进行一个理论推导。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;能量分布的数学形式&lt;/h2&gt;
&lt;p&gt;如果我们看到两个小球，这两个小球来自同一个能量分布，那么一个小球其出现在&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_1)\)&lt;/span&gt;，出现在&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;能量区间上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_2)\)&lt;/span&gt;。我们考虑一个特殊情况，一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;，发生这种情况的概率应该是两个概率的乘积，也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1) \times f(E_2) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后，我们考虑所有宏观态中能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况，因为我们进行了区间划分，所以能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况只出现在所有能量和为这个数的小球组合里。由于我们之前假设了所有微观态是等概率的，那么出现这个能量和的小球组合方式也是等概率的。那么在某种程度上，会有下面的公式成立：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1)+f(E_2) = h(E_1+E_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们可以认为上面那个特殊情况，也就是一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;的概率是属于&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;这个能量分布的，那么根据第一段的推理出现这个情况时的概率应该是乘积。也就是两个能量和的出现概率既是两个独立能量区间概率的和又是乘积，那么满足这个条件的数学形式只能是指数形式。那么我们可以得出，在某个能量区间上小球的概率密度函数应该是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E) = A e^{-E/E_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里面的常数&lt;span class=&#34;math inline&#34;&gt;\(E_c\)&lt;/span&gt;是描述独立于小球的系统变量，玻尔兹曼经过推导得出这一部分是系统温度（总能量）的线性函数，所以我们就有了玻尔兹曼分布的数学形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_b(E) = A e^{-E/kT}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这个分布描述了在封闭系统内不同能量的小球概率分布形式，简单说就是能量高的概率很低，而能量低的概率比较高。这个现象是客观存在的，但同时由于存在量子效应限制，有些能量段上的小球是无法稳定存在的，所以能谱分布实际并不连续，不过这就是另一个故事了。那么这个符合粒子运动的物理模型跟社会行为有什么联系呢？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为&lt;/h2&gt;
&lt;p&gt;在一个达到平衡的社会经济系统中，我们对小球的假设基本是符合现实的，能量代表财富，那么很直观的现象就是这个社会的财富分配天然就不是平均的，会基本符合指数分布。也就是说，总是少数人富有而多数人穷。那么有人会反对说发达国家都是纺锤形的，明显模型有问题。没错，发达国家可以是纺锤的，但如果把全球看成一个缓慢增长的经济体，一个区域特别稳定富有只可能是牺牲其他区域的财富增长才能实现。所谓共同富裕，在全球封闭尺度下是无法自发形成的。这里看起来跟自由市场理论有矛盾，在自由市场下，看不见的手可以调节保证一个健康的经济发展，但在小球模型里经济并不发展，只是自由分配与交易。不过换句话，在经济停滞后的交易行为应该会导致更极端的财富分配。至于说自由市场里的自发交易可以促进合理的财富分配，我觉得比较扯，更靠谱是富有者所承担的社会文化压力，要不然亚当·斯密也不至于再去写一本《道德情操论》。&lt;/p&gt;
&lt;p&gt;此外，请注意关键词：自发。也就是说不进行法律政策干预，但是如果博弈规则发生改变，那么物理系统就不那么好使了。另一个关键词是稳态，也就是如果社会整体财富增加了，那么即使分布不变，所有人生活的幸福指数也会提高。而增加社会整体财富的方式在最近这一百年的主要表现方式是科技革新，有意思的是，目前可以让财富分配更不平等的最主要动因也是科技的不平等。&lt;/p&gt;
&lt;p&gt;在接下来的两篇中，我们先看看到底这个世界的财富在不同的小球模型中会有如何的分布特征，然后再看看社会财富实际是如何分配的而物理学规律能给我们什么样的启发。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real high-throughput for LC/GC-MS</title>
      <link>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</guid>
      <description>&lt;p&gt;If someone want to know whether some compounds exist in certain samples. He always need to make pretreatment for that samples and make them into a little vial for analysis in sophisticated instruments like mass spectrum. If you want to analysis many components in one sample, you also need some separation methods like gas/liquid chromatography. How about analysis multiple samples and multiple compounds in a single run?&lt;/p&gt;
&lt;div id=&#34;pseudo-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pseudo high-throughput&lt;/h2&gt;
&lt;p&gt;If you use LC/GC-MS to analysis samples, all the efforts for high-throughput would be limited at the injection step. You could arrange 96-well plate for analysis. But wait, ONE BY ONE. If some short-live compound could survive in the pre-treatment, they would disappear in the auto-sampler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-mass-spectrum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to Mass Spectrum&lt;/h2&gt;
&lt;p&gt;Actually, similar issue happened when we perform Multiple Reaction Monitoring(MRM) to collect the intensities from different ions. If the detector could only measure one ion at one time, mass spectrum simply use a high frequency scan like 50ms or 20ms for one ion and re-construct the time profile by smooth the points into a line. To my knowledge, 15 points would fit a bell curve well for one peak with smooth. OK, if the peak width is 15s, for each ion we only need 1 scan per second as shown below. OK, that means if our instrument could reach 10ms per scan per ion, we could monitor 100 ions at the same time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-09-real-high-throughput-for-mass-spectrum_files/figure-html/sim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then how about full scan, some detectors like orbitrap and tof-tof could monitor a full scan for all ions at almost the same time. Common high resolution mass spectrum could collect more than 10 spectrum per second. If the compounds could show peaks’ width larger than 15s, we could actually collect 10 spectrum from different samples, which is the real high-throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real high-throughput&lt;/h2&gt;
&lt;p&gt;All we need to do is to synchronize the injection and mass spectrum scan. I have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this solution, we injected 6 samples at the same time. The column should be the same. Then when the samples reach MS part, they were arranged into one sequence. All we need to do is to ensure every six full scan on the mass spectrum could meet six identity sample from the LC/GC part. Then MS could collect and rebuilt six samples’ retention time - mass profile and output six data set for those samples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with Multiple channels&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this way, we need some controls on pumps to on/off when one channel’s sample get into MS for full scan. Or we could have cells to guide the samples into the slit before the lens. This option is better to avoid the cross contamination. However, we need re-design the MS ion source for multiple channels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single column with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you do not need to modify your current instruments. All you need to do is the injection of the samples by sequence without considering the full separation in former sample. However, you need a lot of efforts to deconvolution. Since it’s the same column, the separation for most ions should be same. You could build a model to capture such patterns and separate the samples by those patterns. In such way, the batch effects could be minimized. However, the requirements for data mining are maximization. I like this way.&lt;/p&gt;
&lt;p&gt;I think in the near future we would find the real high-throughput LC/GC-MS. Those devices would short the analysis time between sample collection and data acquisition. MS-based scientists could reach more interesting findings with REAL high-throughput.&lt;/p&gt;
&lt;p&gt;Happy explore!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text mining for top academic journals</title>
      <link>/en/2017/07/07/text-mining/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/07/text-mining/</guid>
      <description>&lt;p&gt;Text mining is often used to find spatiotemporal trends in news, government report and user generated contents in SNS websites. We could make sentiment analysis to find the real opinions of netizens. Also we could track the popularity of certain phases and find the connections among them. Such technique might also be useful for scientists or researchers to read papers.&lt;/p&gt;
&lt;p&gt;Scientists or researchers’ daily life is immersed by a lot of literature. Most of the them are only focused on limited area in certain subjects. However, a modern scientist should always know what had happened in all of the other subjects. Some techniques used in other research might inspire your research. The only problem is that you need a lot time reading top general journals like Science, Nature and PNAS. Wait, we actually do not need to know the technique details and all we want to know is the patterns in those journals. Well, text mining would help.&lt;/p&gt;
&lt;p&gt;The main advantage for text mining in academic journals is that academic papers always share same structures in one journals. Public academic databases such as PubMed or Google scholar could always show you the structured records for papers such as journal, authors, title, published dates and even abstracts. We could directly fetch those data and save in database. I developed scifetch package to get those data from PubMed. This package would support Google scholar, bing academic and baidu xueshu in the future. Actually I support PubMed in the first version because they have a user-friendly API and I could connect such pipe with xml2 package in a tidyverse way. Besides, easyPubMed package is also a good package to extract such data from PubMed.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data collection&lt;/h2&gt;
&lt;p&gt;Here I collected the information from all the papers published in SNP i.e. science, nature and PNAS in the past three years as xml format and clean them into a dataframe for further text mining. The search grammar could be find from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK3827/&#34;&gt;NCBI websites&lt;/a&gt; and a cheat sheet here.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/cheatsheetpubmed.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;There are 26559 papers and I will use such data for text mining. PubMed has a limitation for 10000 records per query. So we need to fetch the data multiple times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 26,557 x 7
##                          journal
##                            &amp;lt;chr&amp;gt;
##  1                        Nature
##  2                        Nature
##  3 Proc. Natl. Acad. Sci. U.S.A.
##  4 Proc. Natl. Acad. Sci. U.S.A.
##  5                       Science
##  6                       Science
##  7                       Science
##  8                       Science
##  9                       Science
## 10                       Science
## # ... with 26,547 more rows, and 6 more variables: title &amp;lt;chr&amp;gt;,
## #   date &amp;lt;date&amp;gt;, abstract &amp;lt;chr&amp;gt;, line &amp;lt;int&amp;gt;, time &amp;lt;dttm&amp;gt;, month &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;description-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description statistics&lt;/h2&gt;
&lt;p&gt;Firstly, let’s see the papers by journal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we could check the high frequency terms in the title and abstract of these papers.&lt;/p&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordtitle-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As top journals, one of the most obvious features is that they all need correction and reply. With high influences, those journals would be the best place to discuss the leading edge techniques and findings. However, Science likes U.S., glance and paleoanthropology more while Nature and PNAS like tumor to be used in the titles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordabs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, when we focused on the abstracts, something interesting happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They all focused on tumor while Nature use ‘tumour’ as a journal from U.K.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nature’s title and abstracts look similar while Science’s title always use different terms compared with their abstracts. Maybe Science’s authors like to be clickbaits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PNAS’s authors use a lot of abbreviation in their abstracts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Those top journals all like tumor, behavior and modeling and now you know how to pick up a topic to be published.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;temporal-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Temporal trends&lt;/h2&gt;
&lt;p&gt;Here we use logistic regression to examine whether the frequency of each word is increasing or decreasing over time. Every term will then have a growth rate associated with it.&lt;/p&gt;
&lt;div id=&#34;title-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find some trending terms like CRISPR, gut, and corrigendum are ‘promising’. However, some topics like Ebola, Hiv and cell differentiation would leave us. Another interesting trending is that the names of certain subjects is disappearing in those top journals like biology, chemistry, neuroscience, ecology and policy. Maybe most titles would like to focus on specific topics or certain problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Now let’s review the temporal trends of terms in abstracts during the past three years by months.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to find a clear pattern in growing words in abstracts. Maybe the abstracts focused more on technique details. However, shrinking words like viral, gene expression and pathway showed clear trends. Meanwhile, we could find some words like suggests, previously and production are discarded by the top scientist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-among-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship among words&lt;/h2&gt;
&lt;p&gt;N-gram analysis could be used to find a meaningful terms in those papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/bigramt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/biabs.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Well, climate change, transcription factor, stem cell and cancer would always be the favorite bigrams in the titles of top journals. For the abstracts, cell related topics such as function, protein and expression are always preferred. Anyway, life science is always the center of trending sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic modeling&lt;/h2&gt;
&lt;p&gt;Relationships among words could show us some trending. However, we could employ topic modeling to explore the topics as a bunch of words in the abstracts of those top journals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./images/tm.png&#34; /&gt; This topic model showed topics like climate change, virus infection, brain science and social science are other important research topics other than life science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment analysis&lt;/h2&gt;
&lt;p&gt;Text mining could also be used to find the sentiment behind those journal papers or the customs using certain words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/SA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, I think the afinn word list for sentiment analysis is not suitable for scientific literature. Some words is actually neutral in academic journals. If someone could develop a specific word list for scientists, we might find something ignored by the writing lessons on campus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here, I demo some basic text mining results for top academic journals. Just like Google trends might predict the popularity of flu, text mining for academic journal might be a good tool to reveal unknown patterns or trends in certain subjects or top journals. Besides, we could also find unique usages of some words and some tones behind the journal. Besides, such methods might work better than impact factor or H index as the evaluation tools for certain researcher, journal or institute in a dynamic view. The most attractive thing is that every scientist could use this tools through open databases and find their own answers. This is the best benefits from information era.&lt;/p&gt;
&lt;p&gt;You might read this excellent on-line &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;book&lt;/a&gt; and David Robinson’s &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;blog&lt;/a&gt; to make more findings.&lt;/p&gt;
&lt;p&gt;All the R code for this post could be found &lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/en/2017-07-07-text-mining-for-trends-in-top-journal.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>文献阅读的文本分析流派</title>
      <link>/cn/2017/06/16/nlp-literature/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/06/16/nlp-literature/</guid>
      <description>&lt;p&gt;读文献是科研人员的基本功，一方面是了解学科发展，另一方面更现实一点，就是为了发文章。起步阶段读论文一般是模仿与学习，但到了中后期如果你的视野不够开阔，很容易陷入到安全区陷阱，认为自己做自己那一小摊就挺好，其实很有可能大浪过来，全军覆没，说直白点就是申不到钱，课题与项目运转不下去，思路也会枯竭。当你去开学术会议时，那些大会报告的报告人的开场总有个全局概览的视野，这种评论是需要经验去堆的，但其实也挺虚的：你回头去看容易知道哪里有坑哪里有丘，但身处时代浪潮之中是不太容易感知趋势的。&lt;/p&gt;
&lt;p&gt;但传统基于核心关键词的检索跟全局观是本质相悖的，核心关键词往往限制了内容，虽然有利于聚焦但不利于发散与概览。不过当前文献数据空前开放，如果你有类似全局视野问题，是可以自己探索的。这里要用到一个名为自然语言处理（NLP）的工具，简单说就是我不去看单篇文献或荟萃分析，而是通过语义关系探索大量文献中的潜在模式，进而找出热点。今天我用pubmed这个免费的文摘数据库来做个演示，探索下科学研究的整体前沿，结论不一定对，但方法思路如果你能掌握并举一反三，会有发现新大陆的感觉。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据获取&lt;/h2&gt;
&lt;p&gt;数据获取思路是这样的：如果想知道整体前沿，最需要的是综合类期刊，全文的数据量我的笔记本也跑不了，就考虑摘要，这样也过滤了那些没有摘要的评论与观点，更多关注研究性论文。期刊选择为综合类的《科学》、《自然》与《美国科学院院刊》，收集2016年一整年的论文摘要，用&lt;code&gt;easyPubmed&lt;/code&gt;包来搜索并整理成相对干净的数据集。这里我只收集了题目、摘要、出版期刊与日期进行文本数据挖掘。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,815 x 6
##                                                                          title
##                                                                          &amp;lt;chr&amp;gt;
##  1 A high-throughput small molecule screen identifies synergism between DNA me
##  2                       2017 sneak peek: What the new year holds for science.
##  3                            Wolf transplant could reset iconic island study.
##  4 Scientists in Germany, Peru and Taiwan to lose access to Elsevier journals.
##  5                                                   How scientists use Slack.
##  6                 Saliva protein biomarkers and oral squamous cell carcinoma.
##  7 Reply to Galvão-Moreira and da Cruz: Saliva biomarkers to complement the vi
##  8         Cell morphology drives spatial patterning in microbial communities.
##  9                 Deborah S. Jin 1968-2016: Trailblazer of ultracold science.
## 10 Autophagy wins the 2016 Nobel Prize in Physiology or Medicine: Breakthrough
## # ... with 8,805 more rows, and 5 more variables: abstract &amp;lt;chr&amp;gt;,
## #   year &amp;lt;chr&amp;gt;, month &amp;lt;chr&amp;gt;, day &amp;lt;chr&amp;gt;, jabbrv &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;发文量&lt;/h2&gt;
&lt;p&gt;首先我们先看看着三份期刊的发文量：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这三份期刊里，PNAS发文量最大，占总数一半。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;高频词&lt;/h2&gt;
&lt;p&gt;然后我们看一下各期刊的前十大摘要高频词：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/abs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里解释一下，如果我们单纯寻找高频词其实这几个期刊都应该差不多，但这里我们用的是TF-IDF来加权筛选，这个加权不严谨的说就是这个词出现在该期刊的词频与出现在所有期刊词频的比例，通过这个值我们可以找到单个期刊比较重要的词。我们可以看到肿瘤与行为均出现在三个期刊的十大关键词中，推测相关研究应该是去年的重点。此外，《自然》与《美国科学院院刊》都出现了模型这个词。就特色而言，《自然》去年更关注造血过程、信号传递与衰老问题；《科学》杂志则关心磷酸化、spo11蛋白与火山口还有小尺度问题；《美国科学院院刊》主题特色不算明显，但比较喜欢强调研究重要性。&lt;/p&gt;
&lt;p&gt;如果我们只考虑题目里的文字呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/title-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们可以看出，《自然》上的论文题目跟摘要内容契合度比较高；《科学》上论文题目喜欢出现中美的国家标签；《美国科学院院刊》看意思题目里专业名词比较多。此外，三份期刊的题目里都出现了勘误，这倒是前沿高影响力期刊的特点：容易被质疑。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;词关系&lt;/h2&gt;
&lt;p&gt;看完整体你应该想到，单个词并非孤立，那么这些词之间会不会有相关性呢？这个问题我们也可以用NLP工具来研究：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-2.png&#34; width=&#34;672&#34; /&gt; 其实这个技术更常见，平时你用的输入法就实现去考察一些字词的关系，然后让其出现的排序更符合常识。这里我们可以看到，从题目里我们能看到气候变化、干细胞以及前面提到的勘误问题。从摘要里我们则会发现大多数是生物相关的主题，也就是前沿科研应该是生命科学在导向。但到目前为止我们都是把这一些文本当成一个整体，但科学是分科的，也就是有不同的主题，此时我们就要用到主题模型来探索去年前沿科研关注的主题分类。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;主题模型分类&lt;/h2&gt;
&lt;p&gt;所谓主题模型，就是通过探索字词间内部关系对文本进行分类的模型，举例来说某个潜在的主题包含7个关键词，如果某篇文章命中6个，那么这篇文章大概率就属于这个潜在主题。当然，现实生活我们并不知道这些潜在主题会是什么，但通过隐含狄利克雷分布，也就是LDA方法我们就可以去探索结构，然后去拟合实际经验。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/topic-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;从上面我们可以看出，有些探索出来的主题大概我们知道是哪个领域的，有些则属于误判或者说界限不明显的综合领域，这说明跨学科研究正在崛起。其中，我能识别出来的主题大体有癌症、脑科学、病毒、社会行为、基因组、膜蛋白结构、气候变化、进化、动态系统、材料。总体来看，细胞生物学与分子生物学还是主流，但病毒、气候变化等问题导向的学科也在发展。其实也可以直接分析10年的时间变化趋势，不过这个就留成课后题吧（其实是我个人电脑跑不动）。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;情感分析&lt;/h2&gt;
&lt;p&gt;一般认为科研人员都是比较乐观的，但其实文字背后究竟是否乐观可以用文本的情感分析来回答。这个分析的原理就是事先找个标注过情感的语料库，然后通过语料库与词频来分析具体文本的情感倾向性。正常这个语料库是要自己根据语境去构建的，例如商品的好评差评，但作为资深懒汉，我直接用了现成的AFINN语料库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sen-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果基本符合乐观为主的预期，不过按说有些词在科研中属于中性词，我们可以通过这个分析来考虑论文写作的用词方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;其他&lt;/h2&gt;
&lt;p&gt;其实这只是一个很初步的分析，我甚至没用用到引用与被引用的关系，也没有考虑作者与研究机构的时空分布特征，但类似这样的文本分析应该是一个现代科研人员所具备的属性。这种分析的好处在于你不是在采样，而是直接分析所拥有的整体，也就几十兆的文本量，如果你电脑跑得动，把十年二十年的文献沿革都可以概览一下，这是这个时代给我们的红利，不要白不要。&lt;/p&gt;
&lt;p&gt;你可以研究一个大牛几十年的论文发表来发现其独到的眼光；也可以针对某个期刊挖掘其关注点的变更；还可以构建自己认可的课题组的文献库，通过其发表内容探索同行那些自己都没意识到的行为改变。这个时代学科内的经验贬值飞速，很多东西没必要闭门造车慢慢悟，利用开放数据的便利性你可以很快了解整体学术动态，这样不至于随波逐流。更麻烦的是如果你不懂而别人懂，那你将很容易体会到别人眼神中的怜悯，做一个好奇心使然的科研人员，现在起步从来都不晚。&lt;/p&gt;
&lt;p&gt;更重要的是，这类技术本质是让你满足好奇心的，你可以用这个来了解社会，例如纽约时报就给个人提供API，你可以看看其对川普用词风格的变化；为什么最近比特币搜索指数集中在拉美？欧洲吸引难民究竟是政治正确还是劳动力人口不足？不要等着看新闻来指导自己，要学会发现生活中的闪光点；不要通过键盘上情感喧嚣来面对社会，要用键盘甚至语音编程（我果然很自然的想到了最懒的方法）从繁复的公开数据中挖掘趋势；不要总是等着大牛来带，在未知的领域人人都可能成为大牛，你需要掌握一些实现方法而已，你甚至不需要太了解算法细节（会忘，比如我），但要有自己的兵器库随想随用。你不需要带着目的性去学，这说到底只是一种生活方式，你变强了也秃了的可能性是存在的（你能否感到我最近在看漫画）。&lt;/p&gt;
&lt;p&gt;本文实现代码可见我的&lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/cn/2017-06-16-nlp.Rmd&#34;&gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;想要了解世界吗？想要的话可以全部給你，去找吧！我把所有的线索都放在互联网上了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;参考文献&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://tidytextmining.com/&#34; class=&#34;uri&#34;&gt;http://tidytextmining.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection of open source software platform for metabolomics or lipidomics</title>
      <link>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</guid>
      <description>&lt;p&gt;Recently I reviewed some platforms for metabolomics or lipidomics and tried to find out which one is currently available to my demands. I knew some instrumental company supplied software or solutions for metabolomics and lipidomics. I just disliked them since they concealed too many details about data processing which made omics studies as a magic box like Artificial Neural Network. It gains nothing for scientist to get insights from the data and you might only follow the workflow defined by the company. I read some papers using those kind of software and felt the authors know little about what they performed. Data analysis for metabolomics or lipidomics is a systems engineering. If someone really want to use this tools, just choose a open source platform and inspect the code when you needed. Otherwise, your work could be replaced by AI someday. It’s not a joke. Here is a summary for the selection of mass spectrum based metabolomics or lipidomics software platform.&lt;/p&gt;
&lt;div id=&#34;principles-for-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principles for selection&lt;/h2&gt;
&lt;p&gt;A decent solution should have functions covering the following required functions or features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open source: as I said before, researcher could benefit a lot from open source community. Based on your experiences, Java, matlab, python and R would be your choices. I liked R most while python might dominate everything in the further.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data input/output: this work could be done by msconvert from &lt;a href=&#34;http://proteowizard.sourceforge.net/tools.shtml&#34;&gt;proteowizard&lt;/a&gt;. I preferred to install msconvert directly on the instruments and convert the vendor files into mzML or mzXML files to perform further analysis. I just dislike Windows. On the other hand, some software from the instruments could output CDF files, which is also nice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turn the Raw data into mass-retention time matrix: single data file from mass spectrum would always contain matadata and profile data. The former container has records for that data and the latter were always full scans of mass spectrum indexed by retention time. Then you need to map such data into a matrix because the following peaks picking and alignments always based on algorithms for matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peaks picking: you also need functions to pick up the peaks from mass-retention time matrix. Functions like centWave or isotope-based algorithms would help to find the peaks. Sometimes you need to bin the matrix first before peak picking and such requirements should be carefully checked. Anyway you could just use one function to perform peaks picking in most software. The output of this step would be a peak list with intensity, mass and retention time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Retention time correction: when you process more than one data file such as technique replicates, the drift of retention time would make the final peak list contained replicated peaks. Someone use certain peaks to corrected the data, while I prefer some global similarity analysis based algorithm to undertake this task. However, some software might add one step to group the peaks before and/or after retention time correction. Anyway, the output of this step is the peak list from one group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Batch effects correction: this is another issue from analysis procedure and batch effects would also bury signals from noise. Researchers from analytical chemistry always use random experiment design and pooled sample to check if the batch effects exist. However, I think the most important things were correction. Just treat a series of samples suffered batch effects and use some statistical analysis to correct them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical analysis: when you get the peaks list, you could perform the following supervised or non-supervised analysis to find bio-markers or data pattern. I think &lt;a href=&#34;http://www.metaboanalyst.ca/&#34;&gt;Metaboanalyst&lt;/a&gt; could satisfy most researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Annotation or identification: the peaks annotation is also important. &lt;a href=&#34;www.hmdb.ca&#34;&gt;HMDB&lt;/a&gt; and &lt;a href=&#34;http://www.lipidmaps.org/&#34;&gt;LIPID MAPS&lt;/a&gt; would be the basic version for mapping peaks to compounds. In most cases, MS/MS data would be used. I think most of MS/MS dataset could be separated handled by &lt;a href=&#34;https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp&#34;&gt;GNPS&lt;/a&gt; or &lt;a href=&#34;https://metlin.scripps.edu&#34;&gt;Metlin&lt;/a&gt; and predicted by &lt;a href=&#34;http://www.csi-fingerid.org/&#34;&gt;FingerID&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pathway analysis: sometimes we also need to know the relationship among compounds and pathway analysis is always performed for that purpose.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Platform&lt;/h2&gt;
&lt;div id=&#34;xcms-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS online&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xcmsonline.scripps.edu/landing_page.php?pgcontent=mainPage&#34;&gt;XCMS online&lt;/a&gt; is hosted by Scripps Institute. If your datasets are not large, XCMS online would be the best option for you. Recently they updated the online version to support more functions for systems biology. They use metlin and iso metlin to annotate the MS/MS data. Pathway analysis is also supported. Besides, to accelerate the process, xcms online employed stream (windows only). You could use stream to connect your instrument workstation to their server and process the data along with the data acquisition automate. They also developed apps for xcms online, but I think apps for slack would be even cooler to control the data processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PRIMe&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://prime.psc.riken.jp/Metabolomics_Software/&#34;&gt;PRIMe&lt;/a&gt; is from RIKEN and UC Davis. It supports mzML and major MS vendor formats. They defined own file format ABF and eco-system for omics studies. The software are updated almost everyday. You could use MS-DIAL for untargeted analysis and MRMOROBS for targeted analysis. For annotation, they developed MS-FINDER and they also developed statistic tools with excel. This platform could replaced the dear software from company and well prepared for MS/MS data analysis and lipidomics. They are open source, work on Windows and also could run within mathmamtics. However, they don’t cover pathway analysis. Another feature is they always show the most recently spectral records from public repositories. You could always get the updated MSP spectra files for your own data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OpenMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.openms.de/&#34;&gt;OpenMS&lt;/a&gt; is another good platform for mass spectrum data analysis developed with C++. You could use them as plugin of &lt;a href=&#34;https://www.knime.org/&#34;&gt;KNIME&lt;/a&gt;. I suggest anyone who want to be a data scientist to get familiar with platform like KNIME because they supplied various API for different programme language, which is easy to use and show every steps for others. Also TOPPView in OpenMS could be the best software to visualize the MS data. You could always use the metabolomics workflow to train starter about details in data processing. pyOpenMS and OpenSWATH are also used in this platform. If you want to turn into industry, this platform fit you best because you might get a clear idea about solution and workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mzmine-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MZmine 2&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://mzmine.github.io/&#34;&gt;MZmine 2&lt;/a&gt; has three version developed on Java platform and the lastest version is included into &lt;a href=&#34;https://msdk.github.io/&#34;&gt;MSDK&lt;/a&gt;. Similar function could be found from MZmine 2 as shown in XCMS online. However, MZmine 2 do not have pathway analysis. You could use metaboanalyst for that purpose. Actually, you could go into MSDK to find similar function supplied by &lt;a href=&#34;http://www.proteosuite.org&#34;&gt;ProteoSuite&lt;/a&gt; and &lt;a href=&#34;https://www.openchrom.net/&#34;&gt;Openchrom&lt;/a&gt;. If you are a experienced coder for Java, you should start here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xcms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/release/bioc/html/xcms.html&#34;&gt;xcms&lt;/a&gt; is different from xcms online while they might share the same code. I used it almost every data to run local metabolomics data analysis. Recently, they will change their version to xcms 3 with major update for object class. Their data format would integrate into the MSnbase package and the parameters would be easy to set up for each step. Normally, I will use msconvert-IPO-xcms-xMSannotator-metaboanalyst as workflow to process the offline data. It could accelerate the process by parallel processing. However, if you are not familiar with R, you would better to choose some software above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emory-mahpic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Emory MaHPIC&lt;/h3&gt;
&lt;p&gt;This platform is composed by several R packages from Emory University including &lt;a href=&#34;https://sourceforge.net/projects/aplcms/&#34;&gt;apLCMS&lt;/a&gt; to collect the data, &lt;a href=&#34;https://sourceforge.net/projects/xmsanalyzer/&#34;&gt;xMSanalyzer&lt;/a&gt; to handle automated pipeline for large-scale, non-targeted metabolomics data, &lt;a href=&#34;https://sourceforge.net/projects/xmsannotator/&#34;&gt;xMSannotator&lt;/a&gt; for annotation of LC-MS data and &lt;a href=&#34;https://code.google.com/archive/p/atcg/wikis/mummichog_for_metabolomics.wiki&#34;&gt;Mummichog&lt;/a&gt; for pathway and network analysis for high-throughput metabolomics. This platform would be preferred by someone from environmental science to study exposome. I always use xMSannotator to annotate the LC-MS data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;others&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://genomics-pubs.princeton.edu/mzroll/index.php?show=index&#34;&gt;MAVEN&lt;/a&gt; from Princeton University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vpr.colostate.edu/pmf/wp-content/uploads/sites/23/2015/06/Broeckling_2014.pdf&#34;&gt;RAMclustR&lt;/a&gt; from Colorado State University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/html/MAIT.html&#34;&gt;MAIT&lt;/a&gt; based on xcms&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yufree/enviGCMS&#34;&gt;enviGCMS&lt;/a&gt; from me&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.ebi.ac.uk/metabolights/&#34;&gt;Metabolights&lt;/a&gt; for sharing data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;All of the platform above could be used for metabolomics and lipidomics. However, best selection would be based on your programming skills and the popularity in your research area. Every tool need training before analysis your data and you could choose one randomly and be focused on the source code for one day. If you feel you could handle it, just use it. Otherwise select another one. Enjoy and fight with the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>留学生、隔离与适应</title>
      <link>/cn/2017/05/22/study-aboard/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/22/study-aboard/</guid>
      <description>&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;留学生&lt;/h1&gt;
&lt;p&gt;作为国产土鳖，在海外做博后接触最多的除了土著就是华人移民、各国留学生与国内访问学者，这里面还是有很多有意思的现象，而这些现象其实对应了几篇非常有意思的论文。&lt;/p&gt;
&lt;p&gt;首先我们看一下国内来的留学生群体。目前国内的留学越来越低龄化，很多接触到的国内来的本科生，往往在国内读个一年高中就跑来这边读高中，用这边高中学历来申请学校。大学读完了基本都是身份导向，学签转工签，工签转枫叶卡，枫叶卡转公民。你如果问为什么不回国，他们的回答往往不是常见的国内机会不好，而是认为国内没有认识的人，主流社交关系都在国外，一回国跟回炉重造差不多。这是个死循环，出国年龄越小，人际关系越在海外，越不可能回国。而且很多本科生的高中同学其实在国内也是高中同学，有个留学生跟我说他们国内一个高中班，到了高二整建制的来了北美，这种情况怕是一个也回不去了。&lt;/p&gt;
&lt;p&gt;另一个有意思的现象是留学生中还存在鄙视链。高中过来的鄙视大学过来的，大学过来的觉得研究生过来的太多国内习气不合群，而他们又都鄙视国内交流项目过来的，认为交流生都是拿国家钱混经历，选的课还简单，语言也不行。同样的鄙视链在华人移民中也存在，最早的移民多是劳工移民，八九十年代流行技术移民，前十年是投资移民，最近是留学移民，鄙视链基本就是按照出国早晚与学历来的。国外生活处处需要职业代理人，找华人并不见得比找老外有经济优势，但年龄约长，越喜欢找固定的华人代理人，其实老外也一样。我跟一个刚办移民的博后聊天，他海外博士曾在国内工作过一年，但终究还是觉得国内环境不适应跑出来了，他目前就打算拿身份了而且也坦然说不希望国内发展太好，因为这样就会说明自己选择国外身份是错误的，他希望国内能发展到过的很舒适但比国外还差一点点的状态。这个想法大概在留学生跟移民的内心里都有点吧，不信你去mitbbs上转转就知道了，我倒觉得这个现象其实背后有博弈论的影子。&lt;/p&gt;
&lt;p&gt;还有一个现象是留学生置产。这是个经济问题，很多学生来读书到银行开户问的第一件事就是房价，一般来说，本科来了买房，交了首付把不住的房间出租，租金可以抵消月供，等毕业要回国或外地工作时转卖，读书的时间房价的上涨其实还能赚一笔作为工作启动资金。也许你觉得首付存在外汇管制比较难，我只能说私下的外汇交流挺多的，存在庞大的代理人市场，要不然比特币也不会中国人玩的多，此外国外有定居亲戚朋友需要国内置产的两方利益交换就可以了，当然这个比较吃信任，所以只能亲戚朋友。有位银行职员曾跟我吐槽中国人怎么那么有钱，经常全款买房，买的车也都是好车，一身加拿大鹅，我赶忙解释那些人国内也是有钱人，不代表平均水平，另外奢侈品国内定价比国外高一大截，他们买是因为便宜。&lt;/p&gt;
&lt;p&gt;再就是家长陪读现象。有次跟访学博后聚餐，很意外发现带孩子来的孩子都是学校本科生，说白了很多访学博后出国一方面是学术交流，另一方面也算是陪读。另外陪读家长一般都是母亲，有当地华人说美国西海岸是小三村，加拿大东部则可看作正房陪读村，至于一家之主大都身居国内经商赚钱。不得不说还是国人精打细算，懂得规划自己，处处“精致”。&lt;/p&gt;
&lt;p&gt;然后我们再看下另一个群体，港台留学生或移民，一个特别的现象就是这两拨人互相独立，就连华人学生会也是两个，显著区别就是语言，港台留学生英语更好，私下用粤语。据说97年左右大量香港人移民加拿大，后来又回去一拨，但这也造就了这个群体的人口基础。不过我遇到的港台人都是比较友善的，基本默认不聊政治，也听说过有台湾留学生打飞的回去投票展示优越性，不过台式民主也算是一道别样风景了。&lt;/p&gt;
&lt;p&gt;还有三个我们的近邻留学群体，一个是韩国人群体，一个是日本人群体，另一个就是印巴人群体。韩国人群体跟中国人比较像，勤奋好学，比较鲜明的特点就是韩国人专属教堂，感觉他们的传教士也算是一道风景了。日本人群体则很难发现，因为他们国际留学生读学位的非常少，一方面是因为本国好学校不少，另一方面半终身雇用的日式企业对大三学生要搞什么内定实习，你要跑国外了，回去根本找不到工作，也就是海归在日本并不吃香。但也有例外，那就是如果你去语言学校溜达，会突然发现扎堆的日本人，打听下才知道日本留学主体大都是学语言的，为了企业国际化，语言学校没有学位但有语言环境，时间也就几个月，而且女生比例奇高。这一方面说明日本人留学其实为了国内就业，另一方面，出来读的也大都是不喜欢日式大男子主义的女生，这个现象我们后面分析。至于印巴人，虽然英语带口音，但真的很主动去交流，抱团现象明显，家庭观念强。其比较特色的就是留学生之间相互认识，互相扶持，确实存在移民一个人，带来一大家的现象，而且普遍没有任何归国意愿。&lt;/p&gt;
&lt;p&gt;再有就是中东、南美与东欧留学生群体。这三个群体不知为何经常系统性被忽略，其实也都各具特色。中东留学生大都是来自伊朗，也是勤奋好学，应该说伊朗是个被制裁太多的地方，好好的从世俗国家搞成宗教国家了。但在那个人口基数上人才数量很庞大，灯塔国不欢迎他们，就跑枫叶国了。另外就是我见过的都很能说，滔滔不绝，喜欢打听事，这种八卦文化在中东据说比较普遍。南美留学生也属于比较抱团的，但模式是巴西人单独抱团，其他人抱团，估计也属于语言隔离。南美上世纪陷入中等收入陷阱，到今天感觉还没缓过来，但新兴领域的科研势头也很猛。跟南美人打交道要注意其宗教信仰与家庭结构，一般男士比较健谈活泼，但当家的其实是女士，组里的巴西小哥为了给女友买香水也吃了一个月的简餐。南美人相比中国人更熟悉日本人，可能是南美有数量不小的日本移民，跟他们聊日本动漫会发现也基本就是《七龙珠》跟《足球小子》，对了南美人周末会踢足球，他们主要社交就是足球与健身。东欧留学生跟我们思考方式也比较像，可能是冷战造就的，人口中波兰人非常多，语言上英文都挺好，有机会要去尝试波兰水饺，学术上对数学非常自信，也比较勤奋。&lt;/p&gt;
&lt;p&gt;除了上面这些，还有比较小众的西欧留学生跟东南亚留学生，接触很少，非洲留学生在国内天天见，这边并不多。最后就是加拿大当地人，普遍喜欢嘲笑美国人，特别是大选之后，但比较安于现状，对海外留学生存有迷之优越感但又表现的非常尊重你且尊重规则与多元文化，有时候让人哭笑不得。他们比较喜欢跟人打交道的专业，所以理工科基本是留学生天下，有自己的圈子，留学生基本进不去，但CBC就可以，主要是关注话题不同，脸书跟推特是他们第二家园，给我感觉就是单纯、理想主义但内里是有点排外的并且不太愿意主动了解世界其他“落后”地方。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;隔离&lt;/h1&gt;
&lt;p&gt;好了，扯了那么多的现象，现在说点理论。我现在感觉国内外最大的不同就在于国内其实是一元文化而国外存在更多的多元化，这个差别很可能是很多问题的根源。我们用日本来举例，日本就是单一民族国家，当其做学生时，也是大量派出留学生到海外学习，但当其发展起来后，其留学生就更多是实用主义了。中国虽然不是单一民族国家，但基本可以看作单一文化国家，中国人在海外两极分化比较重，要么就彻底认同海外生活，也不打算回来了；要么就根本不认同，在海外构建自己文化圈或就是实用主义，学成归国。但目前来说，留学生中前者比例占优，但如果国家能均衡发展到日本那个程度，后者比例就会提高。而且由于20-30岁的人际关系对事业初期发展很重要，所以出国年龄越早，前者比例会越高，这个也是很多海外优秀人才不想回去的主要原因，担心回去被国内熟人文化玩死。&lt;/p&gt;
&lt;p&gt;国外的多元化则是陌生人文化主导，强调反歧视与文化尊重，更简单的词汇就是政治正确。国内来的人会觉得有时候政治正确会效率低下，例如吃顿饭要先发个调查表问问食物禁忌与是否素食主义；建筑物设计里必须有残疾人车位与通道且经常见到所有车位都满了但残疾人车位还是空着一排；平时聊天需要注意各种雷区…这些跟前面提到的现象有关，一个社会如果是移民社会，那么所有团体都会为自己的文化与习惯争取政治上的尊重，这样的社会就存在单一文化社会里根本就意识不到的不尊重。例如你在沙特可以随意说娶多个老婆但你就是不能说喝酒的事，同样的话放到国内就成了无法理解的论题，而多元社会的人群构成很复杂，所以生活在这样的社会，你会感到所有人对你的尊重，前提是你需要政治争取，如果不争取，还不如单一文化社会，歧视更严重。&lt;/p&gt;
&lt;p&gt;这可能是国内推广女权、LGBT还有其他反歧视最大的难题，如果你生活在多元社会，权利分散而文化多样，大家就可以讨论小团体的问题。但如果这个国家文化惯性比较大，那么这个问题就变成了要么西风压倒东风，要么东风压倒西风，结果很容易把争取权利的一方搞得极端化与妖魔化。不过应该说虽然国家是单一文化的，但城市却可以是多元的，越是大城市与新城市，那些在原有环境不满的多元团体就容易聚集，最终发现自己的局部绿洲。灯塔国就是这么构建出来的，不管前提是原住民几乎死光。而前面提到日本留学生女性为主，很多人其实是不打算回日本，因为那里大男子主义横行，女性事业发展空间有限，此处不留爷，自有留爷处。&lt;/p&gt;
&lt;p&gt;这本质上是隔离问题，所有人都希望生活在最尊重自己的环境中而拒绝不舒服的地方，人又都是两足动物，所以不高兴就用脚投票。最后结果就是大家都在自己认为最适应的圈子里抱团取暖，不关心其他人的生活。这个隔离问题我在之前说过很多次，而且我不认为这是个很好的现象，或者说隔离现象还有一个孪生现象——适应。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;适应&lt;/h1&gt;
&lt;p&gt;所有人都是用脚投票，但其实这有个隐含前提，那就是已经做出了选择，观点明确。现实中，这个前提很有问题，多数人是在观察观点而不是发表意见，投票时不见得全心全意。目前我们可以看到两种聚集模式，一种就是按照谢林隔离模型所描述的种族隔离聚集，另一种则是以色列正统犹太人的街区黑化模型，前者我提过很多次不赘述，后者的含义则是当地人转化了外来人的文化，从外来人的角度就是适应了当地的生活方式。所以一个人跑到一个陌生的地方实际上有两种选择：融入或逃离。如果这个人不愿意适应当地文化，那么他就会逃离或者自我隔离；而如果他愿意去适应，就会融入新环境并从内部加入自己文化的组分。&lt;/p&gt;
&lt;p&gt;中国文化其实是很欢迎其他文化融入的，但比较麻烦的是不太欢迎整合其他文化。同样的问题也出现在西方国家，表面上是很欢迎移民但内在却排斥。所不同的是，西方国家存在政治正确的斗争来促进适应的发生而中国文化尊重传统基本是跟政治两条平行线，自由发展。所以适应性问题其实应该当成某种现代人素质进行理解式宣传而不是填鸭式宣传。所有人都不喜欢听别人说自己做错了，但引导他自己得出自己观点需要改进或变化是很有必要的，否则就是单纯情绪宣泄与抱团取暖，并不解决实际问题。&lt;/p&gt;
&lt;p&gt;适应问题其实也不是单一文化国家与多元国家之间的转换问题，搞成多元化却有可能出现“政治正确”而加深隔离现状，而是一个普遍存在尚未解决的问题，古话说就是要达到“和而不同，周而不比”的状态。目前大趋势是表面的文化多元化与内在的认知割裂化，不论国内国外，不论网上网下，不论职业年龄，具体怎么处理，只能说是个个人持续内心斗争决策与对外权益斗争的过程，很难一劳永逸。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;选修c站密歇根大学的模型思考课程，阅读参考教科书&lt;/li&gt;
&lt;li&gt;选修SFI的复杂性科学导论课程，阅读参考论文&lt;/li&gt;
&lt;li&gt;选修c站斯坦福与UBC合开的两门博弈论课程，阅读参考论文&lt;/li&gt;
&lt;li&gt;这是我的相关笔记本：yufree.cn/notes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最近重心转到科研，不会三个主题去写了，比较累。上面的参考文献如果能通读，则会显著提高对社会的了解，就这样。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>组学实验设计中的功效分析</title>
      <link>/cn/2017/05/21/omics-power-analysis/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/21/omics-power-analysis/</guid>
      <description>&lt;p&gt;最近设计了一组代谢组学实验，因为用到动物所以需要学校动物保护委员会的许可，完成了一系列理论学习与实验室参观后，主管人员要求提供预实验结果与功效分析来确定动物数量。这个要求就诡异了，组学实验同时测上千个指标，这功效分析根本无从谈起。但因为是必填项，硬着头皮去查了下资料，如果是代谢组学数据，确实基本没人提功效分析，有也是针对特定测量值。然而，蛋白质组学跟基因组学都曾经讨论过这个问题，这里我大体总结下。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;功效分析&lt;/h2&gt;
&lt;p&gt;常规功效分析至少需要显著性水平、组内标准差、组间差异、功效及检验方式来确定样本数。其实说白了就是知道想看到一个显著差异所需要的最小样本数，如果不能达到这个数，那么你这个实验基本可以当成白干了。功效分析跟显著性分析关注点并不一样，前者关心看到的差异真不真，后者关心这个差异存不存在。一般而言，控制实验都能看到差异，所以关心这个差异靠不靠谱其实对实验设计很重要，而具体表现就是样本数。样本越大越容易看出细微差异，如果样本数固定，那么可以通过功效分析测定哪些差异靠谱。从上面我们可以总结出两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;功效分析需要显著性水平的设定&lt;/li&gt;
&lt;li&gt;功效分析是知道多个求一个，这个数可以是样本数，也可以是差异本身&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;组学实验&lt;/h2&gt;
&lt;p&gt;组学实验最大问题就是同时测定两组样本的多维属性，这里面显著性水平要考虑整体错误发现率。同时，功效分析对于单一变量考察是没什么问题的，但如果你同时测量多个变量，那么这个样本数对不同变量应该是不一样的。你需要对结果进行筛选，而这个筛选反过来影响样本数。此外，组学实验中存在大量白噪音，不考虑这个问题的功效分析也是有问题的。我们通过下面的例子来说明。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BiocParallel)
library(xcms)
## Load 6 of the CDF files from the faahKO
cdf_files &amp;lt;- dir(system.file(&amp;quot;cdf&amp;quot;, package = &amp;quot;faahKO&amp;quot;), recursive = TRUE,
         full.names = TRUE)

## Define the sample grouping.
s_groups &amp;lt;- rep(&amp;quot;KO&amp;quot;, length(cdf_files))
s_groups[grep(cdf_files, pattern = &amp;quot;WT&amp;quot;)] &amp;lt;- &amp;quot;WT&amp;quot;
## Define a data.frame that will be used as phenodata
pheno &amp;lt;- data.frame(sample_name = sub(basename(cdf_files), pattern = &amp;quot;.CDF&amp;quot;,
                      replacement = &amp;quot;&amp;quot;, fixed = TRUE),
            sample_group = s_groups, stringsAsFactors = FALSE)
## Read the data.
raw_data &amp;lt;- readMSData2(cdf_files, pdata = new(&amp;quot;NAnnotatedDataFrame&amp;quot;, pheno))
## Find peaks 
cwp &amp;lt;- CentWaveParam(snthresh = 20, noise = 1000)
xod &amp;lt;- findChromPeaks(raw_data, param = cwp)
## Doing the obiwarp alignment using the default settings.
xod &amp;lt;- adjustRtime(xod, param = ObiwarpParam())
## Define the PeakDensityParam
pdp &amp;lt;- PeakDensityParam(sampleGroups = pData(xod)$sample_group,
            maxFeatures = 300, minFraction = 0.3)
## Group the peaks
xod &amp;lt;- groupChromPeaks(xod, param = pdp)
## Fill in peaks with default settings. Settings can be adjusted by passing
## a FillChromPeaksParam object to the method.
xod &amp;lt;- fillChromPeaks(xod)
## Get the data matrix
data &amp;lt;- featureValues(xod, value = &amp;quot;into&amp;quot;)
## Get the complete cases
data0 &amp;lt;- data[complete.cases(data),]
dim(data0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 243  12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在上面的示例中，两组数据每组样本是6个，共提取出了243个峰的完整数据，那么，此时的功效分析首先要确定整体错误发现率。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(genefilter)
## Get group infomation
lv &amp;lt;- as.factor(pData(xod)[,2])
## Get median difference and standard deviation in one group
tr &amp;lt;- rowttests(data0,lv)
## Estimates the proportion of true null p-values, for FDR
ts &amp;lt;- mean(abs(tr$statistic),na.rm = T)
library(qvalue)
pi0 &amp;lt;- pi0est(tr$p.value)$pi0
hist(tr$p.value,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-05-21-omics-power-analysis_files/figure-html/FDR-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从上面我们可以看出这一组多重比较数据的p值分布符合均匀分布, 道理上说这组数据基本可以看作没有多少差异，但直方图却显示应该存在一些有显著差异的峰。那么我们基于当前数据可以进行一下功效分析，看看究竟差异多大可以认为是真的。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get median difference and standard deviation in one group
dm &amp;lt;- median(abs(tr$dm))
sd &amp;lt;- median(rowSds(data0[,1:6]))
## Get the difference
library(ssize)
## Get the differences number fitting the power in this DoE
power.t.test.FDR(n = 6, sd = sd, FDR.level = 0.05,power = 0.8, pi0= 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 6
##           delta = 6149021
##              sd = 366486.2
##       FDR.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(tr$dm&amp;gt; 6149021)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get the sample numbers fitting the power and 50% differences true in this DoE
power.t.test.FDR(delta = dm, sd = sd, FDR.level = 0.05,power = 0.8, pi0= 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 9675.235
##           delta = 136197.7
##              sd = 366486.2
##       FDR.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们大体可以认为，在当前实验设计下，只有1个峰的显著差异是靠谱的。同理，也可以计算出如果想看到50%的差异为真，我们每组需要9675个样本。&lt;/p&gt;
&lt;p&gt;同时，如果不考虑错误发现率的结果则是：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power.t.test(n = 6, sd = sd, sig.level = 0.05,power = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 6
##           delta = 658041.6
##              sd = 366486.2
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(tr$dm&amp;gt;497799.2,na.rm = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power.t.test(delta = dm, sd = sd, sig.level = 0.05,power = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      Two-sample t test power calculation 
## 
##               n = 114.6297
##           delta = 136197.7
##              sd = 366486.2
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当前实验可以发现27个差异是靠谱的，或者需要每组115个样品才能表示出一半的差异是真的。&lt;/p&gt;
&lt;p&gt;产生上述问题的本质在于高通量数据的错误发现率控制降低了发现差异的p值，所以更少的差异是真的。同时如果严格控制错误发现率，那么所需要的样本数会非常多。此外，这个结果提示我们如果样本数不多，那么你其实只能对那些差异很大的峰给予信心。&lt;/p&gt;
&lt;p&gt;但上面的计算有一个最大的问题，就是组内标准差其实差异很大，此时应考虑每个样本的情况:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;power &amp;lt;- 0.8
alpha &amp;lt;- 0.05
pv &amp;lt;- tr$p.value
df &amp;lt;- cbind.data.frame(tr$dm,rowSds(data0[,1:6]),alpha,pv)
df &amp;lt;- df[df$pv&amp;lt;0.05,]
rs &amp;lt;- vector()
for (i in c(1:nrow(df))){
        r &amp;lt;- power.t.test.FDR(delta = abs(df[,1][i]), sd = df[,2][i], FDR.level = 0.05,power = power, pi0 = 1)
        rs[i] &amp;lt;- r$n
}
sum(rs&amp;lt;6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样，我们发现会有0个差异是真正显著的，这比使用平均样本控制得到的少，说明单个去看其实更难找到真正有差异的峰。&lt;/p&gt;
&lt;p&gt;功效分析不一定会得到跟FDR控制更少的峰，而且FDR控制的算法设计里实际也考虑了类似的过程，但是从全局分析的功效分析对于实验设计非常有用。当你进行了一组预实验，功效分析可以告诉你结论中多少峰是靠谱的，如果一个都没有，那么就增加样本量吧。如果你的研究目的是至少发现一个峰是靠谱的，也就是生物标记物研究，那么此时功效分析所需的样本数最好再多一个，否则结论可能无意义。&lt;/p&gt;
&lt;p&gt;下面就把上述过程整合成一个函数，输入提取好的MSnExp对象与功效值、p值与q值的阈值就可以返回一个满足条件的峰列表，这个函数只对两组数据对比有用，也仅适合 xcms 3 的新对象类型，不过稍微修改就可以推广到其他检验方式了。这个峰列表是进一步研究的基础，如果不严格考察，后面的分析大概率得到错误结论。在检验模型的选择上，如果你的数据存在技术重复或需要线性混合模型求解，这里面的功效分析会比较不同，需要你自己设计算法求解，这里不展开说了，大概是个生统硕士论文的工作量。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(genefilter)
library(qvalue)
library(ssize)
getrealpeaks &amp;lt;- function(xod, power = 0.8, pt = 0.05, qt = 0.05, n = 6){
        data &amp;lt;- featureValues(xod, value = &amp;quot;into&amp;quot;)
        idx &amp;lt;- complete.cases(data)
        data1 &amp;lt;- featureDefinitions(xod)
        mz &amp;lt;- data1$mzmed[idx]
        rt &amp;lt;- data1$rtmed[idx]
        lv &amp;lt;- as.factor(pData(xod)[,2])
        data0 &amp;lt;- data[idx,]
        tr &amp;lt;- rowttests(data0,lv)
        qvalue &amp;lt;- qvalue(tr$p.value)
        pi0 &amp;lt;- qvalue$pi0
        alpha &amp;lt;- pt
        df &amp;lt;- cbind.data.frame(diff = tr$dm,sd = rowSds(data0[,1:n]),p = tr$p.value,qvalue = qvalue$qvalues,mz,rt,data0)
        df &amp;lt;- df[df$p &amp;lt; pt,]
        rs &amp;lt;- vector()
        for (i in c(1:nrow(df))){
                r &amp;lt;- power.t.test.FDR(delta = abs(df[,1][i]), sd = df[,2][i], FDR.level = qt,power = power, pi0 = pi0)
                rs[i] &amp;lt;- r$n
        }
        df &amp;lt;- cbind(n = rs,df)
        df &amp;lt;- df[df$n &amp;lt; n &amp;amp; df$qvalue&amp;lt;qt,]
        return(df)
}
getrealpeaks(xod = xod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] n        diff     sd       p        qvalue   mz       rt      
##  [8] ko15.CDF ko16.CDF ko18.CDF ko19.CDF ko21.CDF ko22.CDF wt15.CDF
## [15] wt16.CDF wt18.CDF wt19.CDF wt21.CDF wt22.CDF
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>差异、相似与模式</title>
      <link>/cn/2017/05/17/pattern/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/17/pattern/</guid>
      <description>&lt;p&gt;最近审了一份主题是质谱数据分析的稿子，领域是分析化学，很明显的感受就是作者但是很明显是为了用新方法而用，没有从问题出发，所以借机讨论总结下科研数据分析的思考视角。 差异&lt;/p&gt;
&lt;p&gt;科研数据分析最基础的出发点就是寻找差异，你观察到了两组数据，这个分组是根据实验设计或人为划分的，你想了解两组数据差异。最朴素的思路就是分组聚合，例如选取出现最多的众数，排序中间位置的中位数以及平均值。但是这个思路只是简单的将一组数描述为一个数，并无法表示这组数的离散程度，也就是丢失了一部分可以进行对比的信息。如果你考虑上表示离散程度的方差，结果就成了对比两个数。这样的对比其实只是描述性的，如果你愿意且具备统计与数学功底，你可以构建出无数用来描述一组数据的单一或多个数值进行比较，这些数值在不同领域可能有不同称呼，可以理解为指标，或者称作统计量。统计量的构建至少满足两个条件：包含想要考察的信息与具备可比较的数学性质。前者比较好理解，后者就需要概率论做基础了，在必要时可根据实际问题修改统计量的数学描述方式。万万不可别人让你用什么就去用什么，这样永远是雾里看花。&lt;/p&gt;
&lt;p&gt;有了统计量，我们就可以进行比较了。不论你使用p值还是贝叶斯推断，其核心思路就是将统计量对应到一个分布里去，然后从概率视角看一下这个差异离不离谱（或者跟先验的概率分布比较），如果离谱就认为差异是显著的，跟0差异不大就认为没有明显的差异。但需要知道的是没有明显的差异是对应你假设检验方法而言的，不是说真实是没差异的，用不同的检验方式，结果可能不同。对于严谨的科学发现，主流的检验方式得到的结论应该是相似的，如果结果有差异，那么要么是检验方式无效或不适用，要么就是你的数据对差异判断并不支持。很多时候，如果技术发展不到位，数据的噪音掩盖信号，此时你不能验证结论，需要上更先进的仪器。从这个角度看，数据采集技术是否先进会制约科学发现，这也是很多学者寻求发展时经常要考虑平台是否先进的根本原因，没有技术平台，科学发现只能说模棱两可。好比你想用放大镜研究细胞结构，基本只能拿着鸡蛋看看了。&lt;/p&gt;
&lt;p&gt;差异多数时候是两两间的，但有时候我们的问题是在某个因素不同水平的影响是否显著，例如我想知道某种污染物的自由态浓度是否受高中低不同土壤含水量的影响，这里土壤含水量是因素，高中低是三个水平，此时用方差分析就可以得到土壤含水量是否影响污染物自由态浓度的判断。推而广之，如果水平是连续变量，那么此时的差异分析实质上是相关分析或者说是线性模型的一个特例。你总是要对模型系数进行假设检验来确定这个系数是否影响了你要考察的变量，如果你要进一步进行讨论，参考下面的模式那一部分。&lt;/p&gt;
&lt;p&gt;发现一个差异并从统计角度说明其出现概率比较低或跟先验知识包含不同的信息量是科研数据分析最常见的应用场景。基本思考流程可以归纳为首先构建代表性可比较的统计量，然后进行假设检验的比较，最后根据结果给出判断。这个判断过程有统计学与概率论做支撑，独立于观察过程，因此判断具备相对客观的属性。&lt;/p&gt;
&lt;p&gt;相似&lt;/p&gt;
&lt;p&gt;另一个科研中常见的数据分析场景是寻找数据的共性，但其实基本思路跟找差异比较接近但面对的数据结构会不太一样。在差异分析中，通常考察的是单一属性；相似性分析中，通常你会得到对同一客体的多个描述角度。举例而言，我得到两组河水样品，想知道两组水样是否接近，此时每组样品如果只测定一个指标，那么对比一下就完了；但如果测了很多组指标如何来衡量？两组河水pH值很接近但COD差距很大，可同时溶解氧几乎相同，如何判断？&lt;/p&gt;
&lt;p&gt;一个朴素的想法就是测量可比指标间的标准化差异，然后求绝对值和或平方和，越大表示相似度越小，或者用类似核函数的思想把高维数据映射到低维空间，还可以进行傅立叶变换来通过低维数值保留核心信息量进行比较。此外一个我非常欣赏的思路就是通过打乱分组构建随机统计量来对比实际发生的统计量出现概率，大概就是Fisher精确检验的套路，这个角度是纯统计思路且在计算不那么贵时很好用。&lt;/p&gt;
&lt;p&gt;相似性分析的科研应用场景会越来越多的，首先是数据库比对，目前组学技术发展很快，相关数据库累积也很快，你发现一个功能蛋白，反推出序列可以直接去搜库做进化树，这里面的相似性分析大都用到的动态编程与数据变换，不然速度跟不上。在质谱上就是谱库检索与比对，此时要考虑同位素分布、质量亏损、源内反应加合物等等，不然也很难比对相似性。另一个应用场景是非监督学习里的聚类分析，这里面相似性统计量是进行聚类的基础。其实虽然科学发现里寻找差异更符合探索逻辑，但实际上当前的数据驱动性研究更多是发现共性，当数据累积越来越多，对于共性的新研究方法或新统计量的提出可能更有价值。&lt;/p&gt;
&lt;p&gt;模式&lt;/p&gt;
&lt;p&gt;差异与共性分析都是最基础直观的科研数据分析思路，学科内规律性的东西有时候并不能直接从差异跟共性分析中得到，这时需要识别数据中的模式规律。所谓模式在科研结果中有两种，一种是探索未知，另一种是拟合已知。前者并不需要特别强的理论基础，用来发现模式；后者需要有比较强的理论支撑。&lt;/p&gt;
&lt;p&gt;拟合已知的最常见，例如线性拟合、多项式拟合等，很多时候由本学科的理论来提供基本形式，例如物理里的牛顿三定律、化学里的能斯特方程、生物里的米氏方程。这些方程的数学形式已经在学科内得到了认可，所以当你获取相关数据时，模式是固定的，你所需要求解的是某个参数。参数的稳定性也可以侧面反应理论的真实性，但拟合严格说更像是验证模式而不是发现模式。同时拟合背后的理论基础及来源也是要深入理解的，例如做吸附等温线有两种基本拟合方式：弗里德里希方程与朗缪尔方程，如果你搞不清机理随便用任何一种都会发现拟合效果都说得过去，但一个是纯理论另一个是经验公式，在使用时得考虑你研究目的。有一类研究比较看重预测效果，此时拟合就可以放宽，例如用多项式拟合考虑个自由度就可以了，甚至有时候可以考虑不同数值范围采用不同数学形式，在边界上用样条平滑下就可以了。但不论拟合理论公式还是经验公式，起码这类分析总还是有个数学公式来做骨架的，探索分析则没有这个限制。&lt;/p&gt;
&lt;p&gt;探索未知模式并不是说让你直接把所有科学问题都抽象成 y = f(x) ，然后你收集一大堆y跟相关x，直接扔到多层人工神经网络里去训练，然后搞个验证集看下效果就用，这是工程学思路，到头来也许解决问题，但你可能根本不理解问题。正常的探索过程基本还是有个模型指导的，你可以从最简单的线性模型开始尝试，然后不断提高模型的复杂度，例如引入交互作用或不同x用不同模型的广义加性模型。当然，你可以引入层级模型来探索数据内部结构。当模型复杂到一定程度，就有点人工神经网络的意思了。如果你是为了发现新模式，那么可视化手段是很好的探索起点。但如果是为了预测，其实样条平滑、小波分析等黑魔法就可以随意发挥了，大前提是你真的理解算法，知道自己在干什么。&lt;/p&gt;
&lt;p&gt;从寻找差异开始，我们很多朴素的想法背后其实都有基本模型在起作用，例如t检验就是方差分析的特例、方差分析就是线性模型的特例，线性模式又可以推广到广义线性模型。从最原始的单一统计量构建到多个独立统计量关系探索，再到考虑交互作用，再到层级结构，模型的复杂度可以不断提升。模型的生成过程也要伴随大量的验证与理论支持，并不是随意可以套用，除非你搞机器学习。其实你应该体会到数据分析与科学发展是紧密联系而不是割裂的，很多数据分析方法就是为解决特定科学问题提出来的，没有想象的那么黑箱，反倒是总把新数据分析方法当成黑箱的思路是很危险的。在绝大多数情况下，科学问题的数据分析方法都是针对性的，你也应该能从分析过程体会到背后的抽象与基本假设，否则并不真正理解，虽然这可能不妨碍你发文章。&lt;/p&gt;
&lt;p&gt;从问题视角出发去构建一个方法要比直接套用学科内常见方法更容易体会到《科学研究的艺术》。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data analysis for Desorption Electrospray Ionization</title>
      <link>/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</link>
      <pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</guid>
      <description>&lt;p&gt;Desorption Electrospray Ionization (DESI) is known for on-site mass spectrum analysis. For example, you could use DESI-MS to get the distribution of certain ions on the surface or cross section of sample. Without chromatograph or related separation process, the mass spectrum is actually the average intensities of all the mass during the sampling time. Recently I am thinking how to process such data via xcms.&lt;/p&gt;
&lt;p&gt;Supposing we have data from 1 min sampling and we want to get the mass spectrum. For mass spectrum, 1 min usually means more than 100 full scan. The basic idea to process such data is directly binning the mass and average the intensity. However, I found &lt;code&gt;group.mzClust&lt;/code&gt; function and &lt;code&gt;MSW&lt;/code&gt; method are designed for this purpose.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(msdata)
mzdatapath &amp;lt;- system.file(&amp;quot;fticr&amp;quot;, package = &amp;quot;msdata&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names = TRUE)

xs &amp;lt;- xcmsSet(method=&amp;quot;MSW&amp;quot;, files=mzdatafiles, scales=c(1,7),
              SNR.method=&amp;#39;data.mean&amp;#39; , winSize.noise=500,
               peakThr=80000,  amp.Th=0.005)
xsg &amp;lt;- group.mzClust(xs)
xsg &amp;lt;- fillPeaks.MSW(xsg)
r &amp;lt;- groupval(xsg,&amp;#39;medret&amp;#39;,&amp;#39;into&amp;#39;)
z &amp;lt;- as.data.frame(peaks(xsg))
file &amp;lt;- cbind(z$mz,r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could save the data as csv file. Then you could perform further analysis such as peak picking or PCA analysis. If your data could be organized for spatial analysis or imaging, all the data has been ready and you could draw them by one for loop or just use &lt;code&gt;animation&lt;/code&gt; package for a gif. I used this trick for my last group meeting as PhD students.&lt;/p&gt;
&lt;p&gt;PS: the new design of xcms 3 is much more friendly to process such data via &lt;code&gt;XCMSnExp&lt;/code&gt; object and the parameters design is very friendly to users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>社会感知、文献解读与棋盘思考</title>
      <link>/cn/2017/05/06/social-sense/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/05/06/social-sense/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会感知&lt;/h2&gt;
&lt;p&gt;最近看到一篇&lt;a href=&#34;http://journals.lww.com/co-allergy/Fulltext/2017/04000/Social_media_use_for_occupational_lung_disease.4.aspx&#34;&gt;综述&lt;/a&gt;，大意介绍了下通过社交网络数据来进行职业性肺病的研究趋势。其实回想一下，从大概四五年前就开始有利用类似数据进行研究的报导了，甚至还有个相关名词：社会感知。&lt;/p&gt;
&lt;p&gt;字面理解，社会感知通过收集社会群体传感数据来研究社会中群体行为或相关科学问题。例如，我收集医院呼吸科就诊数据，然后看看这一观察点是否与其他环境因素或个体遗传因素有关联，譬如有人会&lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153099&#34;&gt;发现&lt;/a&gt;雾霾严重时呼吸科就诊率会上升。但其实研究可以不局限于特定行业，社交网络的快速发展一方面成为很多人日常生活的必需品，另一方面也提供了大量信息输出。例如有人就&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2835988&#34;&gt;研究&lt;/a&gt;过在重度空气污染时人们会在微博上的讨论行为。同样的主题似乎在地理学研究中用到，我曾听过一个报告使用的就是微博签到数据来观察旅游旺季游客的聚集行为。如果不局限在国内，实际各大不存在的社交网站都发表过类似论文，有的是基于地理信息，有的是基于语义分析，还有的则基于传播结构用图论的方法去研究。&lt;/p&gt;
&lt;p&gt;总的来说，社会感知可以看作一个研究手段或视角，其应用面还可以更大。就环境分析而言，污染物调查类论文往往侧重于研究其在环境介质中的迁移转化规律，但逻辑上看，污染物的产生多半是人类行为，也就是说迁移转化规律也可能从社会感知角度去考察。例如我们可以关注一些污染企业的经济指标，其波动很有可能与污染物的环境浓度产生关联，如果是滞后性关联，我们是可以预测一些污染的集中爆发的。同样，人们在社交网络里关注的污染物是存在新闻传播高峰的，也就是脉冲式的，但污染物浓度的变化一般是连续的，这之间的差异或趋势可以用来区别环境污染的生理影响与心理影响。如果环境科研工作者不尽快掌握相关研究手段，那么不出意外做社会学研究或计算机科学研究的人会渗透过来。&lt;/p&gt;
&lt;p&gt;最近掌握相关数据的公司纷纷成立研究院，人员组成学科背景都比较多样，很有可能比学术界优先发现有意思的现象。相比之下，学术界学科间壁垒还比较明显，很有可能因为研究手段的落后而走下坡路，而且业界科研的待遇在国外是普遍高于起步期学界待遇的，而人才的流动一般都会是往前沿去，如果在业界那就会流向业界。现在的研究视角越来越多样化与面向问题出发，单一学科或视角、固定的研究模版与闭塞的交流很容易快速衰落而不自知。例如当前大家都喜欢投学科顶尖期刊，很多老牌期刊却存在门户之见，对新方向把握不充分，新研究无法在原有学科体系内成长就会到其他领域或开辟领域野蛮生长，很多人看不上眼的 plos one 跟 scientific report 等新综合性期刊在慢慢孕育新学科。从引用量上可能不明显，但媒体曝光度上来说，新的研究方向更高。如果所有科研经费总量不变，新学科趋势通过媒体影响力会去逐渐挤占传统学科的资源，届时可能会出现系统性学科衰落。这就好比科学共同体的新陈代谢，缓慢却不可逆。所有学科都曾是新学科，也将会成为传统学科，如果内部不能换血，那就只能等外部输血了。这里血指的就是类似社会感知这样的前沿趋势与视角，这对起步期科研人员尤其重要，因为一旦搞错形势，后面不论你多努力可能都会有一种听天由命的焦虑感。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;文献解读&lt;/h2&gt;
&lt;p&gt;最近几年，科普类网站或公众号的快速崛起促进了科学传播，越来越多的人愿意去把最前沿的研究发现带给有好奇心的听众，特别是目前正在进行科研工作的研究生与老师。但我发现很多人在进行解读时并没有很好的说明白研究类型而更多去讨论发现的事实与意义，这样会形成一些误解。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;没有研究基线的盲信&lt;/h3&gt;
&lt;p&gt;很少有研究是凭空蹦出来的，但对科学报道的受众而言，绝大多数都是头一回听到一些研究领域。也就是说听众都是白纸一张，你说什么他们会首先接纳为事实而不是批判思考，因为多半根本就没有提供批判思考的知识体系。所以文献解读很重要的一点就是要说明我们之前对这个问题了解到什么程度了，有一个基线与背景可以让读者去展开批判思考而不是全盘接受。由于很多论文解读出自领域内专家，而专家通常喜欢用复杂术语树立权威感，所以这个偏误多半只能读者自己揣度，留有余地。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;现象与规律分不清&lt;/h3&gt;
&lt;p&gt;社会科学方面的研究论文解读时经常出现，论文描述的是当前现象，但解读时进行了规律演绎。例如队列研究发现富人们存在早期打高尔夫球的共同喜好，这是一个现象，不能解读成当前你去打高尔夫球，以后就可能变成富人。现象与规律分不清不能用简单的相关不代表因果来解释，更深层次的问题在于，即便很多现象说明的因果关系是事实存在的，时过境迁后，这个因果关系解除了。同样是上面的例子，也许早期打高尔夫球其实是因为能在高尔夫球场获取更好的人脉，但如果现在富人们主要社交方式变成打壁球，那么这个现象只能描述为历史事件，曾经是合理的，但当前完全不合理了。规律是相对稳定不变的而现象是历史性的，一般读者看论文解读的视角是规律视角而不是现象视角，如果解读者自己没搞清楚，那就很容易产生误导。多数科学发现都是有语境的，语境就是适用范围，也可理解为历史条件下的现象。而放之四海而皆准的规律是可遇不可求的，特别社会科学里存在舆论反馈与行为习惯的转变，很多结论在自然态下成立在社会系统中就面目全非了，整个社会到处都是局部最优解与区域护城河，不能当成均相系统研究。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;决策风险&lt;/h3&gt;
&lt;p&gt;要知道很多人的决策，特别在健康、育儿与理财等切身相关的主题决策上是很依赖专家意见的，一旦误导后果都是自负的。烟盒上都标个吸烟有害健康，论文解读上最好也提升下决策风险。不知道从什么时候开始，很多原来自然而然的事都出现了各种专业人士去提供服务并帮助决策，所有人都变成了决策困难者。就育儿而言，当前最新的研究成果与30年前父母自己摸索的育儿经验究竟能造成多大的差异，我想这也是个摸石头过河的事。但解读研究成果的人往往并不提示风险，直接论述方法与成效。从读者角度，这个风险往往会被期刊的权威性所掩饰，但事实上越是影响力大的期刊，撤稿率越高；越是前沿的发现，越有可能是缺少证据的。如果论文解读最终变成了个人经验的论述，那么读者应该放弃其决策价值，不然跟信教没啥区别。&lt;/p&gt;
&lt;p&gt;关于文献解读甚至其他种类的文章，读的时候最好也要有点类似社会感知的想法，想想看这是不是现象唯一的解释与思考视角？解读者有没有夹藏私货？自己有没有意识到其中的偏误与决策风险？&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;棋盘思考&lt;/h2&gt;
&lt;p&gt;上面关于文献解读的分析实际上也只是棋盘思考的一个应用。设想你跟我下棋，如果你下你的，我下我的那就没意思了，一般情况双方都会去思考对方的立场与策略。好比你去考试，如果你身经百战，那么你一定掌握了出题人视角与阅卷人视角，出题人考察你对某个问题的理解而阅卷人关注的是得分点，作为答题者最好就是把答案分条，展示你理解了某个知识点。其实出题人与阅卷人也会意识到答题者的应对策略去制定答案，这样最后真正懂问题的人总会高分通过，而一知半解的人很难蒙混过关。而在互联网上看信息或报道，你至少要意识到一个三方对弈场景。&lt;/p&gt;
&lt;p&gt;所有报道都存在一个事实基础，把事实展示给你的人是第二方，第三方就是你。大多数人并不能意识到事实与展示事实的人之间的互动关系而看成一体，但其实立场差异不同。A采访了B，B所叙述的事实经过A的加工往往会产生微妙的情感共鸣与调动，而受众就是C。C永远只看到A笔下的B，甚至B是否存在C都不会去怀疑。C当然可以自主采访B，但同样C在描述时也是带有某些情感共鸣的。想到这一点，所有报道，包括自己所提供的，对于别人都是相对主观的。也就是说，棋盘思考的核心不在于了解对方想的是什么，而是意识到自己的局限性。&lt;/p&gt;
&lt;p&gt;只有知道自己的局限性，才能对别人的局限性进行合理评估并达成一定程度上的信任或不信任。这始终都是个决策问题，只是日常生活中发生的太过频繁，很多人意识不到其中发生的过程，总是跟着感觉走。但其实跟着感觉走可能也是一种良好的决策，只是在行为经济学大行其道的今天，总有商人盯着你的行为，考虑能否从中挖到一桶金。无论如何，现代社会实在是错综复杂，所有人都在犯错或试错，站在承认局限性的位置思考总不会错的太离谱。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using xcms offline for metabolomics study</title>
      <link>/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</guid>
      <description>&lt;p&gt;XCMS online is preferred for its convenience, especially with Stream. However, the storage is limited and you need to wait for some time to process your data. Actually, almost all of the functions online could be processed offline on local computer. Here I will show you some tips about using xcms package locally in R.&lt;/p&gt;
&lt;div id=&#34;optimized-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimized Parameters&lt;/h2&gt;
&lt;p&gt;Most of the users like xcms online because they have optimized parameters for different instruments and you could directly choose them. Those parameters are related to peaks extraction, grouping, retention time correction and fill missing peaks. Authors of xcms online has published &lt;a href=&#34;http://www.nature.com/nprot/journal/v7/n3/fig_tab/nprot.2011.454_T1.html&#34;&gt;paper&lt;/a&gt; and show the table of suggested parameters. Thus in the local version, you could directly use them. If you still feel hard, I write a function &lt;code&gt;getdata&lt;/code&gt; in the &lt;code&gt;enviGCMS&lt;/code&gt; package. You could install it from Github (CRAN version has not been updated):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;#39;yufree/enviGCMS&amp;#39;)
# we need parallel computing
library(enviGCMS)
library(BiocParallel)
library(xcms)
# you need faahKO package for demo
cdfpath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;, package = &amp;quot;faahKO&amp;quot;)
# directly input path and you could get xcmsSet object
xset &amp;lt;- getdata(cdfpath, pmethod = &amp;#39;hplcqtof&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;getdata&lt;/code&gt; could directly perform peaks extraction, grouping, retention time correction and fill missing peaks and return the &lt;code&gt;xcmsSet&lt;/code&gt; object for further analysis.&lt;/p&gt;
&lt;p&gt;However, I suggest use &lt;code&gt;IPO&lt;/code&gt; package to optimize the parameters for certain instrumental. Here is the R script for optimizing. You need to be patient because such process usually take half day. After finding the parameters for your instrumental, you could use those parameters for the following studies. Here is the R script to optimize parameters for certain instrumental:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# path and files
# use pool qc or blank for this optimization
mzdatapath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;,package = &amp;quot;faahKO&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names=TRUE)
library(IPO)
# use centwave if you use obitrap
peakpickingParameters &amp;lt;- getDefaultXcmsSetStartingParams(&amp;#39;matchedFilter&amp;#39;)
#setting levels for min_peakwidth to 10 and 20 (hence 15 is the center point)
peakpickingParameters$min_peakwidth &amp;lt;- c(10,20) 
peakpickingParameters$max_peakwidth &amp;lt;- c(26,42)
#setting only one value for ppm therefore this parameter is not optimized
peakpickingParameters$ppm &amp;lt;- 20 
resultPeakpicking &amp;lt;- 
  optimizeXcmsSet(files = mzdatafiles[6:9], 
                  params = peakpickingParameters, 
                  nSlaves = 4, 
                  subdir = &amp;#39;rsmDirectory&amp;#39;)

optimizedXcmsSetObject &amp;lt;- resultPeakpicking$best_settings$xset

retcorGroupParameters &amp;lt;- getDefaultRetGroupStartingParams()
retcorGroupParameters$profStep &amp;lt;- 1
resultRetcorGroup &amp;lt;-
  optimizeRetGroup(xset = optimizedXcmsSetObject, 
                   params = retcorGroupParameters, 
                   nSlaves = 4, 
                   subdir = &amp;quot;rsmDirectory&amp;quot;)


writeRScript(resultPeakpicking$best_settings$parameters, 
             resultRetcorGroup$best_settings, 
             nSlaves=12)
# https://github.com/rietho/IPO/blob/master/vignettes/IPO.Rmd&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;Actually, the statistival methods in xcms online are limited compared with Metaboanalyst. In last post, I have shown how to install Metaboanalyst locally. Here, I also supply a function in &lt;code&gt;enviGCMS&lt;/code&gt; to directly get the csv file to be uploaded to Metaboanalyst. You need to show a xcmsSet object and the name for the file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this xcmsSet object could be directly get from getdata function
getupload(xset,name = &amp;#39;peaklist&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eic-and-boxplot-for-peaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EIC and Boxplot for peaks&lt;/h2&gt;
&lt;p&gt;If you like the report from xcms online, you could also get them with the figures. I also write a function called &lt;code&gt;plote&lt;/code&gt; in &lt;code&gt;enviGCMS&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# you also need the name for subdir of EIC and Boxplot, you might also change the test method for the diffreport
plote(xset,name = &amp;#39;test&amp;#39;,test = &amp;#39;t&amp;#39;, nonpara = &amp;#39;y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the function has been documented. I might update the CRAN version in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;waters-q-tof-mass-lock-issue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Waters Q-ToF mass lock issue&lt;/h2&gt;
&lt;p&gt;If you use Waters Q-ToF, you might be confused by data conversion. I suggest you use the most updated msconvert to convert RAW folder into mzxml, which you could input the lock mass(older version miss this function). However, such data still have gap, you might use the &lt;code&gt;lockMassFreq = T&lt;/code&gt; in xcms to imput such gap to get more peaks. Such parameters could be transfer in &lt;code&gt;getdata&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xset &amp;lt;- getdata(path,lockMassFreq = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation&lt;/h2&gt;
&lt;p&gt;For the annotation part, I suggest using &lt;code&gt;xMSannotator&lt;/code&gt; package. You could install it from my github repo since the author didn’t use github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# You might need to install the following packages before installing this package
install.packages(&amp;#39;data.table&amp;#39;)
install.packages(&amp;#39;digest&amp;#39;)
source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite(&amp;quot;SSOAP&amp;quot;)
biocLite(&amp;quot;KEGGREST&amp;quot;)
biocLite(&amp;quot;pcaMethods&amp;quot;)
biocLite(&amp;quot;Rdisop&amp;quot;)
biocLite(&amp;quot;GO.db&amp;quot;)
biocLite(&amp;quot;matrixStats&amp;quot;)
biocLite(&amp;#39;WGCNA&amp;#39;)
devtools::install_github(&amp;quot;yufree/xMSannotator&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other functions&lt;/h2&gt;
&lt;p&gt;I have writed some other functions in &lt;code&gt;enviGCMS&lt;/code&gt; package and you could explore them. You might find some Easter Eggs. Also I will documented them as vignette in the future.&lt;/p&gt;
&lt;p&gt;This post and the post before is about finding the peaks and performing statistical analysis for metabolomics. In the next post, I will show you some tips about annotation based on &lt;code&gt;xMSannotator&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;If you have other issues about metabolomics data analysis, you could comment here and I’d like to discuss them. Also you could sent email to &lt;a href=&#34;mailto:slack@yufree.cn&#34;&gt;slack@yufree.cn&lt;/a&gt; to get invitation for a slack group about metabolomics data analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>环境化学的组队原则</title>
      <link>/cn/2017/04/21/es-team/</link>
      <pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/04/21/es-team/</guid>
      <description>&lt;p&gt;最近同组两个博后都拿到了教职，聊天时就说到了自己建课题组所需要的人员组成。因为当前课题组的研究方向是分析化学，所以一般至少会需要仪器仿真方向、材料合成方向、数据处理方向与特定应用方向的人才。如果是课题组长，这四方面至少要会两到三项，不然很多项目根本做不起来。回过头来想想，环境化学其实也可以考虑下组队问题。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;平台&lt;/h2&gt;
&lt;p&gt;环境化学目标污染物常规一点的大概就是无机方面的重金属与有机方面的农残、食品添加剂、POPs名单上那一坨、PPCPs还有一些生物毒素。往前沿看基本都是非目的检测或筛选，简单一点的是已知污染物的代谢产物与结构类似物，复杂一点就是效应诱导分析或通过QSPR预测毒性这类。这些主题比较热门，如果是环境调查方向，可以各种污染物排列组合去发，毕竟考虑三间分布的话，时间地点人群不同，迁移转化规律也不一样。同时还有非生物介质、生物介质等等污染物载体的差异，研究视角不同，看到的东西也不一样。&lt;/p&gt;
&lt;p&gt;从分析技术上看，首先得有前处理设备。常规的超声、ASE、SPE、旋蒸、氮吹、离心机等得有，如果是作方法学的，什么SPME、整体柱、分子印迹、微流控、离子液体、纳米颗粒负载修饰等技术得有会的人，特别纳米材料那一块，考虑上磁性容易做出体系，当然实用性另当别论。这些设备与技术能让你萃取净化到目标物，当然你做无机搞微波消解然后赶酸啥的当我没说，你们那不叫前处理。&lt;/p&gt;
&lt;p&gt;处理完了的东西就要检测，环境分析检出限都是ppb级及以下，有机的就别来掺和了。无机检测ICP-MS是一站式解决方案，你要有钱搞得起MC-ICP-MS还可以做无机同位素分析，前提是有钱。有机一般就是质谱了，获取你比较关心核磁红外紫外，但那个出的结果大概率被审稿人鄙视。不过核磁另当别论，关键环境污染物含量太低，一般满足不了核磁要求，如果你能找到做合成的人帮你鉴定未知物那最好不过。质谱里面有条件上obitrap就上，再不济也得有个tof，如果你有FT，那基本可以抛弃前面的色谱了。定量的话qqq跟单杆最好都有，离子阱如果不做便携质谱或是普渡厨子的门生就不要考虑了。电离源也要配齐，气谱EI跟CI，液谱ESI跟APCI，冷门点的双喷雾或DESI也可以搞搞，不然有些污染物你可能根本就测不到。光谱也是一个思路，不过你得往高通量或可视检测去做，发不出文章也可以出点专利产品，这部分一般思路就是做抗体或者化学发光、荧光啥的，主要就是原子分子光谱，不过跟质谱比最大的优势可能就是便宜了。此外，表面共振拉曼光谱也是个快速发文章的渠道，配合一些稀奇古怪的修饰，总能在现场分析上找到突破口。光谱其实也有个大杀器，光源，前提你约的上，高能射线下能对很多过程进行细致分析与成像。此外，元素分析、各类显微镜电镜还有生物方向的特异性分析、测序跟芯片技术在环境领域也有应用，就看你想回答的科学问题是什么了。&lt;/p&gt;
&lt;p&gt;上面扯半天主要就是说得有个仪器平台可以依赖，而且多半你也不会买维修合同，这说明初期得有点当金工、电工甚至木工的手艺，没有也没关系，好的平台一般都标配。我现在组里那几台质谱基本都是老板各种谈判低价搞过来的，不是二手demo机就是根本已经坏了，故障五花八门全是自己修，搞得这边博士生毕业都可以去培训仪器公司工程师了，反正多数上门工程师的顶级秘籍就是换件。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;人才&lt;/h2&gt;
&lt;p&gt;仪器只会报数，只有人才能把数据变成论文。在这一点上，一个课题组至少要有一个人有数理统计与编程技术背景，甚至每个组员都要有应用层次的编程技术。这可不是说看到别人用了pca，你也用pca去照猫画虎，你至少要知道这些技术为什么用，什么时候用，怎么用。只有这样写论文时才知道你在干什么而不是只知道套模版怎么做。软件工具python、r或matlab都可以。此外，也要有一个工科背景的员工或博后，懂仿真。统计模型与仿真模型完全是两个概念，一个面对数据归纳，一个面对实物或系统用规律演绎；一个用回归，一个各种偏微分方程；一个做假设检验，一个直接在虚拟空间进行实验。小到模拟一个吸附解吸过程，大到生态系统的物质能量循环，仿真模型与统计模型是要相辅相成的。软件工具comsol、matlab甚至netlogo都可以。这两类人是基础人才，能把数据变成故事。&lt;/p&gt;
&lt;p&gt;但作为独立课题组只有基础人才是不够的，课题组长一般都有一技之长。环境化学最容易耦合的学科是毒理学、污染控制与环境暴露。毒理学上的人才要对污染物致毒的分子机制玩的转，懂得基因修饰去验证被污染物影响的生物学过程，当然偷懒一点搞点QSPR研究下分子蛋白相互作用机制也是可以的，不过你得真的搞得明白从分子动力学到DFT那一系列的坑，你搞不明白没关系，別坑学生。当然，现代毒理学里急性慢性毒性实验、行为学表征还有流式细胞术什么的基本每篇文章都是标配，不是你的长处也别成为短处。更深入的组学技术如果不是基础够强，花钱找公司做，但数据分析要自己来，很多公司的数据处理技术还停在上个世纪。如果你是污染控制方向，污染物的降解热力学动力学自然要了解吧，拿个mopac算算前线轨道能也可以编出个不错的故事。如果你跑去做工艺养污泥颗粒，那目前似乎没点基因芯片也说不清楚降解机理了。如果你是大气环境化学方向，额，你似乎兼顾有点催化背景好一点。如果是土壤，怎么说也淋过土柱玩过同位素标记吧。如果你搞了个环境暴露方向，最好认识医院里那些缺论文升职称的大夫，随便搞点病人与健康人的样品测下污染物浓度就可能有不错的发现。如果你是公卫那边转过来的，搞点暴露组学配合流行病学研究会有不错的故事。如果你气象那边过来的，搞点气候变化对污染物迁移转化规律的影响也会不错。总之课题组长要会找到研究切入点与生态位，跟风怎么也要等你手头有资源再说，早期做出特色更重要。&lt;/p&gt;
&lt;p&gt;说了这么多你会发现现在要想做出点名堂多半是要靠合作的，很多技能不可能同时出现在一个人身上，多数课题组也一般只会关注到一个方向，但如果科学问题的解决需要多角度切入说明，那就拿出诚意去找合作者。环境科学本身就是面向问题出发的，而阐述问题的证据角度越多，结论越靠谱。同时，要不断从基础学科的理论与技术进展中汲取营养，一个学科里聪明人多，那么新思想出的就多，这些思想不仅可以解决这个学科里的问题，也可以解决其他学科的问题。在这一点上，物理、化学与生物的技术进展与计算机科学、统计还有数学的理论思想都值得关注。在那些聪明人比较多的学科里的人也可以搞个跨界，大家共同发展。这里面最明显的就是计算机科学对生物信息学与金融的带动，往往一个学科的进步暗含了其他学科进步的契机。如果你打算做点学术事业，到聪明人最多的学科里去学习，然后回自己的学科里发展。这里面最大的问题就是，一般聪明人多的学科工资要比不那么多的学科高，这时候就得把学术理想当鸡血用了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;合作&lt;/h2&gt;
&lt;p&gt;有两种做事态度，一种是做所有事都是从自己需求的出发，追求个人的功成名就，科研成果是垫脚石；另一种是从解决问题的需求出发，解决问题是唯一目标，个人名利是副产品。据我观察，两种态度都不影响个人成长与学科进步或退步，但后一种生活态度的人想的少一点，幸福指数要高一点。我个人喜欢面向问题解决问题，当然这个态度会让你不断得罪人你自己还意识不到，不过整体大脑负担小，睡得香。如果面向问题，一作可以不强求、首先发现也可以不要但问题要说明白与说清楚，把好的思想传播出去而不是跟宝贝一样藏着掖着，太阳底下没有新鲜事。这个态度另一个好处就是容易促成合作，如果合作可以成功就比各干各的的社会效益或学科贡献更多。&lt;/p&gt;
&lt;p&gt;很多人都知道囚徒困境里一个比较好的策略就是以牙还牙，但其实这个策略只是可以保证整体收益为零，如果想为正，最好的方法就是双方不断合作。以往的思路都是构建在把自己的课题组搞成巨无霸，目前国内主流就是如此，这样内部合作强于外部合作，容易稳定学术界地位，从这个角度看巨无霸课题组是可取的，我读博的中科院主流基本如此，配合良好的顶层设计可以攻坚克难。但目前来看，如果是一个新课题组长，这样搞一是根本就没那么多资源，二是新人初期很难招到理想的同事。所以此时的优势策略不应该依附大课题组，而应该是跟同龄人合作，在平等的基础上共同成长，面向问题解决问题。不要担心自己的技术被别人学走，别人学走正是说明这个技术有价值，没人学才是大问题。另外，早期技术跟别人合作推广了，你还可以开发新技术嘛，我从未见过有课题组靠一项保密技术可以维持30年的，而对于正常科研人员，30年差不多也该给后辈挪窝了。&lt;/p&gt;
&lt;p&gt;合作是没必要在办公室发生的，很多问题或思路有时候就是差一句话。印象中有公司就通过设计让员工的午餐排队时间控制在3～4分钟左右好让排队时员工能多交流，时间长了员工就出去吃，少了讨论不充分。合作甚至不用物理接触，现在实时通讯工具一大把，随时都可以交流。不过，搞成微信群那样就太花哨，也容易被朋友圈跟红包分散精力，用邮件列表功能又太单一，目前我组内用slack。一方面可以实时交流，另一方面通过区分频道可以针对主题或项目进行讨论协作，此外也可以作为个人项目管理与点对点交流工具。&lt;/p&gt;
&lt;p&gt;我猜的不错的话多数人看到这里也就停了，因为周围可能没有人用，不过万事都有个开头，我已经建立了一个slack主页：yufree.slack.com，欢迎大家给我发邮件slack at yufree.cn拿邀请尝试，不建议留邮箱，原因你懂的。可以当作一个即时交流组队平台的实验品，相互学习，共同进步。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Virtual、世界三与传参</title>
      <link>/cn/2017/04/13/virtual/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/04/13/virtual/</guid>
      <description>&lt;div id=&#34;virtual&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Virtual&lt;/h2&gt;
&lt;p&gt;这个单词究竟怎么翻译曾经困惑我很久，因为实在搞不清究竟它指的是虚拟的还是事实上的，其实两个意思都有。这个词看似精神分裂，但却可以很好的描述当前人的状态，也就是说现实中的虚实界限逐渐在模糊，互联网在这个过程中功不可没。&lt;/p&gt;
&lt;p&gt;首先，大多数文娱活动都是在构建一个Virtual空间。玩一部游戏、读一部小说、看一部电影、听一段音乐甚至欣赏一幅画。你都是在闯入一个异世界，这个世界逻辑矛盾、规矩奇特甚至根本就不可能在现实中遇到，但沉浸其中后，你可能并不会感到什么违和感而更多的是亲切感。在这些空间中可以感到强烈的情感、感同身受的震撼与对背后形而上探求的渴望。但这些并不是喜欢这些活动的真正原因，喜欢Virtual空间其实是因为这个空间比真实空间更舒适。换言之，真实空间的充满了不确定性，而且似乎无始无终，没有太多剧情，没有人去设计也毫不讲道理。&lt;/p&gt;
&lt;p&gt;你可能说，不是啊，世界不是这样啊，你三观有问题太悲观了吧。可以这么说，但抛掉乐观悲观的情感，这个世界里未知的远远多于已知的。如果你突然感染了未知病毒，身体一步步虚弱下去却毫无治疗方法，此时求生的信念毫无事实可以依托该如何面对？&lt;/p&gt;
&lt;p&gt;意识，特别是自我意识，更多是一种进化上副产品。但这个副产品却衍生出了一大堆符号与交流运算体系，甚至构建了文明社会，但回头看看生命体本身，其实就是个碳氢氧等元素的有序组成，且这个有序都有可能是虚幻的，因为自然界不定义有序无序。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;世界三&lt;/h2&gt;
&lt;p&gt;卡尔·波普儿最为人所知的是证伪的思想，但在我看来，他在后期提出的三个世界更合适作为科学研究的指导理论。在这个认识论理论里，世界一指的是物理客体的时空世界，这个世界不受干扰的存在；世界二则是人们的感官或意识世界，绝大多数人通过意识来感知到这个世界，特别是世界一的存在；而世界三则可以看作一个客观的知识世界，这个世界可被感知但不受世界二的干扰，也就是一个共同意识世界。所谓科学研究，大多数时候是站在世界一里运用世界二提炼追求世界三的一种活动，也就是客体-主体-知识体三级跳。而世界三里的科学知识是可证伪的，也就是会存在证据能说明这个知识是错的，如果不可证伪，那么也属于知识，但不是科学研究的领域。这样一来，逻辑跟数学是不算科学，其公理体系是逻辑自洽且无法证伪，但很特殊的是逻辑跟数学却在绝大多数情况下提供了科学研究的工具。另外的一些理论，例如历史唯物主义、精神分析法、行为分析法等虽然逻辑自洽，但无法证伪，也就不在科学讨论的范畴里。这样来看，科学研究是有界限的，界限内可讨论，界限外无法讨论。例如宗教语言等学科，你可以用科学方法去研究，但涉及到形而上的东西谁也说服不了谁的，最经典的解释就是显微镜小妖，据说刚发明显微镜时，宗教人士并不认可细胞结构，于是他们说所有显微镜镜片都有魔鬼附体，看到的是魔鬼的幻象。这个理论从根源上拒斥了讨论的可能，毕竟有个无所不在的魔鬼理论，什么都可以往上推。&lt;/p&gt;
&lt;p&gt;现代社会的一大贡献就是创造了大量的世界三来消耗人类因为劳动力进步而富余的精力，大量的空闲时间是需要多样的娱乐形式来满足的，而娱乐形式不外乎两种：一种让你体验现实存在但不经常发生的事；另一种则直接就是创造一个虚拟空间。值得注意的是，设计这样的虚拟空间也许用不到客体知识世界的严谨性，只需要附加或重构已有规则或规律就可以了，甚至只是单纯描述同样生存规则下另一个人的经历都可以。但我预料，人们会越来越对现实世界感到无聊而滑入新规则世界，特别是在有互联网的今天。越来越多的娱乐作品开始宣传全新的世界观与代入感，好比构建一个新的世界三，而这个世界要比现实世界更刺激但又更安全，这样停留在这里十分舒适。这就好比一个局部最舒适区，也许继续努力会达到一个更舒适的地方，但很多人已然满足。但科学家是不能满足的，因为他们直面的是现实世界里那个规则并不清晰的知识世界并期望自己的努力能让我们对那里的了解更多一点。同样是玩游戏，现实世界的游戏的挑战性其实更多，只是伴随知识积累，容易的基本都被发现了，剩下的都是难题。或者，你可以找出一个新领域。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;传参&lt;/h2&gt;
&lt;p&gt;人类的娱乐活动是会反过来影响现实生活的，或者说，这两者从来就没有很好的区分过，分不清楚是问题，分得太清楚也是问题。人总是要在多个系统间互相切换的，保持一致性就相当于参数传递，分场景角色扮演就相当于参数独立。很多人精于角色扮演而八面玲珑，但如果没有场景传参，个体就更像是一个完全失去人格的社会动物，只能沉浸在集体里感知自己。但如果传参过多，会让人觉得太过自我，与社会脱节。这个现象出现在社会生活里，也体现在很多细节里，或者说传输规范。&lt;/p&gt;
&lt;p&gt;你打算搜索一个关键词，这个关键词通过拼音或五笔编码输入到屏幕上的搜索栏，确认无误点击搜索，此时数据被统一的规范封包，转换成光信号或电信号进行传输，服务器收到这个信号，用同样的规范解包，然后转化为搜索指令，通过数据库检索返回结果，然后服务器再通过网络传输协议传给你的电脑，经过系统软件与浏览器的翻译，最终你能看到你想要的结果。也就是你动动手指，但为了实现这个目的，其实有无数的传递协议与接口来保驾护航。&lt;/p&gt;
&lt;p&gt;科研过程也是如此，最初你有个想法或发现一个现象，你想告诉其他同行，如果认可就算是做的不错。但事实上交流过程没那么简单，你得确认想法的真实性，这样就需要做实验或调查，通过统计决策或逻辑推理来阐述现象并讨论机理，然后整理成论文，投稿，接受，会议做展示，课堂教授等等。每一步其实都有自己的方法论，你需要保证信息传递过程不被噪音干扰，这并不简单。如果最初目的发生改变，那么这个交流就是无效交流。&lt;/p&gt;
&lt;p&gt;为虚拟现实世界之间的通讯设计构建端口有可能是未来最重要的事，我能想到的问题都出在这个环节。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>搬家</title>
      <link>/cn/2017/04/08/movingagain/</link>
      <pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/04/08/movingagain/</guid>
      <description>&lt;p&gt;经过长期跟拖延症的斗争与谢大的&lt;a href=&#34;https://yihui.name/cn/2017/04/url-to-content/&#34;&gt;指引&lt;/a&gt;，我终于在这周把个人网站进行了blogdown一站式处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了域名上的小绿锁，我把空间托管所从github pages搬家到了Netlify&lt;/li&gt;
&lt;li&gt;使用Migadu注册了 &lt;a href=&#34;mailto:xx@yufree.cn&#34;&gt;xx@yufree.cn&lt;/a&gt; 域名邮箱，xx当然是42&lt;/li&gt;
&lt;li&gt;DNS解析从万网搬家到了Cloudflare，主要为了域名短一些跟邮箱加密&lt;/li&gt;
&lt;li&gt;中文与英文网站进行了合并处理，中文网站增加笔记入口，英文网站增加代谢组学流程入口，用blogdown管理&lt;/li&gt;
&lt;li&gt;主题自然又是无耻的抄了谢大了，不过我对主题配色与一些细节进行了修改，放在&lt;a href=&#34;https://github.com/yufree/hugo-lithium-theme&#34;&gt;这里&lt;/a&gt;，除非万不得已，应该懒得修改了&lt;/li&gt;
&lt;li&gt;最大的问题就是域名，原来使用了blog跟blogcn，这次用了cn跟en，也就是说你用原来的访问模式看到的gitbub的页面应该不会更新了，rss也会停了&lt;/li&gt;
&lt;li&gt;如果你访问yufree.cn，你会被引导到新网站，但搜索引擎记录的更多的是原来网站的地址，简单说就是你还是可能看到github托管的网页，只是不再更新了&lt;/li&gt;
&lt;li&gt;以后中文博客地址就是https://yufree.cn/cn ，英文则是https://yufree.cn/en&lt;/li&gt;
&lt;li&gt;rss的地址更新如下： - 中文：&lt;a href=&#34;https://yufree.cn/cn/index.xml&#34; class=&#34;uri&#34;&gt;https://yufree.cn/cn/index.xml&lt;/a&gt; - 英文：&lt;a href=&#34;https://yufree.cn/en/index.xml&#34; class=&#34;uri&#34;&gt;https://yufree.cn/en/index.xml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;由此带来的不便深表歉意，但应该不会再搬家了，除非上面的某个服务商倒了。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>社会动物、利率与三间分布</title>
      <link>/cn/2017/04/08/rates/</link>
      <pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/04/08/rates/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会动物&lt;/h2&gt;
&lt;p&gt;这周读完了戴维·布鲁克斯的《社会动物》，这本书很特殊，一方面你可以把它当非虚构类来读，因为里面综合了大量社会学、经济学、心理学甚至脑科学的研究成果，但另一方面却又可以看作一个虚构类的小说，因为所有研究成果都贯穿于一对夫妻的成长过程，从其父母的相识相爱、童年、青少年、大学、创业、职场成长、中年危机、事业巅峰贯穿到退休后的生活与男主角的死亡，作者将全过程娓娓道来，娴熟地把研究成果、社会现象与生活经验体会捏合到一个时间尺度之中，阅读体验非常好。同类的书还有《苏菲的世界》，不过那个是将哲学的，如果你对经济学感兴趣，《狼与香辛料》这部轻小说也是类似的结构，读这类书不必执着于对错与严谨性，沉浸于故事中就好。&lt;/p&gt;
&lt;p&gt;但读这本书我最大的体验就是作者观点其实跟另一本去年读过的《贪婪的大脑》是有一定冲突的。《社会动物》的作者戴维·布鲁克斯是芝加哥大学历史学专业毕业的，目前是一位专栏作家兼政治评论员，其背景类似男主角哈罗德。《贪婪的大脑》的作者丹尼尔·博尔则是一位神经科学家，论述风格带有明显的理工类论文的节奏。这种迥然不同的背景似乎也影响了其文章思想的体现。《社会动物》中，女主角埃丽卡创立了一家以引入各种行为经济学研究成果来帮助企业发展的咨询公司，但是那些拎着核磁共振仪的神经科学家却更受企业欢迎，这导致创业初期并不顺利。这影射了目前两种咨询策略，一种是诉诸非理性或潜意识的研究而另一种则是理性量化的实验性科学。严格说这并不对立，但在《贪婪的大脑》中，丹尼尔·博尔认为很多关于无意识起作用的研究实质上并没有回答原始问题而仅仅是忽略或混淆了某些过程中意识与自我意识的区别与作用方式，因而埃丽卡的成果或许有用，但并不是一个正确的解释。然而戴维·布鲁克斯却认为过分依赖理性的力量也存在严重的缺陷，那就是在这类研究中过分依赖有良好效果的理性主义与模式的识别，人的社会性等难以量化分析的成分都被系统性地忽略了，而社会属性才是解决当前社会问题的关键。具体来说就是为什么社会学研究精彩纷呈，但落实到政策上总是答非所问，似乎理性的策略设计总是在社会层面不成功，例如美国教育改革与税收政策并没有实现原有的目标，反而不断激化了社会矛盾与割裂。&lt;/p&gt;
&lt;p&gt;这是一个有意思的对比，《社会动物》中，戴维·布鲁克斯对社会各阶层的运转、教育系统及政治运作有着近乎调侃的洞见，例如描述社会中上层人生活方式时会说他们“选购耐用品时无比奢侈，而购买消耗品时则能省则省”、“政治演说的结构非常简单：前12分钟讲你，后12分钟讲我”、“管理国家的才能不可避免地等同于掌控心灵的才能”……这本书开头就定了一个基调，那就是“痛苦是通往智慧的必经之路”，文中的调侃与故事发展也印证了这一点。反观由科学家写作的《贪婪的大脑》，虽然同样涉及了意识、潜意识与冥想等主题，但确实少了很多人性化的东西，虽然结论类似。然而，在《贪婪的大脑》中，结论的描述经常是符合科学原则的去限定范围与保留疑问空间，而《社会动物》中你就会发现很多论断式的对研究结果的引用，掷地有声但拒绝了进一步地讨论，作为观点的陈述无可厚非，但过于绝对。例如对于是否存在自由意志的问题，《贪婪的大脑》认为如果我们接受人脑是机器的或者说一元论的，那么自由意志确实不该认为是存在的，然而大脑的复杂性却保留了产生自由意志的可能，而且作者讨论了关于自由意志问题上存在的定义问题。在《社会动物》里，却常常出现“行为的改变往往先于态度和感情的改变”等共识性论断而缺乏得出结论的细节与背后争论的讨论，这可以看作文科背景人的短板，如果其更深刻理解科研中的不确定性，可能会更真实描述问题。但这却是一种常见的写作手法，可以理解为liberate art中核心技艺，我疑心调和这种科学理性与感性表达的尝试是徒劳的。科学家总应该读点哲学与人文的东西来理解社会运转，而社会学者则急需科学哲学与科研思维的训练来搞清楚探求真相的过程，不要过度延伸。&lt;/p&gt;
&lt;p&gt;不过，确实应该重视《社会动物》中提到的问题，特别是情感共鸣、群体孤独与社会信任感的缺失等问题。戴维·布鲁克斯给出的药方是教育与有限权利政府，着力于修复认知负荷加剧的现代社会中社会流动性问题，解决多元化社会中的伦理困境。虽然作者指向的是美国社会的问题，但其实是个全球面对的难题：如何让所有人都切实感受到幸福而不是割裂与分离。也许戴维·布鲁克斯的另一本书《品格之路》从历史人物的成长过程角度探讨了这个问题并给出了个人层次的解决方案，不过我没读过，不好评论。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;利率&lt;/h2&gt;
&lt;p&gt;《社会动物》中，虚构的男女主人公虽然经历了一生，但其实时间是相对静止的。所有阶段都像是从当前社会抽离出一个年龄段的人来进行分析，这样做丰富了当前，但历史没有被考虑。人本质上是历史的，在时间尺度下微小的变化会成长为巨大的差异。&lt;/p&gt;
&lt;p&gt;如果你跟我一样是80年代末90年代初婴儿潮一代，到了今天应该是忙碌而焦虑的状态，说白了现在社会对年轻人的希望是同时负担起养老负担与子女教育，不忙碌焦虑才怪。财富累积与生活方式的改进可能是大多数人无法摆脱的负担，工资也许在涨，但花销也在涨。据我观察，这个年龄段的人大都陷入了信用卡、保险与常旅客积分里程计划的研究之中。背后的逻辑很简单，通货膨胀。&lt;/p&gt;
&lt;p&gt;如果今天有个产品年化增长率5%，那么10年复合增长率就是&lt;span class=&#34;math inline&#34;&gt;\(1.05^{10} - 1 = 0.63\)&lt;/span&gt;，也就是说，100块放10年就变成了163块。你看到的是变多了，其实这里面忽略了货币购买力问题，如果现在100元可以买一袋大米，10年后一袋大米成了163块，那你其实只实现了保值。如果10年后一袋大米变成200块，那其实这个年化增长率5%的产品的最大意义就是如果你现在有100元没有需求去消费，那么买这个产品可以一定程度让其购买力减退没那么快。&lt;/p&gt;
&lt;p&gt;同时你可能也反应过来了，如果现在就有需求，那么应该优先满足现在的需求，因为跑赢通胀是在你有闲钱的情况下该考虑的，如果你没有闲钱，优先考虑消费，不然你以后要付出更多来实现同等程度的需求满足。所有向你兜售理财产品的人基本都不提通胀的影响而是给你展示绝对量变化，这在我看来就是欺诈，忽略基线的欺诈。&lt;/p&gt;
&lt;p&gt;这个时间尺度的基线可能是现代社会运作的一个核心，作为个体，你体会的更多是静态，而社会总是在时间轴上一去不返。这里我们做一个基本假设，社会总需求主要由人口决定，如果社会总财富的累积速度低于人口增长，那么每个人的生活条件应该是下行的；如果高于人口增长，那么总体收益。目前我们面对的是后者，一个可预期的后果是，社会结构不变，那么社会累积的财富有两个去向：一个是通过通货膨胀来维持需求价值，另一个则是通过财富集中来维持货币与需求的对应关系。现在来看，两者都在起作用，特别是后者。不过由于前者的存在，整体生活质量是改善的，但后者则可能造成底层人的生活质量下降。同时财富的集中按照《21世纪资本论》的观点应该是向上集中、向富有国家集中。但具体到个人我要加上一个显然但容易忽视的结果：向年长者集中。&lt;/p&gt;
&lt;p&gt;在《社会动物》中，作者提到了一个现象：18-34岁美国人平均每年会收到来自父母38000美元的资助。这个现象就是啃老，它已经使各个发达国家潜在的社会生活结构变化了，我所在的系有个技术人员，女儿20多岁但就是不肯出去找工作，还跟女朋友一起生活在家中，这个技术人员自己是典型西方人，很早就脱离原有家庭，但如今却面临自己孩子无法脱离自己家庭的状况。同样，国内到这边读书的留学生也基本是来了就买房置产，然后全家移民，用的自然是家里的资助。不论东西方，年轻人都面临了巨大的压力，如果没有家庭结构的支持，很难独立。但同时要注意到是，家庭结构也在现代社会运作下在不断解体为个体，大城市的单身中青年比例也在升高，这些结果混合到一起让基于现有模式的预测变得困难。我们的父母在置产上更多是自己累积财富；而到了我们这一代，来自家庭的启动资金对于维持生活逐渐变得必不可少；下一代的教育成长成本的增长则一定程度降低了人口出生率。发达国家财富向年长者的集中或者说代际间的财富转移所造成的社会现象是很值得关注的。这是时间与利率的魔力，也是很多问题的根源，例如如果人工智能会取代一些劳动岗位，那么已经在岗的人数可能出于工会压力不会变化太多，但有可能取消掉计划中年轻人的新岗位，这看起来会更平稳，但代际不平等会不断加剧，这个会对社会产生什么影响，很难预期，因为似乎是个阶段性问题。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;三间分布&lt;/h2&gt;
&lt;p&gt;其实不只是时间，加上空间与人群间的分布统称三间分布，是环境科学论文解读的一个简化视角。也是现在看待社会问题的一个基本视角，例如当新生事物出现时，系统的研究会同时考虑其地理分布、时间尺度分布与波及人群的画像。具体到污染物研究，空间分布特征、时间分布规律与流行病调查是环境过程研究阅读时必须关注的。具体到材料研究，元素表征与电镜就是空间尺度研究，动力学例如吸附过程、团聚过程就是时间尺度，各领域的应用就算是人群间研究。具体到组学研究，空间分布是让你找出组间差异并注释，时间分布是让你找出组内时间序列差异并注释，人群分布可以理解成找个切入角度把注释讲成故事。具体到经济学就是横向与纵向数据分析，不同学科间似乎潜在一个统一的研究框架，当然，我现在并无法清晰描述，只是感觉。&lt;/p&gt;
&lt;p&gt;科研结果的解读是有章可循的，根源上要回溯到物理里关于热力学与动力学的区分，跟人扯上关系就是应用。这类解读本质上还是基于实验或观察数据发现规律，但其实还有一种思路也在某些不容易做实验的学科里流行，那就是基于模型或机制，特别是工科。大量的仿真结果基于规律演绎也可能很好的解决实际问题。那么这个思路如果应用到无法实验的社会现象上是一种过度简化还是新洞见的发现方法。科学研究中控制一个变量，随机其他变量可以很好说明一些问题，但现在面向问题的研究中变量间的相互影响是巨大的，面对复杂系统需要新的解读方式。一种是逻辑清晰但解释力很差的模式，另一种则是表面逻辑混乱但实用性强的模式，我越来越感觉新的数学工具而不是统计学工具应该被引入到这个过程之中，当然，我讲不出什么道理，只是感觉。&lt;/p&gt;
&lt;p&gt;作为一个社会动物，了解时间尺度的利率可能生活轻松些，了解科研方法则可能让思考上更有框架感。最后推荐下戴维·布鲁克斯的TED演讲，应该说获取新知不要拘泥于形式，怎么方便怎么来。最后的最后跟现在赶论文的毕业生一个过来人的经验：&lt;/p&gt;
&lt;p&gt;语音输入&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>