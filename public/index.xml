<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homepage on Miao Yu | 于淼 </title>
    <link>/index.xml</link>
    <description>Recent content in Homepage on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Feb 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>混沌的冬日</title>
      <link>/post/chaos-winter/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/chaos-winter/</guid>
      <description>&lt;p&gt;1962年，美国海洋生物学家 Rachel Carson 出版了《寂静的春天》，这本书展示了农药污染下的没有虫鸣的春天，在其影响下公众开始关注环境污染问题并开启了环境科学研究的序幕。然而，国内公众对于环境污染的关注也许并不用等到春天，近几年国庆刚过，雾霾就开始几次三番的席卷全国，呈现出混沌的冬日。&lt;/p&gt;
&lt;p&gt;有人说发展的问题会在发展中解决，例如发达国家也经历过类似的阶段，但伴随产业转型与法规调控，污染问题都会自然而然地消亡；又有人说虽然城市会被雾霾笼罩，但从统计数据上看居民平均寿命其实比所谓田园风光的乡村更长；还有人说大气污染相比土壤、水还有固废污染都不算严重，只是可见度更高（也就是能见度低）……的确，雾霾这个现象背后有着错综复杂的社会经济影响，从不同的角度去看会发现不一样的东西。多一个角度看问题并不会让你过的更好，但至少更明白些。&lt;/p&gt;
&lt;p&gt;下面我将给出一些非技术与法规调控的视角，希望读后对于理解雾霾还有一些环境污染问题能有帮助。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;研究增长的极限&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 2.2.1     ✔ purrr   0.2.4
## ✔ tibble  1.3.4     ✔ dplyr   0.7.4
## ✔ tidyr   0.7.2     ✔ stringr 1.2.0
## ✔ readr   1.1.1     ✔ forcats 0.2.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2017-11-09-chaos-winter_files/figure-html/pm-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这是 Pubmed 上用颗粒物（particulate matter）作为关键词得到的论文数量，一百年来可以说是持续增长，特别是21世纪以来增长尤为迅猛。但需要注意的是到2015年达到了峰值（3429），16年已经明显下降（3134），今年还有两个月（2348），但不出意外也不会超过16年。至于为什么会有少量18年文献（26），这是学术界硬通货论文的通货膨胀，透支未来可以说是现代社会最伟大也最危险的发明，学术界亦然。也就是说，对于颗粒物的研究兴趣实际已经在降低了。&lt;/p&gt;
&lt;p&gt;这个现象可能有点反直觉，因为近几年大气环境污染的公众关注度非常高，经费投放也很可观，但学术界却降低了学术交流频次。无独有偶，使用传统研究热点例如汞、铬、二恶英、基因组、纳米颗粒去进行检索，都会发现研究在2014-2015年间出现了峰值。但同时如果去看一些新兴研究例如3D打印，颗粒物中的细颗粒物（fine particulate matter），则增长还是非常迅速的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./post/2017-11-09-chaos-winter_files/figure-html/sub-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如果把学术界所有人的研究精力看成是总量稳定的，那么论文数可以看成精力的指标，对于包括大气颗粒物在内的很多环境研究课题而言，学术界正在把蛋糕切给更新的技术与概念。同样进行雾霾研究，如果你从事微米尺度研究而纳米尺度的研究更容易得到学术界的认可，那么你的文章就很难发表，然后就是经费紧张，如此循环，而新概念也不断变成老概念。&lt;/p&gt;
&lt;p&gt;就颗粒物研究而言，目前学术圈总体关注度已经在下降，但分支中却有上升的。那么可想而知，学科内存在激烈竞争，并不是所有的颗粒物研究方向都是热点。而且还可以预期的是少数研究方向的异军突起会吸收更多学科内的研究资源，很多优秀的研究人员可能一开始选错了研究方向，最终的结局就是转行。研究的增长极限是客观存在的，所以如果你在这个年代打算去找专家咨询，最好去问上升期的新人，因为很多概念从出现到流行不到三五年，有经验的专家反而可能因为有学科内竞争关系而给出带有其自己都意识不到的感情色彩的论断。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;有原罪的雾霾&lt;/h2&gt;
&lt;p&gt;如果某天PM2.5爆表，然后你又恰好感觉到嗓子不舒服，那么很自然你会认为这是雾霾的锅。这符合情理，但不一定符合事实，雾霾跟健康是有联系的，但跟健康有联系的可不仅仅是雾霾。即使仅仅考虑大气污染，颗粒物也只是能够产生爆表API的一个因素，其余的例如工业主导的硫酸型烟雾或汽车尾气主导的光化学烟雾都会影响健康，都能让嗓子不舒服，此时你会把原因归到哪里？&lt;/p&gt;
&lt;p&gt;进一步讲，环境因素也只是影响健康的一个方面，遗传也起作用。假如你在雾霾天听到一个有气管炎家族病史的患者在咳嗽，你会认为是环境影响还是遗传作用？而根据最近Science的一份研究，即便你排除掉环境因素与遗传因素，仅仅是新陈代谢过程中DNA的复制次数就可解释癌症的发病率的66%，而这个过程根本就无法用先天后天因素来解释，就是个生长问题。&lt;/p&gt;
&lt;p&gt;在中国，雾霾是有原罪的，它实际承载了社会转型期人们的一部分焦虑。如果其对健康的总影响是10，那么其中真实作用可能也就二三，替遗传和其他污染物背了三四的锅，还有三四则可以说是心因性的。今年柳叶刀上一篇文献就提到，中国PM1跟PM2.5大概贡献了医院急诊的4.47%与5.05%。这种研究有两个问题，第一，即使排除了意外导致的急诊（例如车祸），就诊行为本身就会受天气影响；此外就是 type M 型错误，也就是说这个效应是真实的，但是影响不一定大。这是目前环境研究的一个通病，找一组病人和一组正常人（有的连这个也省了）采集样本，然后一把测定几百上千种污染物（这个现在技术上是没问题的），然后算相关系数，这种情况随机你都可以发现几个的，而这样做出的发现有个通病，那就是效应通常不大，很难重现。一个小而真实的效应或许有学术价值，但舆论一放大就会产生公众心理焦虑，而心理状态又会影响生理状态，这类影响可能并不比真实影响小。&lt;/p&gt;
&lt;p&gt;雾霾是有原罪的，但被过度聚焦了，由此产生的焦虑与恐慌本身也会产生健康影响。如果公众可以更好理解科学研究现状与其中的问题，这并不能客观降低空气污染的健康影响，但在实际意义上却可能减轻雾霾的心因性副作用。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;万金油的幻象&lt;/h2&gt;
&lt;p&gt;不知道从什么时候开始，万金油的心态重新出现了。以前如果我告诉你有一种方法可以让你永远远离雾霾危险，你肯定说我瞎扯。好，现在我换一种说法，在人工智能+区块链+可穿戴设备+大数据的实时监控下，我可以给你一副智能眼镜，上面会实时反应你现在的风险指数，如果指数超过80%，那么你就应该进入室内。逻辑上来说，如果你按照超过指标就躲到室内，那么这个风险永远不会变成100%，也就是说，这跟我刚才说的永远远离雾霾危险实质等同，但是这样的产品你多半不会觉得是瞎扯，甚至会愿意付高价购买，这又是为什么？&lt;/p&gt;
&lt;p&gt;万金油思维从来都没远离过我们，只是从熟悉的名词变成了看似专业的术语。人们有一种看起来很理性但又很荒谬的行为：乐观而盲目地相信着未知的科技。雾霾来了，那就买个最好的空气净化器；外面看不见了，那就来个3M口罩；嗓子不舒服了，那就去搞点清肺的保健品。其实很多人都知道这些科技可能还不成熟，但只要花钱了就有种事情完结可以甩锅的想法。真实的情况往往是越是大家关注的事物，就越有人去贩卖这种包装过的万金油，你买到的更多只是一个确定性的心态。&lt;/p&gt;
&lt;p&gt;在这个分工细致的现代社会里，绝大多数的服务业出售的都是经过专业化包装的确定性，用来抵消分工后一颗颗螺丝钉无法感知全局的焦虑。雾霾就是个全局问题，涉及很多不同专业的知识，当个体被复杂性搞晕时，最简单的方法就是掏出一把钞票买个心安理得。即使问题不能在当前根本解决，但生活总要继续，或许这就是万金油思维在进化上的意义。在雾霾这种大IP下，科学家、政府、骗子、掮客、投机商你方唱罢我登场，过分认真你就输了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;混沌的冬日&lt;/h2&gt;
&lt;p&gt;回溯千年，宋代诗人陆游在《秋霁》中提到:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;驱除云雾极知难&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;除了难在技术与法规，雾霾也是直指人心的。&lt;/p&gt;
&lt;p&gt;看看窗外，凛冬将至。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>假设检验的乌云</title>
      <link>/cn/2017/10/28/nhst/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/10/28/nhst/</guid>
      <description>&lt;p&gt;19世纪的最后一天，开尔文男爵在展望20世纪物理学前景时提出了两朵乌云，后来这两朵乌云分别催生了相对论与量子力学。时至今日，物理学家还在为了统一相对论与量子力学而不懈努力，而由此衍生的圈量子场论跟弦论已经不是几句话说得清楚的了。然而，21世纪的科研天空可就不仅仅是两朵乌云了，简直可以说乌云密布，这其中最大的一块大概就是空假设显著性检验（NHST），以此为基引发了无数次的关于科研成果可重复性、发表歧视、多重检验与p值、因果推断、科学决策等的争论，这些争论有些出现在学术期刊，有些则在科研社交网络蔓延。作为一名科研人员，身处这样一个窘境而不自知是可怕的，这意味着很多在做的工作根基上就有问题，盲从与职业化的科研正在蚕食科研成果的威信。我并无能力提出完美解决方案，但有必要把问题先提出来，疑惑对科研总是有益的。&lt;/p&gt;
&lt;div id=&#34;nhst&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;空假设显著性检验（NHST）&lt;/h2&gt;
&lt;p&gt;NHST更常见的形式是p值，也就是在空假设成立的条件下某事件发生的概率。p值有多流行呢？根据 Jeff Leek 的&lt;a href=&#34;https://docs.google.com/presentation/d/1hzdSDaPPSE9xUYZHhOVfQIRPPdwe0A9SdE7QDsK3bOA/edit#slide=id.g255a5ace66_3_796&#34;&gt;估计&lt;/a&gt;，如果把p值当成一篇文献，那么其被引次数已经超过300万次了，当之无愧的史上被引次数之王，甩&lt;a href=&#34;http://www.nature.com/news/the-top-100-papers-1.16224&#34;&gt;第二名&lt;/a&gt;一个数量级。原因其实很简单，p值已经渗透到几乎所有学科的研究中了，特别是实验学科。可想而知，如果产生p值的 NHST 出了问题其影响力有多大。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;院士身份悖论&lt;/h2&gt;
&lt;p&gt;要了解NHST的问题，我们首先要看下一个基于NHST的悖论。张三研究员是一名中科院非外籍院士，我们对其有两个假设检验，第一个假设检验的空假设是张三是中国人，备择假设就是张三不是中国人，因为我国不承认双重国籍，所以张三身份不存在薛定谔的猫态，要么是，要么不是。第二个假设是张三是中科院非外籍院士，备择假设就是不是，也是互斥的。那么两院院士不到两千人，中国人口14亿，概率大概百万分之二，备择假设的概率是0，这个情况比较特殊，也就是备择假设永远不成立。现在我们不知道张三的国籍，但知道他是中科院非外籍院士，但根据NHST，张三不太可能是中国人因为绝大多数中国人都不是院士，那么拒绝第一个检验的空假设后我们就会发现，张三成了不是中国人的中科院非外籍院士，额，那张三究竟是哪国人？&lt;/p&gt;
&lt;p&gt;也就是说，如果一个假设对另一个假设来说很稀少，NHST会在很低的条件概率下拒绝掉，然后那些稀少的事情在NHST里就成了无法被检验的事情。这个例子最早是 Cohen &lt;a href=&#34;http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf&#34;&gt;提出&lt;/a&gt;用来说明人们在使用NHST时的问题。本质上是多数人在使用p值时搞混了条件概率，拿上面院士身份来说，我们的假设 H0 在面对张三这个数据 D 时给出了拒绝 p(H0|D) = 0，这个决定是构建在假设 H0 成立时出现 D 的概率太低（即p(D|H0)）之上，也就是说NHST下，我们默认下面的概率是成立的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(D|H_0) = p(H_0|D)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果你修过任何基础的统计学课程都会知道这两个概率之间差了一个贝叶斯大爷。通过使用贝叶斯定理，在新数据出现后原有概率是要被更新而不是直接拒绝掉的。通俗点说就是 NHST 属于革命派，不认可就打倒你；贝叶斯属于改良派，用新的证据更新原有理论。好了，这里我们回顾一下科学史，革命派跟改良派确实都出现过，但当学科基础相对稳定后，更多的科学知识是改良派搞出来的，除了物理学两朵乌云，多数科学研究都是N+1模式，你现在在科学领域搞从零到一基本等同于对好几代科研人员同时开群嘲，结果一般会被认为民科或伪科。这个悖论的本质就是把假设下的事实与事实下的假设搞混导致的，这是NHST的一个致命问题。然而致命问题可不止这一个。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;方法学悖论&lt;/h2&gt;
&lt;p&gt;过去的100年，测量方法的精度是在不断提高的，而精度其实又会影响研究结果，很不幸，也是通过 NHST 来进行的。其实 NHST 在实验物理学里用的还是好好的，例如我去检测一个物理量，只有数据出现在其理论预测下数值四五个标准差以外才会对理论产生实质作用。此时，测量精度越高，由于测量误差导致的对原有理论的冲击就会越少，因为物理学的预测性要比化学生物等学科要好不少且此时 NHST 检测的原有理论是比较真实的。但在其他学科，特别是心理学跟医学的控制实验里，在实验开始前你几乎就可以确定空假设是不成立的，要不然你也没必要分组，此时你去搞 NHST ，几乎一定可以找到差异，此时测量精度如果不断上升，那么你会识别到一系列差异，但这些差异的效果是无法体现在p值里的，p值可能非常小，但效应却属于明显但很微弱，这样的结果也许可以发表，但对实际问题的解决几乎没有贡献。更极端的情况是如果你加大了样本量来提高统计功效，你总是能发现差异的，也就是你的空假设里原有学科理论为真也是会被方法学进步给推翻的。总结下就是 Meehl 在60年代就提出的&lt;a href=&#34;https://philpapers.org/rec/MEETIP&#34;&gt;悖论&lt;/a&gt;：方法学的进步与增大样本数对于相对硬（理论根基深厚）的学科证伪是正面的，但对相对软（理论比较模糊）的学科则是弱化。方法学悖论的根基其实是应用学科与基础学科的矛盾，基础学科用 NHST 检验观察事实中的理论，但应用学科用 NHST 来检验的是实验设计预测下的事实，此时实验设计的那个假设与 NHST 的空假设并不对应，而 NHST 先天弱化空假设的问题就凸显了。&lt;/p&gt;
&lt;p&gt;事实上，p值正在成为测量投资与努力而不是事实的标准，给定差异，我们总能找到足够的样本来发现这个差异（这也就是常说的功效分析）。也就是说，NHST 有时候功效不足测不到差异，有时候又一定会能测出差异，但科学事实并不会因为你使用了 NHST 而发生变化，特别是有意义的变化。而作为标准的p值其实在被样本数决定同时又综合了测定效果强度与不确定性，这样的一个标准其实有点多余，你完全可以用描述性统计与置信区间来分别表示效果强度与不确定性。p值也并不能增加新知识，考虑一个多元线性模型，我们只能在多元模型里得到参数，也就是有限检验，不能发现未知参数，但科学就是寻找未知；变量间的关系在数值改变后如何考察，正负关系如何预测，预测性也就无法实现。那么此时，还有必要使用 NHST 吗？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;低垂的果实&lt;/h2&gt;
&lt;p&gt;20世纪的技术有了意义深远的进步，但更现实的问题是，科研里低垂果实已经没有了，学科从分立走向交叉，开始不断服务社会，所有的科研都像是应用科研。服务社会职能的出现要求科学家回答的不再是科学问题而是现实问题，或者说，科学地回答现实问题。但现实问题非常复杂，科学家要想排除影响，大都采用控制实验来验证观察研究中的事实。注意，这里的事实不再是理论假设，而是一个现象，如果本来就观察到了差异，用 NHST 根本就不会让我们知道更多的事实，我们可以用无数独立手段证明这个事实的存在然后整合进学科知识体系，但并不能产生更多的思考，理论的预测效能在 NHST 里实际是体现不出来的。&lt;/p&gt;
&lt;p&gt;抛开这个问题，另一个更现实的问题是很多一线科研人员甚至还没搞懂 NHST ，说不好听一点，就是只会模仿别人论文，这样连错的都一块模仿了。这里面深层次的原因是科研人员的教育仅仅停留在了知识与逻辑层面，没有系统的科学思想训练与科学史背景。后面那两个对发文章不但没用，反而会让你怀疑科生，但没有后面这两个，你只会看到一个各干各的一团和气的研究氛围，没有评论与争执的岁月静好只会让整体科研水平永远停留在二流追随的状态。你去看各学科顶级期刊里的评论与回复，你会体会到哪里在发生的事，举个不恰当的例子，你到 Github 上去看，那些常用的软件都会在不断的更新与协作，而学术论文的更新与协作却少得多，一个重要的因素就是很多所谓科研成果永远都不会有人重复与验证，最大的作用就是放在简历里谋求职业生存，很多人自以为掌握了高端的果实但其实那些果子对学科发展并无意义，如同 NHST 一样，用之无味，弃之生存空间都没了。如果你把科研当职业，起码也要有点 Github 的分享与协作的职业素养。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;路在何方&lt;/h2&gt;
&lt;p&gt;关于 NHST 其实还有很多问题例如多重比较跟p值发表歧视，但系统去看，p值也有着自己的生命力，我想更多人关心的是如果我不用 NHST ，拿什么证明我的结果可靠？如果没得选，这剂毒药还是得吃啊。答案其实上面都大概提到了，你如果坚持使用p值，那么就也请同时报告参数估计与置信区间，虽然这个方法也被人喷过。如果你打算完全开一条新路，那就去学贝叶斯统计，贝叶斯统计有自己成套的处理体系，简单说就是先假设参数分布，然后用数据更新分布，后验分布计算出来就同时有点估计跟方差估计，同时多重比较问题也不存在，但随机错误无法避免，此时参数估计方差大也能体现，后续研究可以使用这次的后验数据作为下次先验数据，这样你可以实现完全的 N + 1 模式科研，其验证与预测性也很大程度依赖采样与模拟技术，之前贝叶斯方法不能流行很重要的一个原因就在于计算比较贵，现在就便宜很多了。但我想说的是，这类知识因为提出时间不长除了几个数的过来的名校开设了课程外几乎完全需要自学，不过你要是真对科研感兴趣，这都不算什么。&lt;/p&gt;
&lt;p&gt;有人说我写过的几篇文章是在劝退，我还真没那个意思，看清现状后的理性选择对人对己都是负责的，如果你根本不适应这个现状还又觉得没了退路，那你肯定是被什么毒鸡汤绑架了，这世界上本来就没路。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>joyplot的前世今生</title>
      <link>/cn/2017/10/23/joyplot/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/10/23/joyplot/</guid>
      <description>&lt;p&gt;1979年，英国乐队快乐小分队（Joy Division）发行了自己的首张唱片《Unknown Pleasuers》，这张专辑发行两周内就卖了5000份，但问题是……印了10000份。然而，当乐队的单曲《Transmission》发布后，这张后朋克唱片很快销售一空。作为一个乐盲，我是没搞懂这歌的意思（好像对收音机很不满）。整个70年代英国社会的不断衰落并催生了朋克运动，青少年们对现实极度不满，采取了一些很极端的表现形式来抒发感情。不过这个专辑在2017年又重新流行了，倒不是因为社会衰落，而是那个设计极为特殊的封面。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>改变中的科普及与科研的互动</title>
      <link>/post/data-popular-science/</link>
      <pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/data-popular-science/</guid>
      <description>&lt;p&gt;科普的环境正在改变。&lt;/p&gt;
&lt;p&gt;2016年，全国高中升高等教育的升学率从1990年的27.3%提升到了94.5%，一个高等教育学位对未来20年的年青人已经成了标配。《趣味物理学》、《从一到无穷大》、《十万个为什么》等科普经典里的内容对于普遍接受过高等教育的群体吸引力正在下降，一方面多数内容实际都成了通识教育的一部分；另一方面则是搜索引擎其实替代了大量的常识性科普，“你不会网上搜一下”成了年青人的常用语。然而，科普的潜在受众并没有减少，打着“养生保健”主题的各类伪科学与阴谋论侵蚀着中老年群体；年青人对科技里技术层面的关注远远超过对科学的关注，实用主义下电脑算命、星座研究等伪科学在技术下获得重生并展示了强大的生命力；流行文化中的成功学虽然不断引入心理学与行为经济学的研究成果，但公众对于得出结论过程的关注显然少于关心结论本身。类比美国科普杂志的发行量与人口比例，科普的主动受众大概是大几百万的级别，占总人口千分之二三，这些人会主动寻找甚至创作科普作品。对其他人而言，在走出校园后，科学知识就成了个“靠谱的黑匣子”，敬而远之，不明觉厉。&lt;/p&gt;
&lt;p&gt;目标人群的变化使得科普内容从通识化走向了专业化与前沿化。体系化的知识被公立教育承包，剩下的碎片知识只能由相关领域的专业人士来总结提炼，而这所谓的专业人士，一般是指处于或曾经处于学术界的科研人员。科普正在去职业化，或者说并入科研体系，成为科学传播里重要的一环。职业化的科研需要为公众负责，而公众则可通过舆论切割科研经费在不同学科的比例。举例而言，北京的雾霾不仅仅让口罩、空气净化器行业高速发展，也让大气环境化学领域的研究人员瞬间经费充裕起来，充盈的资金会吸引到最新的技术进而推动学科发展。然而当资金总盘口一定时，一家吃饱往往意味着另外好几家要节食了。也许很多一线科研人员没有意识到，科普正是释放学科重要性甚至储备人才的最佳渠道，面向大众的科普不但是公益的，也是符合自身学科发展利益的。只有科学的声音更多更大，公众才会给予更多关注与投入，在这一点上科普跟科研并无区别。&lt;/p&gt;
&lt;p&gt;专业与前沿科普所面对的挑战往往来自于写作者自身，科普不是科研，讲求趣味性与故事性，而这两个特性要是出现在科技论文里基本是开送死模式。同样，科研成果的报道是N+1的，也就是说看的人知道了N，你告诉他1就够了。但科普往往是从零开始，当然不用到N+1那么远，但这意味着要面对的读者具备的是常识或通识，而你要告诉他的是从那部分知识发展来的新知。职业作家可以很好组织常识或通识的知识但你让他搞前沿，理解能力就有限了；职业科研人员对付的只是审稿人也就是专家，很多默认知识往往是逻辑上清楚明白的，但历史沿革基本不知道，这就导致故事讲得像论文，趣味性也无从谈起。且高度细分的研究领域让科研人员自身也无法对学科形成整体感知，同一专业的人可能互相都听不懂对方的研究，更不用提对公众的科普了。&lt;/p&gt;
&lt;p&gt;其实科普对于科研人员自身也是很重要的。北美高校会对研究生与博士后开设基金申请与科研写作的课程与讲座，其中就会特别强调申请基金时一定要跳出自己的研究框架，用平实易懂的语言来阐明研究的意义，因为读申请书的人或者说做决策的人对技术细节关注度有限。而进行学术会议的报告时，前几页一定也是讲背景的，而且最好是一个有着婴儿与母亲的故事。既然无论如何都要学会用平实语言讲故事，科普创作是很有益处的，它可以帮助作者系统总结整合知识实现双赢。&lt;/p&gt;
&lt;p&gt;同时，信息技术特别是数据科学也在冲击着科研，自然也冲击了科普。2006年，JoVe（Journal of Visualized Experiments）创刊，2014年被SCIE收录，影响因子达1.325，作为一份实验学科的期刊并不理想，但作为第一份同行评议的以视频形式发表的期刊，这个成绩则标志着学术交流从文字传播的单一形式开始走向多媒体富文本的时代并得到了一定的认可。现在科技论文已经不仅仅是文字传播了，活跃的科研人员正通过社交网络传播着成果，很多期刊都接受用视频、动画甚至幻灯片作为论文的补充材料。而科普也处于这一波技术浪潮之中，科普漫画、短视频、播客及博客都成了科普的阵地。其实借助多媒体互动，很多科普作品开始包含探索式小游戏与在线应用，引导读者自己得出结论。在技术层面科研跟科普的界限不再明显，同样的展示手段既可以用来科研展示，也可以用做科普说明。&lt;/p&gt;
&lt;p&gt;目标群体的变化、内容主题的专业化与前沿化、信息技术的冲击，今天的科普任重道远，但如果可以与科研有机结合互动，相信科学传播会上一个新台阶，公众与科学探索都将最终受益。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>十万阅读量的科普</title>
      <link>/cn/2017/10/01/popularization-of-science/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/10/01/popularization-of-science/</guid>
      <description>&lt;p&gt;前几天有新闻说某高校认为科普文阅读量10万+算成果，我觉得还是有点难度的。我用美国科普杂志来举个例子，《科学美国人》全美发行量不到60万，加上《发现》跟《新科学家》，整体发行量也就是一百多万，美国人口大概是中国人口的1/4，也就是说中国即便科普发达到美国的程度，日常感兴趣的人数大概五百万封顶。按说国内也有科普期刊，但那个发行量惨不忍睹，倒不是说国人不感兴趣，只是感兴趣比较晚，很多人没形成看杂志习惯直面了互联网时代。我估计国内微信公号、果壳、知乎、科学网基本已经让可圈住绝大多数关注科普信息的人了，那么这个人数是多少呢？&lt;/p&gt;
&lt;p&gt;不考虑泛知识化的知乎，果壳跟科学网日活用户用一些站长工具去查加起来大概是一百万，我估计这两个网站至少能占总关注流量的20%，所以目前日常对科普感兴趣的人也就是大几百万级别，跟上面那个发行量的估计差不多。这个覆盖面其实应该跟金融、IT等行业差不多，但远不如养生、娱乐八卦还有新闻。那么百万量级的圈子产生10万阅读量相当于个位数百分点的人都看到了，这个还是很了不得的，这要求内容足够有趣又恰到好处的专业，太难了看不懂，太简单不值得传播。&lt;/p&gt;
&lt;p&gt;用科学网博客来看，其文章周最高点击大概两三万，就算是一天点出来的也都是关于科普的，距离10万+还是有距离。这种量级科普文全国每天能出一篇就很不容易了，而且作者也不可能都是一个高校的老师/学生。而且实话说，很多10万+的文章名义上是科普，实际可能是搞怪或泛娱乐化行文，读者看了并不一定有学到新科学知识的感觉。&lt;/p&gt;
&lt;p&gt;也许10万+的科普本就不易，但我觉得当前科普过多关注了知识与事实，缺少科学思想跟历史沿革的传播与论述，这个可能是更需要普及的。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;科普的背景&lt;/h2&gt;
&lt;p&gt;高中毕业，不论你学的文还是理，知识传授上一般侧重知识或事实本身，或者说学到的是通识。例如地球是圆的、力学有三大定律、元素周期表是按什么排的…这类知识其实就算老师不教，你看看《十万个为什么》什么的也都能知道。科普主要面向知识背景是高中组的，大多数人不进行科研，就算进行科研其很多科学背景知识也是高中的（因为你大学可能学了某个专业，但另外的学科最理想也是停留在高中阶段）。&lt;/p&gt;
&lt;p&gt;这部分内容基本不用科普，或者说包含在更广泛的知识普及中就好了，需要思考推理的部分不多，主要是了解事实，形成背景概念。这对于本来就关注的人没什么难度，但如果是中小学生科普，重点要关注这部分。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;知识向科普&lt;/h2&gt;
&lt;p&gt;大多数科普文章其实是在做大学教材的通俗版，这类文章普及的是专业知识，例如pm2.5是怎么回事？行星间距离如何测量？端粒长度跟寿命关系…这类文章告诉我们从已知的高中阶段背景知识如何得到专业的背景知识，大都是专业概念普及，这个是社会大多数人科学背景的上限，却是专业职业化要求的下限。目前这个层次的科学知识几乎可以被维基百科覆盖，也就是说你可以用维基百科作为这类科普文的一个主要参考，另一方面就是趣味性了，如果你能加入更多互动跟多媒体，自然比枯燥的维基百科要好很多。如果你本科专业是理工学科，那么此类文章基本不用看，因为拿到学士学位就表明你已经掌握了这部分内容。这部分内容互联网的替代效果比较明显，国内做的也不错。但科学知识是不断更新的，如果过分强调已有观点其实不但不是科普，反而是科黑。例如方舟子的作品就有点过分强调知识的正确性而不考虑科学的发展，他本身其实也远离科研很久了，这个语境下他实际成了已有（甚至是过时）科学知识的卫道士的角色。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;前沿科普&lt;/h2&gt;
&lt;p&gt;这部分的科普是从已知走向未知，目前最容易出问题的就是这一部分，因为在这一阶段是要建立在读者有大学水平知识上的已知，同专业的还好，但很可能读者大多数停留在高中阶段，所以他们会看不懂。&lt;/p&gt;
&lt;p&gt;这部分最常见的就是前沿科技成果报道，其实多数人看了后只会产生“高科技”的一个感知，并不能理解掌握其中的知识。在我看来，如果文章只是这样让人不明觉厉，那跟告诉别人我有个魔法箱可以变戏法但不给看内部结构一样，读者跟作者都浪费了时间。这部分写的人最好本身是理工背景且实际进行过至少两个学科以上的科研，同时写作水平也有要求。目前的尴尬在于做科研的不会写，会写的看不懂，然后大家只能很和谐的点下赞，最好的情况就是记住了结论（但要小心记结论并善于操纵读者情感的人，他们通常擅长用海量最新研究把你忽悠得就差跪拜交钱了）。&lt;/p&gt;
&lt;p&gt;但其实我倒觉得一线科研人员不应该对这个陌生，你写的基金或论文前言的前几段也是这个要求，只不过都多少夹了点私货。一般套路是先讲个故事但不说结局，引出研究背景跟意义，然后新发现在哪些方面进行了突破，然后是各专家的采访意见，然后是综合观点，最后又把故事收个尾展望下美好未来，通常也有高质量的图片、插画甚至视频进行解释。&lt;/p&gt;
&lt;p&gt;科学成果的吸引力从来就不如娱乐明星的花边八卦，如果我一天平均有一个小时看书报杂志，那看了花边就不会看科普，注意力总是被竞争的，而且读这类需要有知识背景才能看懂的文章也比较累。从另一个角度看，科普在这个阶段对大众谈不上普及了，但对社会中有求知欲的人而言却很关键，而这部分人很有可能推动科技进一步发展。就国内看，科研工作人员大概也是几百万这个级别，其实面向他们的前沿科普很有必要。遗憾的是，这大几百万从业人员的市场规模非常有限，对他们科普还不如卖成功学鸡汤来的容易，优质内容不能专业化生产，吸引力就很有限。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;科学思想&lt;/h2&gt;
&lt;p&gt;前沿科普走出“看结论”的现状最需要普及的不是知识，因为知识都是比较前沿的，很可能被后续结果推翻掉。前沿科普更重要的是科学研究的思想，这个其实即便一线科研工作者自己可能都比较迷糊，大部分人是站在前人基础上往前推进，前人的研究结论容易保存，但思想可能早就消散了。&lt;/p&gt;
&lt;p&gt;举个例子，我可以找两组高中毕业生，一组让读《审判达尔文》，另一组读《盲眼钟表匠》，完了以后测试其对进化论的认可度，可以预期两个差异会很大。两本书都是从逻辑推理开始的，但结论恰恰相反，如何解释两种思想的冲突？这时候如果没有科学方法论的背景知识，很容易就陷入盲从。可以说，科学思想的科普需要引入这些矛盾而不是结论来让读者进行独立思考判断，并形成存疑并可通过观察与实验验证某个论断的科学素养。&lt;/p&gt;
&lt;p&gt;科学思想或者说方法论其实比知识本身更重要，科学知识可以证伪，但方法跑偏了结论就不靠谱了。通过科学思想的普及，我们可以反观审视科研的每一步并诚实的认错换取进步。但现在很多科学向文章过分强调的知识本身的不可动摇，这对科学思想传播没有好处，属于钻牛角尖了。至少我们要给讨论留下可能性而不是一个个论断，如果结论都确定了，那我们还有必要讨论吗？&lt;/p&gt;
&lt;p&gt;这里理想的文章应侧重于整合已有知识进行创新得到的新知识，基本上只有经验很丰富的人才能站到一定高度上解说一些原理或技术。开篇是一个主题，然后作者需要把相关知识与方法论进行高度整合最后形成自己的结论与见解，跟学术论文要求也差不多。此时仅仅谈思想谈逻辑就不够了，最好要整合历史，把发展沿革搞清楚。我觉得这类文章一个月甚至几个月能看上一篇就很不错了，消化这种文章也需要很长时间。坦言之这个认识高度出的作品属于简本教科书，内容却是职业或专业教科书才覆盖或根本就覆盖不到的，可以参考卢昌海老师的作品还有刘未鹏的博文。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;科学史&lt;/h2&gt;
&lt;p&gt;当前科普文章的另一问题是过分强调了逻辑主导的故事而忽略了历史发展的真实过程。这就导致很多知识在解释时感觉很生硬，没有上下文，直接从石头里蹦出来了。科普文章趣味性的重要来源是故事，历史沿革是很容易讲故事的，教科书普遍采用学科逻辑框架，所以科学史作为科普对于专业科研人员也是有意义的。这个的代表可参考普利策奖非虚构类的获奖名单，很多时候回顾科学史会发现很多有意思的小故事，比明星八卦要精彩多了。&lt;/p&gt;
&lt;p&gt;终极科普可能会走向科幻了，你可以体会其中思考的乐趣，当然知识背景设定就不要管了。自己构建一个逻辑自洽的物理甚至社会运行体制跟历史是非常有挑战性的，但我们终将走向未来。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;新的指标&lt;/h2&gt;
&lt;p&gt;十万阅读量的科普天生是需要知识跟趣味来获取读者的，但更重要的可能是科学思想跟历史的传播，这可以凸显出科学最与众不同的特点，例如不断犯错、重视事实证据、当前存在的不足等等。切不可孤芳自赏，用别人看不懂来作为高深，有些科学知识本身确实不适合普及，但过分用技术名词来包装，然后用泛娱乐的方法来调侃就很没必要了。&lt;/p&gt;
&lt;p&gt;普及类文章的难处在于一方面要影响更多人，另一方面却要保障质量。十万阅读量并不是好指标，含有专业信息网文超过一万阅读其实就很优秀了。让我说另一个需要考虑的指标是评论质量，高质量的评论不仅说明正文很好，也说明可以吸引到高水平的读者，此外评论本身也是科学思想的交锋，读者从这个过程可以学到更多。大家可以去围观下海外期刊或预印本文章下面的评论，哪怕匿名也能体会出读者本身的专业性，反观国内基本上还是情绪化的占多。读者跟作者间从来都是双向选择，越是情绪化，理性读者的流失就越多。&lt;/p&gt;
&lt;p&gt;这是一个竞争注意力的时代，科学知识与方法论有必要在更多人的心中占有一席之地。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>幻化残生中的研究生</title>
      <link>/cn/2017/09/24/ecmb/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/09/24/ecmb/</guid>
      <description>&lt;p&gt;幻化残生，也就是环境、化学、材料跟生物这四大学科的近似谐音，属于各类研究生中实验比例最高的专业。然而，其生存现状并不乐观：&lt;/p&gt;
&lt;p&gt;首先，这四个学科属于建立在脑力劳动之上的体力劳动。例如前处理、过柱、表征、养细胞、涂板子、野外采样等等，流程性非常强，到时间点上不论节假日还是凌晨饭点都得待命，但有时又会发现找个本科生带上两天也能做出来。一个尴尬的事实是实验学科一个重要研究方向就是取代人工操作实现流程自动化与便携化，当实验简单到轻轻一按时，研究生训练得到的技能瞬间贬值，更尴尬的是实现这个过程需要的背景知识是物理、机械跟电子工程而不是幻化残生，会某项实验技能短期可以取得不错的成果，但长期看几乎一定会过时。&lt;/p&gt;
&lt;p&gt;其次，特别拼先进仪器技术，进而导致平台建设重于人才培养。今年这个技术能发顶刊，明年就可能被取代了，有些特殊资源例如光源没有背景想约个机时难得要命，如果不进行一些高开支实验可能编辑就直接拒稿，而先进仪器装备的价格奇高，所以从经济角度，这四个学科都属于很烧钱的。那么这里的尴尬就是你的才能受限于仪器平台，从研究机构角度看，投资仪器显然比投资人才培养在初期更有效果，而人才培养初期其实也就是仪器操作。这个没啥办法，现在很多科学问题的回答其实早就脱离了理论导向阶段，而是我有一个问题想回答，但目前技术回答不了，也就是假设早就有了，就等着新技术检验。你去看这些年诺奖，很多是技术获奖而不是理论获奖。也就是说，实验学科比起人才更需要仪器平台资源。&lt;/p&gt;
&lt;p&gt;再次，这几个学科产业转化基本停留在前言里，毕业后除了年龄比同专业本科生大了不少，在满足业界要求上本质区别并不大，这进一步导致本应分流到业界服务社会的博士硕士继续留在学术界造纸，而想从学术界熬出头你看前人经验借鉴意义不大，很多人没考虑时代造就的红利窗口期而大谈特谈自己的奋斗，但要知道此一时彼一时，目前学术界的门槛比10年前高了很多，同样的奋斗强度10年前进高校很容易，现在可能博后都没人要。如果本科转行也就算了，但到了博士转行就真的是在奉献青春了，当然这可能是无法避免的。&lt;/p&gt;
&lt;p&gt;这些现状经常搞得研究生自身怀疑人生，看着转行金融、咨询、IT的同学心有不甘，用学术理想充值的生活把自己隔离在实验室内，但走出实验室的柴米油盐变量太多，控制不来。同时，你又会很惊奇地发现，这些年报道的学术界年轻有为的青千、优青与各路人生赢家基本都是这四个学科的；而且从经费分配跟论文影响力上看，这四个学科也是超级大户；再从经济学角度去看，你会发现围绕这四个学科的仪器、耗材甚至样品测定跟论文润色服务都已经形成了成熟产业链，行业利润十分惊人。注意，这些产业是对科研进行支撑的而不是业界，如果只是这些行业高速发展而产业界没有起色，那事实上是在用纳税人的经费吹肥皂泡，不会长久。&lt;/p&gt;
&lt;p&gt;这并不奇怪，实验学科的知识与技术更迭速度是非常快的，从走进实验室那一刻，你就会发现师兄师姐用的技术学校里根本就没教过或仅仅做了个展望，系统的学习基本上都被传帮带模式替换。如果你自己不去问为什么，大概率你师兄走的弯路你还得走一遍，你师姐画不出的图你也画不出来。更尴尬的是，有时候你会发现，如果你的想法是属于排列组合出来的，那么其实仪器公司完全可以替你做，他们不做并不是不会，而是等着收服务费，你发你的纸，我赚我的钱，各取所需。在这个场景下如果还没意识到自己的民工本质，那大概率是要做一辈子民工的。&lt;/p&gt;
&lt;p&gt;曾经有人提过学术界存在生态位，大家各做各的相安无事，但这个想法现在看比较天真，因为现在竞争者基本都不是来自学科内，而是其他学科的入侵，如果这个问题你自己学科的人搞不定，别的学科就会过来。例如发现一种新材料，如果你觉得意义不大不掺合短期没啥问题，但做材料的表征完了得找应用出口啊，环境、生物、化学都有，你是无法限制某种研究只关注自己学科问题的，技术有自己的生命力，总有人会转过去，事实上这可能是目前科学进步的一个范式：个别学科突破，带动其他学科发展。&lt;/p&gt;
&lt;p&gt;基础学科对新技术的接受度要快于应用学科，一个常见的模式就是某个数学模型首先应用在物理，然后化学，然后生物，然后是边缘综合学科例如环境、医学，然后就是社会科学。当然也存在某些从应用角度出发的模型后来被应用到其他领域，金融与生命科学中经常出现这样的案例。但你应该发现一个问题，要想解决现在的问题，通常老路是不通的，要么回归基础学科，要么从别的学科借鉴，不论哪一种都需要你持续学习新知识，特别是外学科知识。有一个最简单的办法就是你去看看那些最聪明的人在用什么，然后想想能不能用到自己的学科框架里。&lt;/p&gt;
&lt;p&gt;实验学科的发展有时候是很残酷的，初期势必牺牲掉一批掌握过时技术的研究生，这个国内外都很常见，但国外业界会吸收一部分，国内则是学术界大面积收留，这个问题的后果就是现在很多教授对于学生无力指导，看到概念就回来让研究生试，研究生自然苦不堪言，毕业后就业方向非常窄。但同样是实验学科，高能物理、生物统计的毕业生转行就相对容易些，因为可以去做码农，至少生活水平对得上学位。而很多实验学科的研究生对此并不感兴趣，甚至完全不懂，思想上停留在努力实验发论文拿教职的简单规划上，不喜欢接触社会就只接触仪器。这其实是最大的偷懒，科研是需要脑力持续投入的，如果是实验学科还要加上体力。不但要持续学习，还必须要主动学习，关心前沿，而这又是研究生的普遍弱点。&lt;/p&gt;
&lt;p&gt;学科前沿是一个很模糊的东西，对幻化残生而言，教科书上的实验技术是一定落后于科研的，此时对学术前沿的感知要么来自文献，要么来自会议或培训，坦白说，这两个方法都具有很强的主观性，夹杂很多人的小算盘。好比你想在微信里打开淘宝链接，不是不行，就是要通过复制过程恶心你一把，但其实这种经验过程你也没啥办法。另一个方法是各种文献信息学指标，例如H指数，被引率等等，但这些指标属于后验指标，你得至少等文章发表过去两三年才能开始评价，但这两三年中也会有一大把新趋势出现。还有个方法就是自己当期刊编辑或审稿人，其实这个是很多教授的独门秘笈，因为你会比其他人早好几个月知道新研究的动向，但研究生拿到的审稿机会本来就少，高水平期刊更是不会找研究生审稿。所以其实对于很多研究生而言，想了解前沿跟他人的研究动向几乎不可能，而根据我的观察，如果同行坐到一起聊天你对新动向一无所知，那么对方也就不会在你身上浪费时间了。有些出版方跟研究机构也会发布一些热点文章，但多数基于编辑经验，并不一定准确。&lt;/p&gt;
&lt;p&gt;我之前曾写过用文本分析来探索前沿，但我估计很多人也就是看个热闹，并不真的会用。也罢，这个坑我挖的我填，我做了一个在线app来探索最近三年学科趋势：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://yufreecas.shinyapps.io/journaltone/&#34; class=&#34;uri&#34;&gt;https://yufreecas.shinyapps.io/journaltone/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;之所以选三年，是因为我感觉三年以上用引用指标更靠谱。这个应用保留了最大的自由度，你可以简单修改期刊或关键词用默认代码绘制相关趋势，或者直接尝试修改成看五年甚至十年的趋势。但我需要声明的是这个app我放到一个免费在线平台上，每个月流量有限，超了就没法用了，所以我建议你本地安装运行。源码&lt;a href=&#34;https://github.com/yufree/journaltone&#34;&gt;在此&lt;/a&gt;，此外，本地运行时多数包cran上有，但有一个方便抓pubmed数据的包是我自己写的，你需要从github上下载安装：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;#39;yufree/scifetch&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其实，对于幻化残生的研究生而言，主动了解科研趋势只是一方面，了解你自己才是更重要的。当你觉得不好时，不要总是怪罪时代跟环境，也想想自己身上的问题；当你一帆风顺时，不要总觉得这是自己勤奋与努力的结晶而忘记了科研浪潮的背后推手。随波逐流不会过的太差，但放弃思考是绝难在学术界生存下来的，不要真的幻化残生了。&lt;/p&gt;
&lt;p&gt;师兄只能帮你到这了，剩下的，我也没想明白。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析那些事儿</title>
      <link>/cn/2017/09/14/pca/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/09/14/pca/</guid>
      <description>&lt;p&gt;目之所及，主成分分析应该是科研领域里最通用的一种数据分析手段。在相当长的一段时间里，我认为这种方法主要是用来进行探索分析的可视化手段与数据降维，但最近因为出现了一个绕不过去的数据问题就把主成分分析又拎出来看了一下，这才意识到这个方法其实四通八达，可以把很多数据分析的概念连接起来。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;从线到点&lt;/h2&gt;
&lt;p&gt;首先还是回到一个最简单的场景，我有一堆数，想找出一个来做代表。从距离测量角度，就是找一个点让它离所有点最近，这就成了个优化问题，此时不同测量方法结论是不一样的。例如你考虑距离的绝对值最小，那你就会得到中位数；如果是差异的平方，求导后就是均值。回想下对一堆数找一个数，其实就是一种降维，从1维降低到0维。这里我们只考虑最小化差异的平方，那么求均值就是主成分分析把从1维降低到0维的应用场景。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;从面到线&lt;/h2&gt;
&lt;p&gt;现在复杂一点，我们设想一个二维（无码）平面，如果我们对其降维，那么降一维就是线，降两维就是点。而且我们可以确定降两维的那个点肯定就在降一维的线上，不然你这个降维就丧失了代表性。至于如何保障代表性，一般来说要交给数学家。那么这条线会通过所有点的均值，此时你应该想起来二维线性回归也通过这个点，那条线可以通过最小二乘得到，会不会就是我们要找的那条线？这个答案是否定的，最小二乘里最小化的是因变量到回归线的值，但是这里主成分分析最小化的是所有点到一条线的垂直距离，模型上细微的差别导致结果也会有区别，事实上求解过程也不对等。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;主成分分析的求解思想&lt;/h2&gt;
&lt;p&gt;虽然最小二乘回归线是高斯－马尔可夫定理下线性回归的最佳无偏估计，但主成分分析里二维降一维里那条线的求解思想并非回归到均值，常见有两种解释。第一种是寻找低维度空间，使投影点上到高维度点距离最近；另一种则是从统计角度寻找让点之间方差最大的方向映射，然后寻找跟这个方向正交的方向中方差最大的方向继续映射。从求解上，这两种解释都可转化成最优化问题，都是先归一化，然后求协方差矩阵，通过求导求特征向量跟特征值，那个方差最大的方向或距离最短子空间维度就是协方差矩阵里特征值最大的特征向量，剩下的方向或维度跟前面那个正交，再次找方差最大或距离最小即可。当然协方差矩阵不是一定要求的，如果你选用奇异值分解的套路就完全不用求。在这个求解策略下，解析解就是正交的，如果不是，那就不是主成分分析了。&lt;/p&gt;
&lt;p&gt;除此之外，理论上你也可以用隐藏变量模型迭代求解，不过有解析解不要用数值分析去逼近，而且有些矩阵运算可以进行分布式计算，这个在数据量大时是要特别考虑的。主成分分析求解上可以用矩阵是很大的优势，虽然理论上其概率解释并不完美。不同求解思想的多元分析方法其实背后都是有思想跟应用场景的，虽然理论上很多都是通用方法，但如果不适合你的数据就不要用。当前由于技术进步，之前很多很耗性能的方法目前都可以计算得到，如果搞科研我们要找那个最完美的，但工业应用可能更看重性价比。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;归一化&lt;/h2&gt;
&lt;p&gt;如果我们进一步考察三维空间，那么我们的降维就首先是一个平面，然后是平面上的线，然后是线上的点。此时如果你对所有数据点乘2，那么很自然点、线、面的坐标位置都要变化，这样你就可以理解一个事实，那就是主成分分析对尺度是敏感的，所以一般来说都要对不同尺度／维度的测量进行归一化，否则你的映射会被大尺度的维度带跑偏。到现在为止，我们可以大概对主成分分析有个直观感受：将高维度空间的点映射到低纬度空间的点且要保证这些点之间的差异关系最大程度地保留，至于怎么保留，不同求解思想实际求解结果一致，都可以用矩阵运算，内含了进行转换或映射时要沿着正交的维度走（使用了正定阵），所以求解完矩阵就可以得到符合要求的低维度空间，而且低维空间是正交的。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;投影点的方差&lt;/h2&gt;
&lt;p&gt;主成分分析经常用来可视化，这里我们回到二维平面降维的场景仔细看看我们究竟可视化了什么。首先我们有一个二维点A，这个点投影到一维线上得到点B，这个点跟所有点的均值C连线就是到0维的投影。目前我们已知AC这个线，同时A到一维线的距离又要最小也就是垂直，这样A、B及C构成一个直角三角形。此时根据勾股定理BC这个距离最大，也就是一维到0维时所有投影点的距离之和最长，在这个方向中各点间方差最大程度保留，也就是找到了方差最大的方向。事实上，因为前面提到的直角三角形，每降低一次维度，点之间的距离比高维度都不可避免的减少，如果此时点聚到一起不一定相似度很高，但如果主成分占总方差比重比较大，那么这些点就很有可能十分相似。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;多维标度分析&lt;/h2&gt;
&lt;p&gt;说到距离，其实主成分分析也是多维标度分析的一种。在经典多维标度分析中，测定点很困难，但可以测到点之间欧式距离并构建距离矩阵，因为其映射子空间里点之间方差最大时可以证明点之间的距离也是最大的，这个特性保证了当我们只有距离矩阵时进行主成分分析得到的低维映射也可以保证两个空间间距离最短，这样主成分分析事实上符合经典多维标度分析。也就是说，在你能够测到欧式距离而不是点时，是有可能重构出原始标度的，这点在结构生物学上有应用，但我完全不了解。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;概率化的主成分分析&lt;/h2&gt;
&lt;p&gt;主成分分析在求解上基本都走了矩阵运算的捷径，结果也是等价的。但这个过程不算是一个概率模型，因为可能产生不确定度的白噪音根本没出现在求解模型中。此时，我们应该意识到，这个子空间可能是某个概率模型的解，但如同我们只求了均值没求方差一样，似乎我们没有考虑模型的不确定度。这样我们需要从统计角度把主成分分析统一到基于统计学的数据分析中，这样也许会对将来构建相关假设检验模型有用，当然这也意味着我们可能不太方便再用矩阵运算来求解了。&lt;/p&gt;
&lt;p&gt;首先，我们对数据点进行假设，例如来自一个正态分布，那么主成分分析的问题就转化为求一个子空间，使得映射后的距离最小。让我们把这个映射关系描述成下面这样：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t = Wx + \mu + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里t是我们观察到的数据点，W是映射关系，维度不变可以理解成坐标轴旋转，x是映射后的点，&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;代表x的均值，&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;代表高斯随机变量。这样我们看到的点符合均值&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(WW^t + \psi\)&lt;/span&gt;的正态分布，这里&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;代表了随机误差，如果我们不考虑这一项，那么主成分分析是完全可以用特征值跟特征向量求解的，此时我们默认那个误差项0方差。但是，实际场景中我们都很清楚每一个高维样本点都至少存在测量误差，这个误差的方差不是0，那么此时我们应该在模型中包含误差项，但一个很尴尬的问题是我们对这个误差一无所知。此时我们假定所有点的误差项来自于某一个方差统一的正态分布，然后有了这个限制条件就可以求解了。加入了这一部分后，主成分分析就可以进行假设检验了，例如某个点是否属于异常值。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EM算法&lt;/h2&gt;
&lt;p&gt;说到求解EM算法是绕不过去的，这个算法普适性比较强，存在隐藏变量的模型求解都可以用。主成分分析可以看作一种存在隐藏变量的模型，我们在低维空间看到的点背后是高维空间点但看不到，反之也成立。这样我们先随意假设一个新空间出来，这样我们就可以进行E阶段计算，也就是把看到的点投影到这个新空间上，然后计算距离。之后我们就可以进行M阶段，也就是最小化距离，这样就做出了一个比起始新空间距离更小的空间。然后再进行E阶段，M阶段，直到距离无法缩小。说白了就是模型不存在时先人工创造一个，然后不断按你的目标迭代让模型跟数据契合。在EM算法里，我们就可以很轻松把前面的方差项也扔进去一同优化，最后也能求解。这样概率化的主成分分析就有解了。不过这个算法具体实现存在很高的技巧性，我们吃现成就可以了。同时你会发现，其实EM算法思想可以用在很多不同模型参数求解上，马尔可夫过程、贝叶斯网络、条件随机场等有隐含变量的都可以用。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;因子分析&lt;/h2&gt;
&lt;p&gt;其实在更多资料中引入概率化的主成分分析主要是为了引入因子分析，因子分析跟概率化主成分分析最大区别在于其不限制误差来自方差相同的正态分布。这当然增加了计算难度，但其实因子分析对于解释这种隐藏结构其实比主成分分析更靠谱。但是，因子分析求解上不如主成分分析容易理解，需要通过一些方法来决定因子数或干脆使用者自己决定。此外，因子分析是可以进行预测的，目标就是潜在因子。从概率角度讲主成分分析自然也可进行预测，不过你得想清楚应用场景。同时，因子分析得到的成分也是正交的，这点跟主成分分析一致。正交的优点在于映射之间不相关，但不一定独立，如果数据分布需要独立因素就需要独立成分分析。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;独立成分分析&lt;/h2&gt;
&lt;p&gt;独立成分分析在独立成分符合正态分布时其实就是主成分分析，但当你独立成分并不来自正态分布时，独立成分分析就更有优势将其反推出来。因为独立跟相关是不同的，独立在统计学里比不相关约束条件更强，不相关不一定独立但独立一定不相关，独立因素间的互信息为0或者说高阶统计量上都不相关。最经典的应用就是鸡尾酒会问题，在一个嘈杂的场景里很多人都在说话，你在固定位置放了几个麦克风，这样麦克风收集到的就是多种声音的混合，现在你需要把混音中每个人的声音提取出来。此时你要用主成分分析，你会得到所有人声音的共性，但独立成分分析就可以分辨出每个个体，或者说潜在变量，所以你也猜到了，EM算法也可以求解独立成分分析。需要注意的是独立成分分析不管降维，基本你设定分多少个就有多少个。但不论主成分分析、因子分析还是独立成分分析，本质上都是线性模型的结构，也就是所谓的主成分、因子、独立成分都是原始数据的线性组合。&lt;/p&gt;
&lt;p&gt;在我看来生物组学数据更适合独立成分分析，你可以直接从数据中提取出一组独立模块进行注释，这样可以直接去关联生物学功能而不是反过来。当然你可能会说主成分分析或因子分析也可以做啊，但你如何保证其分布假设与正交假设？不过我们还是用主成分分析来说一下，因为很多人没意识到这个功能。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;聚类／共性／压缩／降噪&lt;/h2&gt;
&lt;p&gt;有些论文用主成分分析搞聚类画圈圈来说明样品间存在内在共性。这个在环境分析中比较常见，因为环境分析通常同时测定上百种化合物，前面提到低维映射里最大程度保留了样品点的差异，此时映射到一起就有可能说明那些样品污染特征接近，便于探索来源或环境过程。实际上此时不一定需要局限在主成分分析，可以直接用聚类分析等统计模型。&lt;/p&gt;
&lt;p&gt;很多人搞不清楚特征值、特征向量还有载荷等的细节，所以主成分分析就被用成了降维画图工具，但其实这个探索分析是针对背后隐藏变量的，具体到主成分分析就是共性。还是举个例子来说吧，我有100个样品，每个样品测了1000个指标，现在我就有了个&lt;span class=&#34;math inline&#34;&gt;\(100*1000\)&lt;/span&gt;的矩阵，通过主成分分析我得到了&lt;span class=&#34;math inline&#34;&gt;\(100*250\)&lt;/span&gt;的矩阵，这个矩阵包含了原数据95%的方差。好了，现在我问你，这250个新指标是什么？对，特征向量，特征向量就是新投影方向，投影可以看作隐含共性。特征值又是什么，共性的权重，越大代表越重要，毕竟可以代表更多的方差。那么载荷又是什么，大概可以理解成原来1000个指标对250个新指标的贡献。那么进行分析时我们在样本和指标之间多了一个共性层，一方面减少了数据维度，另一方面算是提取了指标间不相关的共性（但不一定独立，切记）。对于多出来的共性层，我们同时知道样品在这些共性上的分布，也知道每个指标对共性的分布，常见的biplot就可以同时绘制样品点跟指标在两个最重要共性上的分布，一目了然。此时我们的专业知识就要上场了，我们可能会通过指标相互作用发现共性背后对应隐含因素的物理含义，也可以发现某种分离样品的共性背后暗示的样品潜在来源。总之，多了一个共性层后，我们可以研究的机理就更明显了，例如自然语言处理里可以用其寻找文本主题，基因组学里可以用来寻找基因模块等。但需要提醒的是，这个“共性”并不代表客观规律，只是一种线性变换后的结果，如果跟实际想研究的因素不对应还不如直接上回归分析。&lt;/p&gt;
&lt;p&gt;主成分分析或者说实现主成分分析的奇异值分解的另一个应用就是可以用来压缩数据，上面的例子中100*1000的数据空间如果赶上稀疏矩阵十分浪费，此时就可以用奇异值分解压缩存储空间。从信号处理的角度去看，主成分分析跟傅立叶变换等变换过程本质上都是用一组新信号替代原来信号，由于一般认为信号方差高于噪音方差，通过变换时保留主成分或特定频谱，我们同时可以做到降噪。图形处理也可以用，而所有数据其实都可以用图形展示，那么作为图形降噪的主成分分析背后跟数据降噪是可以联系到一起的，特别环境痕量分析中的降噪。结合前面的结构重构、方差保留等性质，其实哪怕你就只会主成分分析，想明白了能做的事也很多。&lt;/p&gt;
&lt;p&gt;你只是需要一点想象力。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>科研博客圈的书剑恩仇</title>
      <link>/cn/2017/09/04/sci-blog-discussion/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/09/04/sci-blog-discussion/</guid>
      <description>&lt;p&gt;有人的地方就有江湖。&lt;/p&gt;
&lt;p&gt;推动科学进步的是学术争论，大家围坐一席以数据与逻辑为工具互相质疑，寻求共识。但事实上这个过程中并不缺乏个人或群体情感的介入，这一方面是现代科研职业化所导致的拿钱吃饭，另一方面则是科研人员自身的主观好恶。这一点在科学家的博客上展示的淋漓尽致，虽然在学术期刊里发评论比较正式，但在预印本、数据共享与可重复性研究的大趋势下，越来越多的科学家选择时效性更高的非同行评议的博客来对科学进展进行评论。借助这些社交媒体，我们也可以一窥他们对学术观点的爱恨情仇，也许有人不屑于这些主观性比较强的评论，但从学术交流的角度出发，如果我们仅仅通过学术期刊与会议交流学术观点，由于存在审稿与运作周期，很多共识会消耗大量的传播成本来达成，这不仅与信息时代脱节，也会造成资源浪费。下面我们看些案例感受下国外学术界在博客这一媒介上的观点交锋：&lt;/p&gt;
&lt;p&gt;案例一：“主观”的贝叶斯方法&lt;/p&gt;
&lt;p&gt;哥伦比亚大学的 Andrew Gelman 的博客可以算得上是个火药桶了，他本身主张贝叶斯学派，而赶巧贝叶斯学派跟频率学派可以算得上科研数据分析里哲学思想差异最大的两派，起码按我的粗浅认识是根本无法调和的，所以即便实用上甚至算法上都差异不大，想对这两种思想和稀泥基本都会被 Gelman 教授无情嘲讽，如果你还打算说贝叶斯不好，基本上会被博文讨伐。当然，也不是所有人都有这个待遇，同舟子的做法类似， Gelman 教授基本也是逮着大鱼去坑。需要提醒的是他可不是舟子那种十几年不做科研的学术圈外人士，其本人是哥伦比亚大学应用统计中心的主任，其团队的研究领域十分广阔，大家可以感受一下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;why it is rational to vote; why campaign polls are so variable when elections are so predictable; why redistricting is good for democracy; reversals of death sentences; police stops in New York City, the statistical challenges of estimating small effects; the probability that your vote will be decisive; seats and votes in Congress; social network structure; arsenic in Bangladesh; radon in your basement; toxicology; medical imaging; and methods in surveys, experimental design, statistical inference, computation, and graphics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;顺带一提，著名贝叶斯统计软件 stan 就出自这个团队。&lt;/p&gt;
&lt;p&gt;这次事情的起因是卡内基·梅隆大学的 Larry Wasserman 教授（2016年当选美国国家科学院院士）在接受一个博客&lt;a href=&#34;https://errorstatistics.com/2013/12/28/wasserman-on-wasserman-update-december-28-2013/&#34;&gt;采访&lt;/a&gt;时对频率学派与贝叶斯学派下了个定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wish people were clearer about what Bayes is/is not and what frequentist inference is/is not. Bayes is the analysis of subjective beliefs but provides no frequency guarantees. Frequentist inference is about making procedures that have frequency guarantees but makes no pretense of representing anyone’s beliefs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Gelman 教授对其频率学派的观点没啥意见，但那个 “subjective” 直接引爆了火药桶。而按照 Gelman 的定义，贝叶斯方法应该是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Using inference from the posterior distribution, p(theta|y)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;特别的，他还认为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Science is always full of subjective human choices, and it’s always about studying larger questions that have an objective reality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;坦白说这个看法是比较符合科学史的，虽然当今科学理论体系逻辑上相对完备（先排除下哥德尔跟量子力学），但其发展确实很曲折，在实验数据跟统计决策成为主流之前，很多理论在发现或提出时主流科学家并不接受，有的是逻辑上不接受（很多新理论完全不容于旧理论），有的则属于威权集团打压，可以说相当主观。&lt;/p&gt;
&lt;p&gt;但在后面的论述中，Gelman 教授就开始开嘲讽技能了，Larry 认为在高维数据处理中贝叶斯方法没意义无法解释，Gelman 教授则反驳说他觉得除了贝叶斯方法别的方法也都是解释不通的，并且他认为 Larry 自己不懂贝叶斯还瞎定义是十分不妥的。不得不说这段论述很没营养，跟小学生吵架差不多。紧接着 Gelman 教授又提到主观确实是贝叶斯方法的一部分但不是全部，那频率学派是不是可以说成“简单随机采样的技术”，科学研究范围在拓展，各种方法也在发展，贝叶斯方法可以研究客观问题。这个说法也比较中肯，接下来 Gelman 教授又开启了挖坟模式，他把 Larry 08年到13年关于贝叶斯方法中随机性看法的转变给列了出来，紧接着又说我也有这个转变过程。但文章最后他又翻了 Larry 对经济学家的旧账，认为他存在个人偏见。&lt;/p&gt;
&lt;p&gt;看起来这个文章似乎比较正常，但这篇博文真正有趣的是评论，基本上集中了当今统计学中各路高手，下面是个不完全名单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nick Cox 杜伦大学 Stata 元老级开发者&lt;/li&gt;
&lt;li&gt;Larry Wasserman 卡内基·梅隆大学教授 当事人&lt;/li&gt;
&lt;li&gt;Deborah G. Mayo 宾夕法尼亚大学教授 采访 Larry 的人 errorstatistics.com 博主&lt;/li&gt;
&lt;li&gt;Kevin Dick 斯坦福毕业 创业者 possibleinsight.com 博主&lt;/li&gt;
&lt;li&gt;Judea Pearl UCLA 教授 &lt;a href=&#34;http://causality.cs.ucla.edu/blog/&#34; class=&#34;uri&#34;&gt;http://causality.cs.ucla.edu/blog/&lt;/a&gt; 博主&lt;/li&gt;
&lt;li&gt;Christian Hennig 伦敦大学学院教授&lt;/li&gt;
&lt;li&gt;Norm Matloff UC Davis 教授&lt;/li&gt;
&lt;li&gt;Brendan K O’Rourke 都柏林理工教授 &lt;a href=&#34;http://www.brendankorourke.com/&#34; class=&#34;uri&#34;&gt;http://www.brendankorourke.com/&lt;/a&gt; 博主&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以看出这么几件事：首先，这些领域内专家会互相关注对方的个人网站并通过这种方式互动；其次，看他们的讨论很有启发，必看书本上的干货更有意思；再次，很多讨论虽然对问题是没营养的，但有助于我们了解一些学术界的风格或流派。在前沿领域由于知识不全，多数情况是无法达成共识的，但通过了解其流派风格会帮助你更全面的看问题。&lt;/p&gt;
&lt;p&gt;案例二：两个软件会产生一个结果吗？&lt;/p&gt;
&lt;p&gt;巴拉巴西是一位呼声很高的诺奖候选人，其畅销书《链接》可以说把不少科研人员吸引到了网络科学的研究领域，现实中的无尺度网络的幂律分布所具有的奇特性质在很多并不相关的领域都有展示。但这个故事主角不是他，提到他只是想提前表示下同情，因为他在加州理工教授 Lior Pachter 的博客里躺枪了。事实上，2014年 Lior Pachter 在博客上开了个三部曲，本意就是对 MIT 教授 Manolis Kellis 的个人恩怨，但为了把故事讲的通透点，这位老兄追根溯源并展示了自己强大的数理功底，先后对两篇发表文章的创新性进行质疑，从图表到算法，其中一篇就是巴拉巴西的，另一篇则是 Manolis Kellis 教授的。这个故事科学网薛宇老师曾经翻译评论过，我这里不细讲。但 Lior Pachter 教授在后续的博文中又对号称H指数100多的巴拉巴西来了次二次扒皮，严格说被炮轰的其实不算冤枉，但被人挂的如此直白也只有 Lior Pachter 教授能做得出来。而我今天要讲的是他最近又跟纽约大学石溪分校&amp;amp;哈佛&amp;amp;卡内基·梅隆大学的同行掐架了，上演了进攻-防守-再进攻的三部曲。&lt;/p&gt;
&lt;p&gt;首先 Lior 讲在他们那个 RNA 测序定量的圈子里，软件跟软件差异都是很大的，基本你用不同软件想得到一样的结果非常困难（这也说明这个领域的研究共识没有达成）。然后他话锋一转，说自己组里2016开发的一个软件跟最近发表在 Nature Methods 上的软件处理结果却出奇的一致，皮尔逊相关系数三个九，然后又是一通追根溯源。这里岔开说一句，Lior 之所以可以追根溯源，是因为预印本及版本控制系统的流行，最近 ACS 也对化学领域提供了预印本服务，预计不久就会覆盖绝大多数涉及数据分析的实验学科。从版本上 Lior 发现在他们论文发表后 Rob Patro 的软件也有了一个很大的更新，更新前跟他们组软件差异明显，更新后确几乎一样了，最后他认为 Rob Patro 所发表的文章实际上就是抄了自己组里开发软件的思想，然后加了个矫正。当然 Rob Patro 也很快在 github 上发表了一个回应，大意是他们在文章跟源码中多次引用了 Lior 组的论文并且在有些数据集中这两个软件的结果是不一样的，工作流程也不一样。但 Lior 教授显然并不满意，他又写了一篇博文指出其回复混淆视听，所谓的不一样是下游分析，而在 RNA 定量上这两个差距还是很小，如果你去看这篇回复会发现 Lior 甚至使用了动画来展示两者区别很小，可谓精心准备。我在读这三篇文章时学到很多的论述方法与追踪验证方法，可以说很多方法现在还没出现在教科书中，但可以感到早晚会形成趋势。&lt;/p&gt;
&lt;p&gt;凭心而论， Rob Patro 的文章就算是对 Lior 软件的改进也是值得发表的，因为当前科研基本都是N+1模式，都是在前人基础上做功课。但我也比较理解 Lior 为什么这么火大，首先在他眼里这两个软件本来就是一回事，凭什么发 Nature Methods，他自己那篇都没发这么好，另一方面就是 Rob Patro 文章在他看来有硬伤，速度也不快，效果也没那么好，评价标准还有问题。其实说白了也有点个人恩怨而不是就事论事，但在这些问题上你去要求当事人一碗水端平也很难。如同第一个案例所言，科学研究就是会掺杂各种主观情感，但作为旁观者，我们可以从中去学习他们讨论问题的方法，例如 Lior Pachter 教授的论证过程，虽然不如发表文章里那么逻辑完备，但思考步骤都是比较清晰的，而这个过程你在期刊论文中往往看不到，好比你看到的总是对方站在山顶但怎么爬上去的一般都不会写，但有时候这些看似琐碎的步骤却足够让你永远达不到那个高度。&lt;/p&gt;
&lt;p&gt;顺带一提， Lior Pachter 教授的博客上还友情链接了 Andrew Gelman 教授的博客。我想说的是在国外是真真切切存在着通过博客的学术交流的，参与学者的水平也是相当强悍，而且不同于国内科研向公众号或博客满足于对论文的解读，这些博客上更多出现的是一种批判式讨论，而且夹杂了相当重的个人情绪，如果你打算阅读也是需要辨伪存真的，这本身对于提高科研思维也有帮助，所以我推荐高年级本科生、研究生跟科研一线的学者都可以去寻找自己感兴趣领域大牛的博客，省的每次找推荐审稿人都搞近亲繁殖，如果你能从这些火药桶博客里获得正面评价，那么恭喜你，科研对你并不是个坑。&lt;/p&gt;
&lt;p&gt;其实类似的故事还有很多，你可以从这篇文章里出发用关键词去探索。我在前面的文本分析的文章中曾提到越是高端的论文，发表勘误的比率就越是很高，这说明前沿领域的研究不确定性是很高的，思想碰撞也很激烈。如果把社交媒体上的各类花式吐槽也算进去，你会发现科研领域有很多烧脑的故事，各路参与者也从来都不缺名校光环跟牛文加持，阴谋诡计、解释掩饰、爱恨情仇等可能被小心翼翼地埋藏在数据与图表之中，虽然看懂需要比较高的门槛，但也正是这种门槛屏蔽了围观群众，上演一幕幕精彩绝伦但需要自行判断的书剑恩仇。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>唐提保险、区块链与个性化</title>
      <link>/cn/2017/08/07/tontine-insurance/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/08/07/tontine-insurance/</guid>
      <description>&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;唐提保险&lt;/h2&gt;
&lt;p&gt;在纽交所还没成立时，活跃在华尔街的经纪人业务并不像今天这样职业化，他们基本接受各种代理业务，其中有一种名为唐提的养老保险特别受欢迎，规则简单到一句话就可以说清：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有参与者投资，一段时间后幸存者获取所有资金的收益。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;举例而言，10个人每人出1000块，约定10年保险期然后把10000块交给投资人打理，许诺收益率6%，这样每年投资人会拿回600块利息，然后刚开始是10个人分每人60块，到了第五年，有4个人过世，投资人收益将由幸存者分，也就是每人100块。当然也可以不约定保险期，直到最后一个人过世为止，在那之前最后的幸存者独享每年600块的收益。至于本金可以在约定时间后留给死者后代或也留给那个幸存者，如果持续将后代也算进来那么理论上这种保险可以持续运行很久，直到某个家族完全消失，然后幸存家族的份额会得到提升。&lt;/p&gt;
&lt;p&gt;这种保险形式非常特别，因为买这个保险完全不需要精算群体死亡率，只要看懂规则愿意加入就可以，甚至完全不需要保险公司来中介。从某种意义上，这是一种混合了分红与赌博的产品，只要你赌自己可以活的长就行了，保险成本的负担从整体转嫁给了个人。事实上美国就有类似的教育储蓄产品，家长拿出一大笔钱放到银行一年后转回本金，然后大量本金产生的利息将作为子女完成高中教育后上大学的分红，但只有考上大学的人才能参与这笔分红。对每个人而言，整体入学率或死亡率都是没有太大意义的，只要你敢赌自己考得上或能活下来，那么就可以参与这个博弈。作为博弈的管理者也毫无精算风险，只要专注于许诺的收益率就可以了，不用担心投保人大量报案导致的高风险。其实诸如意外险、财产险甚至运费险背后都有这种逻辑，那就是让受害者而不是幸存者收获非受害者的本金。&lt;/p&gt;
&lt;p&gt;从历史上看，最早提出概念的Lorenzo de Tonti 是建议用这种方法来为法王路易十四筹集军费，后来英国也采用过这个方法，他们给出的年利率都不低于10%。但100年后国家层面的唐提保险就消失了，不是因为幸存者之间互相残杀引发的道德问题，而是参与这个保险的人平均幸存率太高，结果国家层面许诺的利率又达不到，所以就维系不下去了。其实这些国家本来是给不出10%的利率的，但如果参与者不断有人离世，那么这个利率就可以维持，结果倒霉的是参与者似乎都很长寿（有些是通过欺诈），然后仅是维持许诺利率都让国家不堪重负了，最后无奈把这个保险转成了年金制。后来这个保险跑到了美国成了一种退休金并开始流行，到上世纪初，这种保险的规模甚至达到了美国总财富的7.5%。事实上，当时4%年收益的唐提保险实际上可以对投资者产生10%的收益。不过，到1904年这种形式就被禁止了，主要问题是保险公司利用这一制度短期聚集了大量资金并且20年内不交代用途只考虑分红，同时就是投资者死亡后面临本金无法赎回的道德问题。所有的参与人都希望自己活得长的另一个说法就是所有参与人都希望别人活的短，这甚至给美国文学作品创作提供了灵感。&lt;/p&gt;
&lt;p&gt;事实上，直接形式的唐提保险虽然被禁止，但间接形式的唐提保险一直存在，在以色列集体农场中，一直都有农场共同基金是采用唐提保险形式的，这样作为集体的财产可以一直保留在集体中而不是伴随遗产外流。但最近几年，美国又有人提出唐提保险可能是一种很好的养老金形式，促成这个想法的技术趋势就是区块链。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;区块链&lt;/h2&gt;
&lt;p&gt;区块链算得上一个被IT行业炒烂的概念，跟它一组的还有：共享经济、大数据还有知识付费。我最近读了下中本聪那篇论文，说实话被震撼到了，并不是说看好比特币投机而是看好这个技术的巧妙设计，用同样的构架可以做很多事。下面我就大概说下核心概念：&lt;/p&gt;
&lt;p&gt;如果我打算从市场买一个产品，付款后得到产品，然后作为媒介的钱究竟怎么来怎么去我并不感兴趣。但现在我告诉你存在一个公开的账本，所有交易的细节都可以广播后从所有人的账本上查到，这样你可以追踪你的货币到其铸造出的那一刻，而货币的铸造则是一个对大量交易打包封装成区块过程的验证奖金，最先验证成功的区块发出广播，所有的账本都会加入这个区块。也就是说，这是一个分布式造就的整体权威而非中心化的，如果活跃账本中多数先收到验证成功的广播，那么这就是主链区块。同时，基于数学特性，几乎不可能有人能小范围打包多个区块来声明更多的货币，因为这样即便延长了小范围的区块链但也不能被其他人认可，后果就是那个链成为废链。同时，所有交易是基于地址的，而这个地址根本不需要权威验证身份，也就是同时做到了匿名与公开，所有人从公共账本里看到的都是地址间的交易，但地址属于谁是无法得知的，因为这样的地址可以无限生成且互相交易，一换手就没办法查了。这也就是为什么今年上半年勒索病毒要求比特币作为赎金，我们也能基于地址查到多少人交了赎金，但就是不知道地址主人是谁。&lt;/p&gt;
&lt;p&gt;那么这跟唐提保险有什么关系呢？唐提保险当初被人诟病很大程度是因为这可能是个熟人间的保险，大家相互认可才签协议，举例而言你不想跟个婴儿签共同的唐提保险，其大概率会成为最终的幸存者，但如果大家相互认识就存在道德问题了，那就是有些人可通过谋杀另一些人来获利。但如果用区块链技术，我们可以匿名参与唐提保险，而由于你是一个区块链用户，我们可以追踪你在区块链上的活动来确认你是否符合参与保险的条件，例如在区块链设计中加入每天步行数上传或心率数据上传的设计，那么只要你活在可追踪的区块链中，唐提保险就会不断分红直到你的区块链活跃度为零。这样道德风险就几乎为零了，因为所有人都不知道谁在参与。同时唐提保险可以设计为无数条链，每条链人数固定，只有同一条链上的人可以凭借自己特有的地址不断收到作为分红的代币，只要保险公司或代币市场可以将其转为法币，那么这个游戏就可以悄无声息的运转，即使你知道别人也在游戏中但因为可能不在一条链上而没有利益冲突。其实这个机制可以很好的回答另一个问题：大数据下的个人价值。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;个性化&lt;/h2&gt;
&lt;p&gt;如果你的行为数据可以被收集用来预测寿命，一个生活习惯良好无不良嗜好的人基本不需要买保险或价格很低，留个遗嘱就可以了；而那些抽烟且有酒驾记录的人想买也会因为价格过高而无法负担。这其实对行业是不利的，保险行业里想买的需要那些不需要买的来共同承担风险，但如果我通过自测发现不需要买，那我很有可能就不加入，结果保险保的都成了高风险的人，最后也会入不敷出。技术上其实越来越允许数据持有者了解并预测自己的行为，巴拉巴西在《爆发》中提到，93%的人类行为可以预测，配合精准医疗技术，未来很多人可能很轻易的知晓自己的行为与最佳生存模式，这种情况下的危机就是人不会主动参与共同抵御危机。举例而言，如果你不可能患罕见病就无所谓购买相关保险，但罕见病患者则可能成为商业规则下被忘记的一批人，无利可图所以没有人关心，市场就是这样。越是精准的营销策略其实越不需要营销，因为你想让人买而那个人其实本来就想买，找到人就OK了。但这事实上会造成不平等，你的价格可能是量身定做的，但往往却是无法负担的。这样的不平等在我看来是可以通过技术来解决的。&lt;/p&gt;
&lt;p&gt;在数据时代没有隐私，但隐私的价值却是变化的。如果存在一种算法，可以综合个人各种风险就可能解决信息不平等。举例而言，A喜欢极限运动但身体健康，B生活规律但带有癌症基因，也就是说A需要意外险不需要重疾险，B需要重疾险而不需要意外险，如果两种保险去卖估计都只能卖出去一份且价格偏高，但是如果A跟B是一组去买两人都可以得到保险且由于卖出两份边际成本会下降则价格不高。如果把一个人一生的动态风险不断打包整合到一个区块链上，综合风险类似的人出现在同一条链上而且不同区块链可以动态转接进行风险控制，保险公司收费后只要在不同风险的链上定期注资，概率上一定有人收益，这样所有人可能都乐于加入这样一个计划，只要你不是一个全面极端的人，在一点上的极端是可以被另外多点的不极端而中和掉。运行这样一个区块链系统同样并不需要实名，只要你乐意加入就够了。你总是会不断出现在跟你综合风险类似的人的链条上，由于你并不清楚对方实际哪方面有风险，实际我们可以引入唐提保险机制，这样可以保证参与度与整体系统的稳定性。这个机制的优点在于每个人即使对自己一清二楚也总能找到一个跟自己类似但细节完全不同的人进行生存游戏，每个人在算法公平下其实也实现了共同保险，而且算法够强的话没有人知道你具体是谁。&lt;/p&gt;
&lt;p&gt;本着个人谋求私利的心降低整体风险，这是对算法技术的新要求，没必要寄托在慢半拍的政治、经济、法规跟伦理上，新的技术与博弈机制可能最终会解决社会风险问题。而个人隐私其实也成了未来每个人的生存筹码，毕竟天天在家里宅着看视频就不太可能被毒蛇咬到，那就找个野外科考队员来分担你II型糖尿病的风险吧，虽然你的数据被泄漏了，但也从某种方面证明了其价值，当这一切被配对整合后，大家都应该可以过自己想要的生活而不用过多担心风险。&lt;/p&gt;
&lt;p&gt;技术产生的问题还是要技术来解决，反正魔盒已经开启。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;参考文献&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/gp/product/B00WMRPWVG/ref=as_li_ss_tl?ie=UTF8&amp;amp;btkr=1&amp;amp;linkCode=sl1&amp;amp;tag=kitcescom-20&amp;amp;linkId=0d2632884eb0eff4608787c0f4f3d660&#34;&gt;King William’s Tontine&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://bitcoin.org/bitcoin.pdf&#34;&gt;Bitcoin: A Peer-to-Peer Electronic Cash System&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kitces.com/blog/tontine-agreement-instead-of-annuity-lifetime-income-mortality-credits-and-book-review-of-milevsky-king-williams-tontine/&#34;&gt;Could A Tontine Be Superior To Today’s Lifetime Annuity Income Products?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2017/03/24/business/retirement/tontines-retirement-annuity.html?_r=0&#34;&gt;When Others Die, Tontine Investors Win&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>从真空里的球形鸡到社会财富分配（二）</title>
      <link>/cn/2017/07/29/%E4%BB%8E%E7%9C%9F%E7%A9%BA%E9%87%8C%E7%9A%84%E7%90%83%E5%BD%A2%E9%B8%A1%E5%88%B0%E7%A4%BE%E4%BC%9A%E8%B4%A2%E5%AF%8C%E5%88%86%E9%85%8D%E4%BA%8C/</link>
      <pubDate>Sat, 29 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/07/29/%E4%BB%8E%E7%9C%9F%E7%A9%BA%E9%87%8C%E7%9A%84%E7%90%83%E5%BD%A2%E9%B8%A1%E5%88%B0%E7%A4%BE%E4%BC%9A%E8%B4%A2%E5%AF%8C%E5%88%86%E9%85%8D%E4%BA%8C/</guid>
      <description>&lt;p&gt;前面说到在一个理想自发交易的稳态环境下，社会财富的分配会符合小球模型里的玻尔兹曼分布：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E) = A e^{-E/E_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在这里，&lt;span class=&#34;math inline&#34;&gt;\(E_c\)&lt;/span&gt; 代表了温度，可以对应经济系统里总体财富的平均水平；&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; 代表了个人财富水平。这是个描述概率的指数函数，个人财富越多，越不可能出现。不过其实更多人熟悉的是财富分配的二八法则，也就是帕累托-齐普夫定律，那么这个定律又是什么鬼？跟玻尔兹曼分布又有什么区别呢？&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;二八法则&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>从真空里的球形鸡到社会财富分配（一）</title>
      <link>/cn/2017/07/24/boltzmann-distribution-1/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/07/24/boltzmann-distribution-1/</guid>
      <description>&lt;p&gt;曾经有个笑话吐槽物理学家研究鸡不下蛋问题时首先假设了一只存在真空中的球形鸡，进而认为理工类研究过于简化了现实世界，再进而推导出了一种不修边幅、不谙人情世故且高智商低情商的理工科书呆子刻板印象。就我个人生活经验而言，这种看法倒也没啥问题，对，就是没啥问题，你没看错。&lt;/p&gt;
&lt;p&gt;但是（说“但是”前的都是废话–马克·于瘟），当今学术研究的一个大趋势就是用理工科的思维研究社会自发行为，我曾经尝试&lt;a href=&#34;http://blog.sciencenet.cn/blog-430956-869450.html&#34;&gt;用马尔可夫过程解释社会阶层流动&lt;/a&gt;，但其实很多理工科出身的科学家是很严肃地研究这类问题的。我们将会看到一个松散假设下的物理模型是可以解释很多（但绝不是全部）自发的宏观社会行为的。在某些视角下，我们也许就是个真空中的球状社会信息综合体。首先，我们从具象到抽象，来了解下一个统计物理学的概念——玻尔兹曼分布。在后续的文章中我们会用这个分布来探索下社会行为，特别是经济行为。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为的抽象&lt;/h2&gt;
&lt;p&gt;社会行为其实就是个体间的相互作用，在物理学视角下，这个个体间相互作用有四种（但我知道多数人不关心这个），简单说就是物质能量交换。而社会经济行为视角下，这个相互作用可以看成价值流动，通俗一点就是钱。钱可以在流通成本很低的现代信息社会下快速在社会个体间转移，在物理学视角下可看作能量在不同粒子间的传递。那么类比一下我们会发现，如果我想知道一个社会整体的财富分配，我们可以类比到一个装满带有能量的小球的空间去观察小球的能量分布。这里有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;社会个体（人、公司、团体）-&amp;gt; 空间里的小球&lt;/li&gt;
&lt;li&gt;财富（钱、资产） -&amp;gt; 能量&lt;/li&gt;
&lt;li&gt;价值交换（买卖行为、资本流动）-&amp;gt; 小球间能量交换&lt;/li&gt;
&lt;li&gt;社会个体的财富分配 -&amp;gt; 空间里的能量分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里我们需要一些假设，首先我们考虑的是一个热力学平衡，也就是说，空间里能量总和一定，也就是要是个封闭空间，同样，社会里总财富在某个时间点也是稳定的。其次，我们想知道的是财富分布或能量分布，那么我们就要有不同财富区间与能量区间来构成分布，这里我们用钱数或能量值来划分，拥有钱数或能量在同一区间的社会个体或小球在分布中是一样的，无法区分，也是等概率的。最后，能量交换或价值交换要达到热力学平衡，这个在小球模型里很容易实现，但社会实际状态只能近似认为达到，这意味着交易是随机的且充分的，后续我们会修改这个限制让模型更符合社会实际。&lt;/p&gt;
&lt;p&gt;根据上面的类比与假设，目前我们大概可以用一堆小球去模拟一些稳态社会行为了，这个抽象过程是大多数理工类科学研究的起点，不然一上来就过于复杂很难讨论。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;封闭空间里的赋能小球&lt;/h2&gt;
&lt;p&gt;好了，目前我们先不去想复杂的社会行为来考虑小球。在一个封闭空间里，小球的能量总和是固定的，但能量范围却可以很广，小球数也可以很多，那么我们再简化下，就考虑3个球总能量6，能量区间就会有7个离散数值：0，1，2，3，4，5，6，7。那么我们想得到的是宏观状态下能量分布，我们首先来个原始方法：数。&lt;/p&gt;
&lt;p&gt;首先要数下有多少个宏观态。例如有一个球上有6的能量，其余的球能量都只能是0。我们可以列出下面的7种宏观态：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Macrostate.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;然后我们考虑每一种宏观态下有多少种微观态，因为球有3个，区间有7个，我们可以看成一个排列组合问题。微观下，一共有&lt;span class=&#34;math inline&#34;&gt;\(3!\)&lt;/span&gt;种排列组合，但在基本假设中，我们认为属于同一宏观状态的球是一种，那么这些情况就要被排除掉。例如在宏观态7中，虽然球可以有6种排列，但因为都在1个能量区间，实际只有1种微观态。也就是说，如果我们有N个球，排列组合上看虽然有&lt;span class=&#34;math inline&#34;&gt;\(N!\)&lt;/span&gt;种组合，但实际由于N个球中有n个球在区间i，我们要从总的排列组合中除掉这种组合。也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\# microstates = \frac{N!}{n_0!n_1!...n_i!}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这样我们可以得到共计28种微观态，在这里所有微观态的出现概率是一样的：&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 2: &lt;/span&gt;State.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Microstate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Macrostate6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Macrostate7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;total&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在宏观态1中所有微观态出现的可能性为&lt;span class=&#34;math inline&#34;&gt;\(3/28\)&lt;/span&gt;，其中针对能量为0的区间贡献&lt;span class=&#34;math inline&#34;&gt;\(2*3/28 = 0.214\)&lt;/span&gt; 个小球，而宏观态2中对能量为0的也贡献&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球，同样宏观态3中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.214\)&lt;/span&gt;个小球而宏观态4中贡献为&lt;span class=&#34;math inline&#34;&gt;\(0.107\)&lt;/span&gt;个小球，这样会有&lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;个小球贡献给能量为0的区间，同样我们可以得到其他能量区间上有多少小球。这个小球的能量分布就是我们打算求的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-07-24-boltzmann-distribution-1_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在这个演示里，能量分布似乎跟数目是线性的，但是如果我们把系统放宽，加入更多的球跟能量，这条曲线应该是指数分布的且指数为负。当然我知道你们不信，这里我们用 Eisberg &amp;amp; Resnick 在量子物理教材中描述来进行一个理论推导。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;能量分布的数学形式&lt;/h2&gt;
&lt;p&gt;如果我们看到两个小球，这两个小球来自同一个能量分布，那么一个小球其出现在&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_1)\)&lt;/span&gt;，出现在&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;能量区间上的概率为&lt;span class=&#34;math inline&#34;&gt;\(f(E_2)\)&lt;/span&gt;。我们考虑一个特殊情况，一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;，发生这种情况的概率应该是两个概率的乘积，也就是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1) \times f(E_2) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后，我们考虑所有宏观态中能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况，因为我们进行了区间划分，所以能量为&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;的情况只出现在所有能量和为这个数的小球组合里。由于我们之前假设了所有微观态是等概率的，那么出现这个能量和的小球组合方式也是等概率的。那么在某种程度上，会有下面的公式成立：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E_1)+f(E_2) = h(E_1+E_2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们可以认为上面那个特殊情况，也就是一个小球来自&lt;span class=&#34;math inline&#34;&gt;\(E_1\)&lt;/span&gt;而另一个来自&lt;span class=&#34;math inline&#34;&gt;\(E_2\)&lt;/span&gt;的概率是属于&lt;span class=&#34;math inline&#34;&gt;\(E_1 + E_2\)&lt;/span&gt;这个能量分布的，那么根据第一段的推理出现这个情况时的概率应该是乘积。也就是两个能量和的出现概率既是两个独立能量区间概率的和又是乘积，那么满足这个条件的数学形式只能是指数形式。那么我们可以得出，在某个能量区间上小球的概率密度函数应该是：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(E) = A e^{-E/E_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里面的常数&lt;span class=&#34;math inline&#34;&gt;\(E_c\)&lt;/span&gt;是描述独立于小球的系统变量，玻尔兹曼经过推导得出这一部分是系统温度（总能量）的线性函数，所以我们就有了玻尔兹曼分布的数学形式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_b(E) = A e^{-E/kT}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这个分布描述了在封闭系统内不同能量的小球概率分布形式，简单说就是能量高的概率很低，而能量低的概率比较高。这个现象是客观存在的，但同时由于存在量子效应限制，有些能量段上的小球是无法稳定存在的，所以能谱分布实际并不连续，不过这就是另一个故事了。那么这个符合粒子运动的物理模型跟社会行为有什么联系呢？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;社会行为&lt;/h2&gt;
&lt;p&gt;在一个达到平衡的社会经济系统中，我们对小球的假设基本是符合现实的，能量代表财富，那么很直观的现象就是这个社会的财富分配天然就不是平均的，会基本符合指数分布。也就是说，总是少数人富有而多数人穷。那么有人会反对说发达国家都是纺锤形的，明显模型有问题。没错，发达国家可以是纺锤的，但如果把全球看成一个缓慢增长的经济体，一个区域特别稳定富有只可能是牺牲其他区域的财富增长才能实现。所谓共同富裕，在全球封闭尺度下是无法自发形成的。这里看起来跟自由市场理论有矛盾，在自由市场下，看不见的手可以调节保证一个健康的经济发展，但在小球模型里经济并不发展，只是自由分配与交易。不过换句话，在经济停滞后的交易行为应该会导致更极端的财富分配。至于说自由市场里的自发交易可以促进合理的财富分配，我觉得比较扯，更靠谱是富有者所承担的社会文化压力，要不然亚当·斯密也不至于再去写一本《道德情操论》。&lt;/p&gt;
&lt;p&gt;此外，请注意关键词：自发。也就是说不进行法律政策干预，但是如果博弈规则发生改变，那么物理系统就不那么好使了。另一个关键词是稳态，也就是如果社会整体财富增加了，那么即使分布不变，所有人生活的幸福指数也会提高。而增加社会整体财富的方式在最近这一百年的主要表现方式是科技革新，有意思的是，目前可以让财富分配更不平等的最主要动因也是科技的不平等。&lt;/p&gt;
&lt;p&gt;在接下来的两篇中，我们先看看到底这个世界的财富在不同的小球模型中会有如何的分布特征，然后再看看社会财富实际是如何分配的而物理学规律能给我们什么样的启发。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real high-throughput for LC/GC-MS</title>
      <link>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</guid>
      <description>&lt;p&gt;If someone want to know whether some compounds exist in certain samples. He always need to make pretreatment for that samples and make them into a little vial for analysis in sophisticated instruments like mass spectrum. If you want to analysis many components in one sample, you also need some separation methods like gas/liquid chromatography. How about analysis multiple samples and multiple compounds in a single run?&lt;/p&gt;
&lt;div id=&#34;pseudo-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pseudo high-throughput&lt;/h2&gt;
&lt;p&gt;If you use LC/GC-MS to analysis samples, all the efforts for high-throughput would be limited at the injection step. You could arrange 96-well plate for analysis. But wait, ONE BY ONE. If some short-live compound could survive in the pre-treatment, they would disappear in the auto-sampler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-mass-spectrum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to Mass Spectrum&lt;/h2&gt;
&lt;p&gt;Actually, similar issue happened when we perform Multiple Reaction Monitoring(MRM) to collect the intensities from different ions. If the detector could only measure one ion at one time, mass spectrum simply use a high frequency scan like 50ms or 20ms for one ion and re-construct the time profile by smooth the points into a line. To my knowledge, 15 points would fit a bell curve well for one peak with smooth. OK, if the peak width is 15s, for each ion we only need 1 scan per second as shown below. OK, that means if our instrument could reach 10ms per scan per ion, we could monitor 100 ions at the same time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-09-real-high-throughput-for-mass-spectrum_files/figure-html/sim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then how about full scan, some detectors like orbitrap and tof-tof could monitor a full scan for all ions at almost the same time. Common high resolution mass spectrum could collect more than 10 spectrum per second. If the compounds could show peaks’ width larger than 15s, we could actually collect 10 spectrum from different samples, which is the real high-throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real high-throughput&lt;/h2&gt;
&lt;p&gt;All we need to do is to synchronize the injection and mass spectrum scan. I have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this solution, we injected 6 samples at the same time. The column should be the same. Then when the samples reach MS part, they were arranged into one sequence. All we need to do is to ensure every six full scan on the mass spectrum could meet six identity sample from the LC/GC part. Then MS could collect and rebuilt six samples’ retention time - mass profile and output six data set for those samples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with Multiple channels&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;\images\htoption2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this way, we need some controls on pumps to on/off when one channel’s sample get into MS for full scan. Or we could have cells to guide the samples into the slit before the lens. This option is better to avoid the cross contamination. However, we need re-design the MS ion source for multiple channels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single column with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you do not need to modify your current instruments. All you need to do is the injection of the samples by sequence without considering the full separation in former sample. However, you need a lot of efforts to deconvolution. Since it’s the same column, the separation for most ions should be same. You could build a model to capture such patterns and separate the samples by those patterns. In such way, the batch effects could be minimized. However, the requirements for data mining are maximization. I like this way.&lt;/p&gt;
&lt;p&gt;I think in the near future we would find the real high-throughput LC/GC-MS. Those devices would short the analysis time between sample collection and data acquisition. MS-based scientists could reach more interesting findings with REAL high-throughput.&lt;/p&gt;
&lt;p&gt;Happy explore!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text mining for top academic journals</title>
      <link>/en/2017/07/07/text-mining/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/07/07/text-mining/</guid>
      <description>&lt;p&gt;Text mining is often used to find spatiotemporal trends in news, government report and user generated contents in SNS websites. We could make sentiment analysis to find the real opinions of netizens. Also we could track the popularity of certain phases and find the connections among them. Such technique might also be useful for scientists or researchers to read papers.&lt;/p&gt;
&lt;p&gt;Scientists or researchers’ daily life is immersed by a lot of literature. Most of the them are only focused on limited area in certain subjects. However, a modern scientist should always know what had happened in all of the other subjects. Some techniques used in other research might inspire your research. The only problem is that you need a lot time reading top general journals like Science, Nature and PNAS. Wait, we actually do not need to know the technique details and all we want to know is the patterns in those journals. Well, text mining would help.&lt;/p&gt;
&lt;p&gt;The main advantage for text mining in academic journals is that academic papers always share same structures in one journals. Public academic databases such as PubMed or Google scholar could always show you the structured records for papers such as journal, authors, title, published dates and even abstracts. We could directly fetch those data and save in database. I developed scifetch package to get those data from PubMed. This package would support Google scholar, bing academic and baidu xueshu in the future. Actually I support PubMed in the first version because they have a user-friendly API and I could connect such pipe with xml2 package in a tidyverse way. Besides, easyPubMed package is also a good package to extract such data from PubMed.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data collection&lt;/h2&gt;
&lt;p&gt;Here I collected the information from all the papers published in SNP i.e. science, nature and PNAS in the past three years as xml format and clean them into a dataframe for further text mining. The search grammar could be find from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK3827/&#34;&gt;NCBI websites&lt;/a&gt; and a cheat sheet here.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/cheatsheetpubmed.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;There are 26559 papers and I will use such data for text mining. PubMed has a limitation for 10000 records per query. So we need to fetch the data multiple times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 26,557 x 7
##                          journal
##                            &amp;lt;chr&amp;gt;
##  1                        Nature
##  2                        Nature
##  3 Proc. Natl. Acad. Sci. U.S.A.
##  4 Proc. Natl. Acad. Sci. U.S.A.
##  5                       Science
##  6                       Science
##  7                       Science
##  8                       Science
##  9                       Science
## 10                       Science
## # ... with 26,547 more rows, and 6 more variables: title &amp;lt;chr&amp;gt;,
## #   date &amp;lt;date&amp;gt;, abstract &amp;lt;chr&amp;gt;, line &amp;lt;int&amp;gt;, time &amp;lt;dttm&amp;gt;, month &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;description-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description statistics&lt;/h2&gt;
&lt;p&gt;Firstly, let’s see the papers by journal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we could check the high frequency terms in the title and abstract of these papers.&lt;/p&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordtitle-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As top journals, one of the most obvious features is that they all need correction and reply. With high influences, those journals would be the best place to discuss the leading edge techniques and findings. However, Science likes U.S., glance and paleoanthropology more while Nature and PNAS like tumor to be used in the titles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordabs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, when we focused on the abstracts, something interesting happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They all focused on tumor while Nature use ‘tumour’ as a journal from U.K.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nature’s title and abstracts look similar while Science’s title always use different terms compared with their abstracts. Maybe Science’s authors like to be clickbaits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PNAS’s authors use a lot of abbreviation in their abstracts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Those top journals all like tumor, behavior and modeling and now you know how to pick up a topic to be published.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;temporal-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Temporal trends&lt;/h2&gt;
&lt;p&gt;Here we use logistic regression to examine whether the frequency of each word is increasing or decreasing over time. Every term will then have a growth rate associated with it.&lt;/p&gt;
&lt;div id=&#34;title-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find some trending terms like CRISPR, gut, and corrigendum are ‘promising’. However, some topics like Ebola, Hiv and cell differentiation would leave us. Another interesting trending is that the names of certain subjects is disappearing in those top journals like biology, chemistry, neuroscience, ecology and policy. Maybe most titles would like to focus on specific topics or certain problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Now let’s review the temporal trends of terms in abstracts during the past three years by months.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to find a clear pattern in growing words in abstracts. Maybe the abstracts focused more on technique details. However, shrinking words like viral, gene expression and pathway showed clear trends. Meanwhile, we could find some words like suggests, previously and production are discarded by the top scientist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-among-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship among words&lt;/h2&gt;
&lt;p&gt;N-gram analysis could be used to find a meaningful terms in those papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/bigramt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;./images/biabs.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Well, climate change, transcription factor, stem cell and cancer would always be the favorite bigrams in the titles of top journals. For the abstracts, cell related topics such as function, protein and expression are always preferred. Anyway, life science is always the center of trending sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic modeling&lt;/h2&gt;
&lt;p&gt;Relationships among words could show us some trending. However, we could employ topic modeling to explore the topics as a bunch of words in the abstracts of those top journals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./images/tm.png&#34; /&gt; This topic model showed topics like climate change, virus infection, brain science and social science are other important research topics other than life science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment analysis&lt;/h2&gt;
&lt;p&gt;Text mining could also be used to find the sentiment behind those journal papers or the customs using certain words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/SA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, I think the afinn word list for sentiment analysis is not suitable for scientific literature. Some words is actually neutral in academic journals. If someone could develop a specific word list for scientists, we might find something ignored by the writing lessons on campus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here, I demo some basic text mining results for top academic journals. Just like Google trends might predict the popularity of flu, text mining for academic journal might be a good tool to reveal unknown patterns or trends in certain subjects or top journals. Besides, we could also find unique usages of some words and some tones behind the journal. Besides, such methods might work better than impact factor or H index as the evaluation tools for certain researcher, journal or institute in a dynamic view. The most attractive thing is that every scientist could use this tools through open databases and find their own answers. This is the best benefits from information era.&lt;/p&gt;
&lt;p&gt;You might read this excellent on-line &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;book&lt;/a&gt; and David Robinson’s &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;blog&lt;/a&gt; to make more findings.&lt;/p&gt;
&lt;p&gt;All the R code for this post could be found &lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/en/2017-07-07-text-mining-for-trends-in-top-journal.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>文献阅读的文本分析流派</title>
      <link>/cn/2017/06/16/nlp-literature/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/cn/2017/06/16/nlp-literature/</guid>
      <description>&lt;p&gt;读文献是科研人员的基本功，一方面是了解学科发展，另一方面更现实一点，就是为了发文章。起步阶段读论文一般是模仿与学习，但到了中后期如果你的视野不够开阔，很容易陷入到安全区陷阱，认为自己做自己那一小摊就挺好，其实很有可能大浪过来，全军覆没，说直白点就是申不到钱，课题与项目运转不下去，思路也会枯竭。当你去开学术会议时，那些大会报告的报告人的开场总有个全局概览的视野，这种评论是需要经验去堆的，但其实也挺虚的：你回头去看容易知道哪里有坑哪里有丘，但身处时代浪潮之中是不太容易感知趋势的。&lt;/p&gt;
&lt;p&gt;但传统基于核心关键词的检索跟全局观是本质相悖的，核心关键词往往限制了内容，虽然有利于聚焦但不利于发散与概览。不过当前文献数据空前开放，如果你有类似全局视野问题，是可以自己探索的。这里要用到一个名为自然语言处理（NLP）的工具，简单说就是我不去看单篇文献或荟萃分析，而是通过语义关系探索大量文献中的潜在模式，进而找出热点。今天我用pubmed这个免费的文摘数据库来做个演示，探索下科学研究的整体前沿，结论不一定对，但方法思路如果你能掌握并举一反三，会有发现新大陆的感觉。&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据获取&lt;/h2&gt;
&lt;p&gt;数据获取思路是这样的：如果想知道整体前沿，最需要的是综合类期刊，全文的数据量我的笔记本也跑不了，就考虑摘要，这样也过滤了那些没有摘要的评论与观点，更多关注研究性论文。期刊选择为综合类的《科学》、《自然》与《美国科学院院刊》，收集2016年一整年的论文摘要，用&lt;code&gt;easyPubmed&lt;/code&gt;包来搜索并整理成相对干净的数据集。这里我只收集了题目、摘要、出版期刊与日期进行文本数据挖掘。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,815 x 6
##                                                                          title
##                                                                          &amp;lt;chr&amp;gt;
##  1 A high-throughput small molecule screen identifies synergism between DNA me
##  2                       2017 sneak peek: What the new year holds for science.
##  3                            Wolf transplant could reset iconic island study.
##  4 Scientists in Germany, Peru and Taiwan to lose access to Elsevier journals.
##  5                                                   How scientists use Slack.
##  6                 Saliva protein biomarkers and oral squamous cell carcinoma.
##  7 Reply to Galvão-Moreira and da Cruz: Saliva biomarkers to complement the vi
##  8         Cell morphology drives spatial patterning in microbial communities.
##  9                 Deborah S. Jin 1968-2016: Trailblazer of ultracold science.
## 10 Autophagy wins the 2016 Nobel Prize in Physiology or Medicine: Breakthrough
## # ... with 8,805 more rows, and 5 more variables: abstract &amp;lt;chr&amp;gt;,
## #   year &amp;lt;chr&amp;gt;, month &amp;lt;chr&amp;gt;, day &amp;lt;chr&amp;gt;, jabbrv &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;发文量&lt;/h2&gt;
&lt;p&gt;首先我们先看看着三份期刊的发文量：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这三份期刊里，PNAS发文量最大，占总数一半。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;高频词&lt;/h2&gt;
&lt;p&gt;然后我们看一下各期刊的前十大摘要高频词：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/abs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里解释一下，如果我们单纯寻找高频词其实这几个期刊都应该差不多，但这里我们用的是TF-IDF来加权筛选，这个加权不严谨的说就是这个词出现在该期刊的词频与出现在所有期刊词频的比例，通过这个值我们可以找到单个期刊比较重要的词。我们可以看到肿瘤与行为均出现在三个期刊的十大关键词中，推测相关研究应该是去年的重点。此外，《自然》与《美国科学院院刊》都出现了模型这个词。就特色而言，《自然》去年更关注造血过程、信号传递与衰老问题；《科学》杂志则关心磷酸化、spo11蛋白与火山口还有小尺度问题；《美国科学院院刊》主题特色不算明显，但比较喜欢强调研究重要性。&lt;/p&gt;
&lt;p&gt;如果我们只考虑题目里的文字呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/title-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们可以看出，《自然》上的论文题目跟摘要内容契合度比较高；《科学》上论文题目喜欢出现中美的国家标签；《美国科学院院刊》看意思题目里专业名词比较多。此外，三份期刊的题目里都出现了勘误，这倒是前沿高影响力期刊的特点：容易被质疑。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;词关系&lt;/h2&gt;
&lt;p&gt;看完整体你应该想到，单个词并非孤立，那么这些词之间会不会有相关性呢？这个问题我们也可以用NLP工具来研究：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/termrelation-2.png&#34; width=&#34;672&#34; /&gt; 其实这个技术更常见，平时你用的输入法就实现去考察一些字词的关系，然后让其出现的排序更符合常识。这里我们可以看到，从题目里我们能看到气候变化、干细胞以及前面提到的勘误问题。从摘要里我们则会发现大多数是生物相关的主题，也就是前沿科研应该是生命科学在导向。但到目前为止我们都是把这一些文本当成一个整体，但科学是分科的，也就是有不同的主题，此时我们就要用到主题模型来探索去年前沿科研关注的主题分类。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;主题模型分类&lt;/h2&gt;
&lt;p&gt;所谓主题模型，就是通过探索字词间内部关系对文本进行分类的模型，举例来说某个潜在的主题包含7个关键词，如果某篇文章命中6个，那么这篇文章大概率就属于这个潜在主题。当然，现实生活我们并不知道这些潜在主题会是什么，但通过隐含狄利克雷分布，也就是LDA方法我们就可以去探索结构，然后去拟合实际经验。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/topic-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;从上面我们可以看出，有些探索出来的主题大概我们知道是哪个领域的，有些则属于误判或者说界限不明显的综合领域，这说明跨学科研究正在崛起。其中，我能识别出来的主题大体有癌症、脑科学、病毒、社会行为、基因组、膜蛋白结构、气候变化、进化、动态系统、材料。总体来看，细胞生物学与分子生物学还是主流，但病毒、气候变化等问题导向的学科也在发展。其实也可以直接分析10年的时间变化趋势，不过这个就留成课后题吧（其实是我个人电脑跑不动）。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;情感分析&lt;/h2&gt;
&lt;p&gt;一般认为科研人员都是比较乐观的，但其实文字背后究竟是否乐观可以用文本的情感分析来回答。这个分析的原理就是事先找个标注过情感的语料库，然后通过语料库与词频来分析具体文本的情感倾向性。正常这个语料库是要自己根据语境去构建的，例如商品的好评差评，但作为资深懒汉，我直接用了现成的AFINN语料库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./cn/2017-06-16-nlp_files/figure-html/sen-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;结果基本符合乐观为主的预期，不过按说有些词在科研中属于中性词，我们可以通过这个分析来考虑论文写作的用词方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;其他&lt;/h2&gt;
&lt;p&gt;其实这只是一个很初步的分析，我甚至没用用到引用与被引用的关系，也没有考虑作者与研究机构的时空分布特征，但类似这样的文本分析应该是一个现代科研人员所具备的属性。这种分析的好处在于你不是在采样，而是直接分析所拥有的整体，也就几十兆的文本量，如果你电脑跑得动，把十年二十年的文献沿革都可以概览一下，这是这个时代给我们的红利，不要白不要。&lt;/p&gt;
&lt;p&gt;你可以研究一个大牛几十年的论文发表来发现其独到的眼光；也可以针对某个期刊挖掘其关注点的变更；还可以构建自己认可的课题组的文献库，通过其发表内容探索同行那些自己都没意识到的行为改变。这个时代学科内的经验贬值飞速，很多东西没必要闭门造车慢慢悟，利用开放数据的便利性你可以很快了解整体学术动态，这样不至于随波逐流。更麻烦的是如果你不懂而别人懂，那你将很容易体会到别人眼神中的怜悯，做一个好奇心使然的科研人员，现在起步从来都不晚。&lt;/p&gt;
&lt;p&gt;更重要的是，这类技术本质是让你满足好奇心的，你可以用这个来了解社会，例如纽约时报就给个人提供API，你可以看看其对川普用词风格的变化；为什么最近比特币搜索指数集中在拉美？欧洲吸引难民究竟是政治正确还是劳动力人口不足？不要等着看新闻来指导自己，要学会发现生活中的闪光点；不要通过键盘上情感喧嚣来面对社会，要用键盘甚至语音编程（我果然很自然的想到了最懒的方法）从繁复的公开数据中挖掘趋势；不要总是等着大牛来带，在未知的领域人人都可能成为大牛，你需要掌握一些实现方法而已，你甚至不需要太了解算法细节（会忘，比如我），但要有自己的兵器库随想随用。你不需要带着目的性去学，这说到底只是一种生活方式，你变强了也秃了的可能性是存在的（你能否感到我最近在看漫画）。&lt;/p&gt;
&lt;p&gt;本文实现代码可见我的&lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/cn/2017-06-16-nlp.Rmd&#34;&gt;Github&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;想要了解世界吗？想要的话可以全部給你，去找吧！我把所有的线索都放在互联网上了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;参考文献&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://tidytextmining.com/&#34; class=&#34;uri&#34;&gt;http://tidytextmining.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection of open source software platform for metabolomics or lipidomics</title>
      <link>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</guid>
      <description>&lt;p&gt;Recently I reviewed some platforms for metabolomics or lipidomics and tried to find out which one is currently available to my demands. I knew some instrumental company supplied software or solutions for metabolomics and lipidomics. I just disliked them since they concealed too many details about data processing which made omics studies as a magic box like Artificial Neural Network. It gains nothing for scientist to get insights from the data and you might only follow the workflow defined by the company. I read some papers using those kind of software and felt the authors know little about what they performed. Data analysis for metabolomics or lipidomics is a systems engineering. If someone really want to use this tools, just choose a open source platform and inspect the code when you needed. Otherwise, your work could be replaced by AI someday. It’s not a joke. Here is a summary for the selection of mass spectrum based metabolomics or lipidomics software platform.&lt;/p&gt;
&lt;div id=&#34;principles-for-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principles for selection&lt;/h2&gt;
&lt;p&gt;A decent solution should have functions covering the following required functions or features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open source: as I said before, researcher could benefit a lot from open source community. Based on your experiences, Java, matlab, python and R would be your choices. I liked R most while python might dominate everything in the further.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data input/output: this work could be done by msconvert from &lt;a href=&#34;http://proteowizard.sourceforge.net/tools.shtml&#34;&gt;proteowizard&lt;/a&gt;. I preferred to install msconvert directly on the instruments and convert the vendor files into mzML or mzXML files to perform further analysis. I just dislike Windows. On the other hand, some software from the instruments could output CDF files, which is also nice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turn the Raw data into mass-retention time matrix: single data file from mass spectrum would always contain matadata and profile data. The former container has records for that data and the latter were always full scans of mass spectrum indexed by retention time. Then you need to map such data into a matrix because the following peaks picking and alignments always based on algorithms for matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peaks picking: you also need functions to pick up the peaks from mass-retention time matrix. Functions like centWave or isotope-based algorithms would help to find the peaks. Sometimes you need to bin the matrix first before peak picking and such requirements should be carefully checked. Anyway you could just use one function to perform peaks picking in most software. The output of this step would be a peak list with intensity, mass and retention time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Retention time correction: when you process more than one data file such as technique replicates, the drift of retention time would make the final peak list contained replicated peaks. Someone use certain peaks to corrected the data, while I prefer some global similarity analysis based algorithm to undertake this task. However, some software might add one step to group the peaks before and/or after retention time correction. Anyway, the output of this step is the peak list from one group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Batch effects correction: this is another issue from analysis procedure and batch effects would also bury signals from noise. Researchers from analytical chemistry always use random experiment design and pooled sample to check if the batch effects exist. However, I think the most important things were correction. Just treat a series of samples suffered batch effects and use some statistical analysis to correct them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical analysis: when you get the peaks list, you could perform the following supervised or non-supervised analysis to find bio-markers or data pattern. I think &lt;a href=&#34;http://www.metaboanalyst.ca/&#34;&gt;Metaboanalyst&lt;/a&gt; could satisfy most researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Annotation or identification: the peaks annotation is also important. &lt;a href=&#34;www.hmdb.ca&#34;&gt;HMDB&lt;/a&gt; and &lt;a href=&#34;http://www.lipidmaps.org/&#34;&gt;LIPID MAPS&lt;/a&gt; would be the basic version for mapping peaks to compounds. In most cases, MS/MS data would be used. I think most of MS/MS dataset could be separated handled by &lt;a href=&#34;https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp&#34;&gt;GNPS&lt;/a&gt; or &lt;a href=&#34;https://metlin.scripps.edu&#34;&gt;Metlin&lt;/a&gt; and predicted by &lt;a href=&#34;http://www.csi-fingerid.org/&#34;&gt;FingerID&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pathway analysis: sometimes we also need to know the relationship among compounds and pathway analysis is always performed for that purpose.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Platform&lt;/h2&gt;
&lt;div id=&#34;xcms-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS online&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xcmsonline.scripps.edu/landing_page.php?pgcontent=mainPage&#34;&gt;XCMS online&lt;/a&gt; is hosted by Scripps Institute. If your datasets are not large, XCMS online would be the best option for you. Recently they updated the online version to support more functions for systems biology. They use metlin and iso metlin to annotate the MS/MS data. Pathway analysis is also supported. Besides, to accelerate the process, xcms online employed stream (windows only). You could use stream to connect your instrument workstation to their server and process the data along with the data acquisition automate. They also developed apps for xcms online, but I think apps for slack would be even cooler to control the data processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PRIMe&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://prime.psc.riken.jp/Metabolomics_Software/&#34;&gt;PRIMe&lt;/a&gt; is from RIKEN and UC Davis. It supports mzML and major MS vendor formats. They defined own file format ABF and eco-system for omics studies. The software are updated almost everyday. You could use MS-DIAL for untargeted analysis and MRMOROBS for targeted analysis. For annotation, they developed MS-FINDER and they also developed statistic tools with excel. This platform could replaced the dear software from company and well prepared for MS/MS data analysis and lipidomics. They are open source, work on Windows and also could run within mathmamtics. However, they don’t cover pathway analysis. Another feature is they always show the most recently spectral records from public repositories. You could always get the updated MSP spectra files for your own data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OpenMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.openms.de/&#34;&gt;OpenMS&lt;/a&gt; is another good platform for mass spectrum data analysis developed with C++. You could use them as plugin of &lt;a href=&#34;https://www.knime.org/&#34;&gt;KNIME&lt;/a&gt;. I suggest anyone who want to be a data scientist to get familiar with platform like KNIME because they supplied various API for different programme language, which is easy to use and show every steps for others. Also TOPPView in OpenMS could be the best software to visualize the MS data. You could always use the metabolomics workflow to train starter about details in data processing. pyOpenMS and OpenSWATH are also used in this platform. If you want to turn into industry, this platform fit you best because you might get a clear idea about solution and workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mzmine-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MZmine 2&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://mzmine.github.io/&#34;&gt;MZmine 2&lt;/a&gt; has three version developed on Java platform and the lastest version is included into &lt;a href=&#34;https://msdk.github.io/&#34;&gt;MSDK&lt;/a&gt;. Similar function could be found from MZmine 2 as shown in XCMS online. However, MZmine 2 do not have pathway analysis. You could use metaboanalyst for that purpose. Actually, you could go into MSDK to find similar function supplied by &lt;a href=&#34;http://www.proteosuite.org&#34;&gt;ProteoSuite&lt;/a&gt; and &lt;a href=&#34;https://www.openchrom.net/&#34;&gt;Openchrom&lt;/a&gt;. If you are a experienced coder for Java, you should start here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xcms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/release/bioc/html/xcms.html&#34;&gt;xcms&lt;/a&gt; is different from xcms online while they might share the same code. I used it almost every data to run local metabolomics data analysis. Recently, they will change their version to xcms 3 with major update for object class. Their data format would integrate into the MSnbase package and the parameters would be easy to set up for each step. Normally, I will use msconvert-IPO-xcms-xMSannotator-metaboanalyst as workflow to process the offline data. It could accelerate the process by parallel processing. However, if you are not familiar with R, you would better to choose some software above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emory-mahpic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Emory MaHPIC&lt;/h3&gt;
&lt;p&gt;This platform is composed by several R packages from Emory University including &lt;a href=&#34;https://sourceforge.net/projects/aplcms/&#34;&gt;apLCMS&lt;/a&gt; to collect the data, &lt;a href=&#34;https://sourceforge.net/projects/xmsanalyzer/&#34;&gt;xMSanalyzer&lt;/a&gt; to handle automated pipeline for large-scale, non-targeted metabolomics data, &lt;a href=&#34;https://sourceforge.net/projects/xmsannotator/&#34;&gt;xMSannotator&lt;/a&gt; for annotation of LC-MS data and &lt;a href=&#34;https://code.google.com/archive/p/atcg/wikis/mummichog_for_metabolomics.wiki&#34;&gt;Mummichog&lt;/a&gt; for pathway and network analysis for high-throughput metabolomics. This platform would be preferred by someone from environmental science to study exposome. I always use xMSannotator to annotate the LC-MS data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;others&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://genomics-pubs.princeton.edu/mzroll/index.php?show=index&#34;&gt;MAVEN&lt;/a&gt; from Princeton University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vpr.colostate.edu/pmf/wp-content/uploads/sites/23/2015/06/Broeckling_2014.pdf&#34;&gt;RAMclustR&lt;/a&gt; from Colorado State University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/html/MAIT.html&#34;&gt;MAIT&lt;/a&gt; based on xcms&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yufree/enviGCMS&#34;&gt;enviGCMS&lt;/a&gt; from me&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.ebi.ac.uk/metabolights/&#34;&gt;Metabolights&lt;/a&gt; for sharing data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;All of the platform above could be used for metabolomics and lipidomics. However, best selection would be based on your programming skills and the popularity in your research area. Every tool need training before analysis your data and you could choose one randomly and be focused on the source code for one day. If you feel you could handle it, just use it. Otherwise select another one. Enjoy and fight with the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>